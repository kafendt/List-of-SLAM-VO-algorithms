@article{Pire2015,
abstract = {This paper describes a visual SLAM system based on stereo cameras and focused on real-time localization for mobile robots. To achieve this, it heavily exploits the parallel nature of the SLAM problem, separating the time-constrained pose estimation from less pressing matters such as map building and refinement tasks. On the other hand, the stereo setting allows to reconstruct a metric 3D map for each frame of stereo images, improving the accuracy of the mapping process with respect to monocular SLAM and avoiding the well-known bootstrapping problem. Also, the real scale of the environment is an essential feature for robots which have to interact with their surrounding workspace. A series of experiments, on-line on a robot as well as off-line with public datasets, are performed to validate the accuracy and real-time performance of the developed method.},
author = {Pire, Taihu and Fischer, Thomas and Civera, Javier and {De Cristoforis}, Pablo and Berlles, Julio Jacobo},
doi = {10.1109/IROS.2015.7353546},
file = {:home/chris/Documents/Mendeley Desktop/Stereo parallel tracking and mapping for robot localization.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {1373--1378},
title = {{Stereo parallel tracking and mapping for robot localization}},
volume = {2015-Decem},
year = {2015}
}
@article{Zhu2017,
abstract = {Visual odometry is an important research prob- lem for computer vision and robotics. In general, the feature-based visual odometry methods heavily rely on the accurate correspondences between lo- cal salient points, while the direct approaches could make full use of whole image and perform dense 3D reconstruction simultaneously. However, the di- rect visual odometry usually suffers from the draw- back of getting stuck at local optimum especially with large displacement, which may lead to the in- ferior results. To tackle this critical problem, we propose a novel scheme for stereo odometry in this paper, which is able to improve the convergence with more accurate pose. The key of our approach is a dual Jacobian optimization that is fused into a multi-scale pyramid scheme. Moreover, we intro- duce a gradient-based feature representation, which enjoys the merit of being robust to illumination changes. Furthermore, a joint direct odometry ap- proach is proposed to incorporate the information from the last frame and previous keyframes. We have conducted the experimental evaluation on the challenging KITTI odometry benchmark, whose promising results show that the proposed algorithm is very effective for stereo visual odometry. 1},
author = {Zhu, Jianke},
file = {:home/chris/Documents/Mendeley Desktop/Image Gradient-based Joint Direct Visual Odometry for Stereo Camera.pdf:pdf},
journal = {Int. Jt. Conf. Artif. Intell.},
keywords = {Robotics and Vision: Robotics and Vision,Robotics and Vision: Vision and Perception},
pages = {4558--4564},
title = {{Image Gradient-based Joint Direct Visual Odometry for Stereo Camera}},
url = {https://www.ijcai.org/proceedings/2017/0636.pdf},
year = {2017}
}
@article{Zhang2015a,
abstract = {We propose a real-time method for odometry and mapping using range measurements from a 2-axis lidar moving in 6-DOF. The problem is hard because the range measurements are received at different times, and errors in motion estimation can cause mis-registration of the resulting point cloud. To date, coherent 3D maps can be built by off-line batch methods, often using loop closure to correct for drift over time. Our method achieves both low-drift and low-computational complexity with- out the need for high accuracy ranging or inertial measurements. The key idea in obtaining this level of performance is the division of the complex problem of simultaneous localization and mapping, which seeks to optimize a large number of variables simultaneously, by two algorithms. One algorithm performs odometry at a high frequency but low fidelity to estimate velocity of the lidar. Another algorithm runs at a frequency of an order of magnitude lower for fine matching and registration of the point cloud. Combination of the two algorithms allows the method to map in real-time. The method has been evaluated by a large set of experiments as well as on the KITTI odometry benchmark. The results indicate that the method can achieve accuracy at the level of state of the art offline batch methods.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Zhang, Ji and Singh, Sanjiv and Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J. and Mur-Artal, Raul and Tardos, Juan D. and Paper, Conference and Caruso, David and Engel, Jakob and Cremers, Daniel},
doi = {10.15607/RSS.2014.X.007},
eprint = {9605103},
file = {:home/chris/Documents/Mendeley Desktop/LOAM$\backslash$: Lidar Odometry and Mapping in Real-time.pdf:pdf},
isbn = {9781479999941},
issn = {15523098},
journal = {IEEE Trans. Robot.},
keywords = {Cameras,Computational modeling,Factor graphs,Lenses,Nonlinear distortion,Simultaneous localization and mapping,Three-dimensional displays,localization,mapping,maximum a posteriori estimation,perception,robots,sensing,simultaneous localization and mapping (SLAM)},
number = {July},
pages = {141--148},
pmid = {7353366},
primaryClass = {cs},
title = {{LOAM: Lidar Odometry and Mapping in Real- time}},
volume = {32},
year = {2015}
}
@article{Buczko2016,
abstract = {{\textcopyright} 2016 IEEE.In this paper, we present an iterative two-stage scheme for precise and robust frame-To-frame feature-based ego-motion estimation using stereo cameras. We analyze the characteristics of the optical flows and reprojection errors that are independently induced by each of the decoupled six degrees of freedom motion. As we will show, the different characteristics of these induced optical flows lead to a reprojection error that depends on the coordinates of the features. When using a proper normalization of the reprojection error, this coordinatedependency can be almost completely removed for decoupled motions. Furthermore, we present a way to use these results for automotive application where rotation and forward motion are coupled. This is done by compensating for the flow that is induced by the rotation, which decouples the translation flow from the overall flow. The resulting method generalizes the ROCC approach [4], where a robust outlier criterion was introduced and proved to increase robustness and quality for large forward translation motions. Therewith the proposed method generalizes ROCC to almost all possible automotive motions. The performance of the method is evaluated on Kitti benchmark and currently2 reaches the best translation error of all camera-based methods.},
author = {Buczko, Martin and Willert, Volker},
doi = {10.1109/ITSC.2016.7795703},
file = {:home/chris/Documents/Mendeley Desktop/Flow-Decoupled Normalized Reprojection Error for Visual Odometry.pdf:pdf},
isbn = {9781509018895},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
pages = {1161--1167},
title = {{Flow-decoupled normalized reprojection error for visual odometry}},
year = {2016}
}
@inproceedings{Zhang2014,
abstract = {Visual odometry can be augmented by depth in- formation such as provided by RGB-D cameras, or from lidars associated with cameras. However, such depth information can be limited by the sensors, leaving large areas in the visual images where depth is unavailable. Here, we propose a method to utilize the depth, even if sparsely available, in recovery of camera motion. In addition, the method utilizes depth by triangulation from the previously estimated motion, and salient visual features for which depth is unavailable. The core of our method is a bundle adjustment that refines the motion estimates in parallel by processing a sequence of images, in a batch optimization. We have evaluated our method in three sensor setups, one using an RGB-D camera, and two using combinations of a camera and a 3D lidar. Our method is rated 2 on the KITTI odometry benchmark irrespective of sensing modality, and is rated 1 among visual odometry methods.},
address = {Chicago},
author = {Zhang, Ji and Kaess, Michael and Singh, Sanjiv},
booktitle = {Int. Conf. Intell. Robot. Syst.},
file = {:home/chris/Documents/Mendeley Desktop/Real-time depth enhanced monocular odometry.pdf:pdf},
isbn = {9781479969340},
pages = {4973--4980},
title = {{Real-time Depth Enhanced Monocular Odometry - IROS{\_}2014.pdf}},
url = {https://www.ri.cmu.edu/pub{\_}files/2014/9/IROS{\_}2014.pdf},
year = {2014}
}
@article{Buczko2016a,
abstract = {In this paper, we present an outlier removal scheme for stereo-based visual odometry which is especially suited for improving high-speed pose change estimations in large-scale depth environments. First we investigate the vari- ance of the reprojection error on the 3D position of a feature given a fixed error in pose change to conclude that a detection of outliers based on a fixed threshold on the reprojection error is inappropriate. Then we propose an optical flow dependent feature-adaptive scaling of the reprojection error to reach almost invariance to the 3D position of each feature. This feature-adaptive scaling is derived from an approximation showing the relation between longitudinal pose change of the camera, absolute value of the optical flow, and distance of the feature. Using this scaling, we develop an iterative alternating scheme to guide the separation of inliers from outliers. It optimizes the tradeoff between finding a good criterion to remove outliers based on a given pose change and improving the pose change hypothesis based on the current set of inliers. Including the new outlier removal scheme into a pure two-frame stereo-based visual odometry pipeline without applying bundle adjustment or SLAM-filtering we are currently ranked amongst the top camera-based algorithms and furthermore outperform camera and laser scanner methods in Kitti benchmark's high- speed scenarios.},
author = {Buczko, Martin and Willert, Volker},
doi = {10.1109/IVS.2016.7535429},
file = {:home/chris/Documents/Mendeley Desktop/How to Distinguish Inliers from Outliers in Visual Odometry for High-speed Automotive Applications.pdf:pdf},
isbn = {9781509018215},
journal = {IEEE Intell. Veh. Symp. Proc.},
number = {Iv},
pages = {478--483},
title = {{How to distinguish inliers from outliers in visual odometry for high-speed automotive applications}},
volume = {2016-Augus},
year = {2016}
}
@article{Pire2017,
abstract = {This paper describes a real-time feature-based stereo SLAM system that is robust and accurate in a wide variety of conditions – indoors, outdoors, with dynamic objects, changing light conditions, fast robot motions and large-scale loops. Our system follows a parallel-tracking-and-mapping strategy: a tracking thread estimates the camera pose at frame rate; and a mapping thread updates a keyframe-based map at a lower frequency. The stereo constraints of our system allow a robust initialization – avoiding the well-known bootstrapping problem in monocular systems–and the recovery of the real scale. Both aspects are essential for its practical use in real robotic systems that interact with the physical world. In this paper we provide the implementation details, an exhaustive evaluation of the system in public datasets and a comparison of most state-of-the-art feature detectors and descriptors on the presented system. For the benefit of the community, its code for ROS (Robot Operating System) has been released.},
author = {Pire, Taih{\'{u}} and Fischer, Thomas and Castro, Gast{\'{o}}n and {De Crist{\'{o}}foris}, Pablo and Civera, Javier and {Jacobo Berlles}, Julio},
doi = {10.1016/j.robot.2017.03.019},
file = {:home/chris/Documents/Mendeley Desktop/S-PTAM - Stereo Parallel Tracking and Mapping.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {Loop closure,SLAM,Stereo SLAM,Stereo vision,Visual SLAM},
pages = {27--42},
publisher = {Elsevier B.V.},
title = {{S-PTAM: Stereo Parallel Tracking and Mapping}},
url = {http://dx.doi.org/10.1016/j.robot.2017.03.019},
volume = {93},
year = {2017}
}
@inproceedings{Engel2015,
abstract = {We propose a novel Large-Scale Direct SLAM algorithm for stereo cameras (Stereo LSD-SLAM) that runs in real-time at high frame rate on standard CPUs. In contrast to sparse interest-point based methods, our approach aligns images directly based on the photoconsistency of all high- contrast pixels, including corners, edges and high texture areas. It concurrently estimates the depth at these pixels from two types of stereo cues: Static stereo through the fixed-baseline stereo camera setup as well as temporal multi-view stereo exploiting the camera motion. By incorporating both disparity sources, our algorithm can even estimate depth of pixels that are under-constrained when only using fixed-baseline stereo. Using a fixed baseline, on the other hand, avoids scale-drift that typically occurs in pure monocular SLAM.We furthermore propose a robust approach to enforce illumination invariance, capable of handling aggressive brightness changes between frames – greatly improving the performance in realistic settings. In experiments, we demonstrate state-of-the-art results on stereo SLAM benchmarks such as Kitti or challenging datasets from the EuRoC Challenge 3 for micro aerial vehicles.},
address = {Hamburg},
author = {Engel, Jakob and Cremers, Daniel},
booktitle = {IEEE/RSJ Int. Conf. Intell. Robot. Syst.},
file = {:home/chris/Documents/Mendeley Desktop/Large-Scale Direct SLAM with Stereo Cameras.pdf:pdf},
isbn = {9781479999941},
pages = {1935--1942},
title = {{Large-Scale Direct SLAM with Stereo Cameras}},
year = {2015}
}
@article{Cvisic2015,
abstract = {In this paper we present a novel algorithm for fast and robust stereo visual odometry based on feature selection and tracking (SOFT). The reduction of drift is based on careful selection of a subset of stable features and their tracking through the frames. Rotation and translation between two consecutive poses are estimated separately. The five point method is used for rotation estimation, whereas the three point method is used for estimating translation. Experimental results show that the proposed algorithm has an average pose error of 1.03{\%} with pro- cessing speed above 10 Hz. According to publicly available KITTI leaderboard, SOFT outperforms all other validated methods. We also present a modified IMU-aided version of the algorithm, fast and suitable for embedded systems. This algorithm employs an IMU for outlier rejection and Kalman filter for rotation refinement. Experiments show that the IMU based system runs at 20 Hz on an ODROID U3 ARM-based embedded computer without any hardware acceleration. Integration of all components is},
author = {Cvi{\v{s}}i{\'{c}}, Igor and Petrovi{\'{c}}, Ivan},
doi = {10.1109/ECMR.2015.7324219},
file = {:home/chris/Documents/Mendeley Desktop/Stereo odometry based on careful feature selection and tracking.pdf:pdf},
isbn = {9781467391634},
journal = {2015 Eur. Conf. Mob. Robot. ECMR 2015 - Proc.},
keywords = {Accuracy,Cameras,Estimation,Feature extraction,Kalman filters,Three-dimensional displays,Visualization},
pages = {0--5},
title = {{Stereo odometry based on careful feature selection and tracking}},
year = {2015}
}
@article{Buczko2017,
author = {Buczko, Martin and Willert, Volker},
doi = {10.1109/IVS.2017.7995805},
file = {:home/chris/Documents/Mendeley Desktop/Monocular Outlier Detection for Visual Odometry.pdf:pdf},
isbn = {978-1-5090-4804-5},
journal = {2017 IEEE Intell. Veh. Symp.},
keywords = {Mapping and Localization,Image,Radar,Lidar Signal },
number = {Iv},
pages = {739--745},
title = {{Monocular Outlier Detection for Visual Odometry}},
url = {http://ieeexplore.ieee.org/document/7995805/},
year = {2017}
}
@article{Zhang2015,
abstract = {— Here, we present a general framework for com-bining visual odometry and lidar odometry in a fundamental and first principle method. The method shows improvements in performance over the state of the art, particularly in robustness to aggressive motion and temporary lack of visual features. The proposed on-line method starts with visual odometry to estimate the ego-motion and to register point clouds from a scanning lidar at a high frequency but low fidelity. Then, scan matching based lidar odometry refines the motion estimation and point cloud registration simultaneously. We show results with datasets collected in our own experiments as well as using the KITTI odometry benchmark. Our proposed method is ranked {\#}1 on the benchmark in terms of average translation and rotation errors, with a 0.75{\%} of relative position drift. In addition to comparison of the motion estimation accuracy, we evaluate robustness of the method when the sensor suite moves at a high speed and is subject to significant ambient lighting changes.},
author = {Zhang, Ji and Singh, Sanjiv},
doi = {10.1109/ICRA.2015.7139486},
file = {:home/chris/Documents/Mendeley Desktop/Visual-lidar Odometry and Mapping$\backslash$: Low-drift, Robust, and Fast.pdf:pdf},
isbn = {9781479969227},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
number = {June},
pages = {2174--2181},
title = {{Visual-lidar odometry and mapping: Low-drift, robust, and fast}},
volume = {2015-June},
year = {2015}
}
@article{Horn1987,
abstract = {Finding the relationship between two coordinate systems using pairs of measurements of the coordinates of a number of points in both systems is a classic photogrammetric task. It finds applications in stereophotogrammetry and in robotics. I present here a closed-form solution to the least-squares problem for three or more points. Currently various empirical, graphical, and numerical iterative methods are in use. Derivation of the solution is simplified by use of unit quaternions to represent rotation. I emphasize a symmetry property that a solution to this problem ought to possess. The best translational offset is the difference between the centroid of the coordinates in one system and the rotated and scaled centroid of the coordinates in the other system. The best scale is equal to the ratio of the root-mean-square deviations of the coordinates in the two systems from their respective centroids. These exact results are to be preferred to approximate methods based on measurements of a few selected points. The unit quaternion representing the best rotation is the eigenvector associated with the most positive eigenvalue of a symmetric 4 × 4 matrix. The elements of this matrix are combinations of sums of products of corresponding coordinates of the points.},
author = {Horn, Berthold K. P.},
doi = {10.1364/JOSAA.4.000629},
file = {:home/chris/Documents/Mendeley Desktop/Closed-form solution of absolute orientation using unit quaternion.pdf:pdf},
isbn = {1084-7529},
issn = {1084-7529},
journal = {J. Opt. Soc. Am. A},
number = {4},
pages = {629},
title = {{Closed-form solution of absolute orientation using unit quaternions}},
url = {https://www.osapublishing.org/abstract.cfm?URI=josaa-4-4-629},
volume = {4},
year = {1987}
}
@book{Sun2014,
author = {Sun, Yu and Behal, Aman and Chung, Chi-kit Ronald},
file = {:home/chris/Documents/Mendeley Desktop/New Developments in Robot Vision.pdf:pdf},
isbn = {9783662438589},
title = {{New Development in Robot Vision}},
year = {2014}
}
@misc{Image:bundle_adjustment,
author = {Sweeney, Chris},
booktitle = {2016},
title = {{Bundle Adjustment}},
url = {http://www.theia-sfm.org/{\_}images/incremental{\_}sfm.png},
urldate = {2017-10-06}
}
@book{Wohler2009,
abstract = {Reconstruction of three-dimensional scene structure from images was an important topic already in the early history of photography, which was invented by Niepce and Daguerre in 1839. The first photogrammetric methods were developed in the middle of the 19th century by Laussedat and Meydenbauer for mapping purposes and reconstruction of buildings (Luhmann, 2003). These photogrammetricmethods were based on geometric modelling of the image formation process, exploiting the perspective projection of a three-dimensional scene into a two-dimensional image plane. Image formation by perspective projection corresponds to the pinhole camera model. There are different image formation models, describing optical devices such as fisheye lenses or omnidirectional lenses. In this work, however, we will restrict ourselves to the pinhole model since it represents the most common image acquisition devices.},
author = {W{\"{o}}hler, Christian},
booktitle = {Methods},
doi = {10.1007/978-3-642-01732-2},
file = {:home/chris/Documents/Mendeley Desktop/3D Computer Vision - Efficient Methods and Applications.pdf:pdf},
isbn = {978-3-642-01731-5},
pages = {181--240},
title = {{3D Computer Vision}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-01732-2},
year = {2009}
}
@article{Lepetit2009,
abstract = {We propose a non-iterative solution to the PnP problem—the estimation of the pose of a calibrated camera from n 3D-to-2D point correspondences—whose computational complexity grows linearly with n. This is in contrast to state-of-the-art methods that are O(n 5) or even O(n 8), without being more accurate. Our method is applicable for all n≥4 and handles properly both planar and non-planar configurations. Our central idea is to express the n 3D points as a weighted sum of four virtual control points. The problem then reduces to estimating the coordinates of these control points in the camera referential, which can be done in O(n) time by expressing these coordinates as weighted sum of the eigenvectors of a 12×12 matrix and solving a small constant number of quadratic equations to pick the right weights. Furthermore, if maximal precision is required, the output of the closed-form solution can be used to initialize a Gauss-Newton scheme, which improves accuracy with negligible amount of additional time. The advantages of our method are demonstrated by thorough testing on both synthetic and real-data.},
author = {Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal},
doi = {10.1007/s11263-008-0152-6},
file = {:home/chris/Documents/Mendeley Desktop/EPnP$\backslash$: An Accurate O(n) Solution to the PnP Problem.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {09205691},
journal = {Int. J. Comput. Vis.},
keywords = {Absolute orientation,Perspective-n-Point,Pose estimation},
number = {2},
pages = {155--166},
title = {{EPnP: An accurate O(n) solution to the PnP problem}},
volume = {81},
year = {2009}
}
@article{Galvez-Lopez2012,
abstract = {We propose a novelmethod for visual place recognition using bag of words obtained from accelerated segment test (FAST)+BRIEF fea- tures. For the first time,we build a vocabulary tree that discretizes a binary descriptor space and use the tree to speed up correspondences for geomet- rical verification.We present competitive results with no false positives in very different datasets, using exactly the same vocabulary and settings. The whole technique, including feature extraction, requires 22 ms/frame in a sequence with 26 300 images that is one order of magnitude faster than previous},
author = {Galvez-Lopez, Dorian and Tardos, Juan D.},
doi = {10.1109/TRO.2011.2179580.},
file = {:home/chris/Documents/Mendeley Desktop/Bags of Binary Words for Fast Place Recognition in Image Sequences.pdf:pdf},
isbn = {1552-3098},
issn = {1552-3098},
journal = {IEEE Conf. Comput. Vis. Pattern Recognit.},
number = {5},
pages = {1188--1197},
pmid = {309728700020},
title = {{Bags of Binary Words for Fast Place Recognition in Image Sequences}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4270051},
volume = {28},
year = {2012}
}
@misc{Image:homography,
author = {Authors, OpenMVG},
title = {{Homography matrix}},
url = {http://openmvg.readthedocs.io/en/latest/openMVG/multiview/multiview/},
urldate = {2017-09-29}
}
@book{RichardHartley2003,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Richard Hartley}, Andrew Zisserman},
booktitle = {J. Chem. Inf. Model.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/chris/Documents/Mendeley Desktop/Multiple View Geometry in Computer Vision (Second Edition).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Multiple View Geometry}},
url = {http://cvrs.whu.edu.cn/downloads/ebooks/Multiple View Geometry in Computer Vision (Second Edition).pdf},
volume = {53},
year = {2003}
}
@article{Hartley1999,
author = {Hartley, Richard and Zisserman, Andrew},
file = {:home/chris/Documents/Mendeley Desktop/Multiple View Geometry Slides.pdf:pdf},
title = {{Imaging Geometry Multiple View Geometry}},
year = {1999}
}
@article{Mur-Artal2014,
abstract = {In this paper we present for the first time a relocalisation method for keyframe-based SLAM that can deal with severe viewpoint change, at frame-rate, in maps containing thousands of keyframes. As this method relies on local features, it permits the interoperability between cameras, allowing a camera to relocalise in a map built by a different camera. We also perform loop closing (detection + correction), at keyframe- rate, in loops containing hundreds of keyframes. For both relocalisation and loop closing, we propose a bag of words place recognizer with ORB features, which is able to recognize places spending less than 39 ms, including feature extrac- tion, in databases containing 10K images (without geometrical verification). We evaluate the performance of this recognizer in four different datasets, achieving high recall and no false matches, and getting better results than the state-of-art in place recognition,},
archivePrefix = {arXiv},
arxivId = {1502.00956},
author = {Mur-Artal, Ra{\'{u}}l and Tard{\'{o}}s, Juan D.},
doi = {10.1109/ICRA.2014.6906953},
eprint = {1502.00956},
file = {:home/chris/Documents/Mendeley Desktop/2014{\_}IEEE{\_}ICRA{\_}Mur{\_}Tardos.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {846--853},
title = {{Fast relocalisation and loop closing in keyframe-based SLAM}},
year = {2014}
}
@article{Rosin1999,
abstract = {We describe methods to measure the following properties of gray level corners: subtended angle, orientation, contrast, bluntness (or rounding of the apex), and boundary curvature (for cusps). Unlike most of the published methods for extracting these properties these new methods are relatively simple, efficient, and robust. They rely on the corner being predetected by a standard operator, thus making the measurement problem more tractable. Using 13,000 synthetic images the methods are assessed over a range of conditions: corners of varying orientations and subtended angles, as well as different degrees of noise.},
author = {Rosin, Paul L},
doi = {10.1006/cviu.1998.0719},
file = {:home/chris/Documents/Mendeley Desktop/Measuring Corner Properties.pdf:pdf},
isbn = {0952189879},
issn = {1077-3142},
journal = {Comput. Vis. Image Underst.},
number = {2},
pages = {291--307},
title = {{Measuring Corner Properties}},
url = {http://www.sciencedirect.com/science/article/pii/S1077314298907196{\%}5Cnhttp://www.sciencedirect.com/science/article/pii/S1077314298907196/pdf?md5=38df182f0236f020ab9a50669d2db49d{\&}pid=1-s2.0-S1077314298907196-main.pdf},
volume = {73},
year = {1999}
}
@article{Calonder2010,
abstract = {We propose to use binary strings as an efficient feature point descriptor, which we call BRIEF. We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to com-pute, instead of the L2 norm as is usually done. As a result, BRIEF is very fast both to build and to match. We compare it against SURF and U-SURF on standard benchmarks and show that it yields a similar or better recognition performance, while running in a fraction of the time required by either.},
author = {Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
doi = {10.1007/978-3-642-15561-1_56},
file = {:home/chris/Documents/Mendeley Desktop/BRIEF$\backslash$: Binary Robust IndependentElementary Features.pdf:pdf},
isbn = {364215560X},
issn = {03029743},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
number = {PART 4},
pages = {778--792},
pmid = {19500939},
title = {{BRIEF: Binary robust independent elementary features}},
volume = {6314 LNCS},
year = {2010}
}
@article{Rosten2006,
abstract = {Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7{\%} of the available processing time. By comparison neither the Harris detector (120{\%}) nor the detection stage of SIFT (300{\%}) can operate at full frame rate. Clearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations [1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion.},
author = {Rosten, Edward and Drummond, Tom},
doi = {10.1007/11744023_34},
file = {:home/chris/Documents/Mendeley Desktop/FAST.pdf:pdf},
isbn = {3540338322},
issn = {16113349},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
pages = {430--443},
pmid = {18684738},
title = {{Machine learning for high-speed corner detection}},
volume = {3951 LNCS},
year = {2006}
}
@misc{,
file = {:home/chris/Documents/Mendeley Desktop/BRIEF{\_}pattern.pdf:pdf},
title = {{BRIEF{\_}pattern.pdf}}
}
@misc{Wiki:ImagePyramid,
title = {{Wikipedia: Pyramid(image processing)}},
url = {https://en.wikipedia.org/wiki/Pyramid{\_}(image{\_}processing)},
urldate = {2017-09-26}
}
@article{Zou2013,
author = {Zou, Danping and Tan, Ping},
file = {:home/chris/Documents/Mendeley Desktop/CoSLAM$\backslash$: Collaborative Visual SLAM in Dynamic Environments.pdf:pdf},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
title = {{CoSLAM : Collaborative Visual SLAM in Dynamic Environments}},
url = {http://www.cs.sfu.ca/{~}pingtan/Papers/pami12{\_}slam.pdf},
year = {2013}
}
@article{Kohlbrecher2011,
abstract = {For many applications in Urban Search and Rescue (USAR) scenarios robots need to learn a map of unknown environments. We present a system for fast online learning of occupancy grid maps requiring low computational resources. It combines a robust scan matching approach using a LIDAR system with a 3D attitude estimation system based on inertial sensing. By using a fast approximation of map gradients and a multi-resolution grid, reliable localization and mapping capabilities in a variety of challenging environments are realized. Multiple datasets showing the applicability in an embedded hand-held mapping system are provided. We show that the system is sufficiently accurate as to not require explicit loop closing techniques in the considered scenarios. The software is available as an open source package for ROS.},
author = {Kohlbrecher, Stefan and {Von Stryk}, Oskar and Meyer, Johannes and Klingauf, Uwe},
doi = {10.1109/SSRR.2011.6106777},
file = {:home/chris/Documents/Mendeley Desktop/A Flexible and Scalable SLAM System with Full 3D Motion Estimation.pdf:pdf},
isbn = {9781612847696},
issn = {2374-3247},
journal = {9th IEEE Int. Symp. Safety, Secur. Rescue Robot. SSRR 2011},
keywords = {Inertial Navigation,Robust and Fast Localization,Simultaneous Localization and Mapping},
pages = {155--160},
title = {{A flexible and scalable SLAM system with full 3D motion estimation}},
year = {2011}
}
@article{Yang2016,
abstract = {— Existing simultaneous localization and mapping (SLAM) algorithm is not robust in challenging low-texture environments because of few salient features. The resulting sparse or semi-dense map also conveys little information for motion planning. Though some work utilize plane or scene layout for dense map regularization, they require decent state estimation from other sources. In this paper, we propose a real-time monocular plane SLAM to demonstrate that scene understanding could improve both state estimation and dense mapping especially in low-texture environments. The plane measurements come from the pop-up 3D plane model from each single image. We also combine planes with point based SLAM to solve the ill-constrained problems. On a public TUM dataset, our algorithm generates dense semantic 3D model with pixel depth error of 6.2 cm while existing SLAM fails. On a 60 m long dataset with loops, our method creates a much better 3D model with state estimation error of 0.67 {\%}.},
archivePrefix = {arXiv},
arxivId = {1703.07334},
author = {Yang, Shichao and Song, Yu and Kaess, Michael and Scherer, Sebastian},
doi = {10.1109/IROS.2016.7759204},
eprint = {1703.07334},
file = {:home/chris/Documents/Mendeley Desktop/Pop-up SLAM$\backslash$: Semantic Monocular Plane SLAM for Low-texture Environments.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {1222--1229},
title = {{Pop-up SLAM: Semantic monocular plane SLAM for low-texture environments}},
volume = {2016-Novem},
year = {2016}
}
@inbook{Fioraio2015,
abstract = {We propose an effective, real-time solution to the RGB-D SLAM problem dubbed SlamDunk. Our proposal features a multi-view camera tracking approach based on a dynamic local map of the workspace, enables metric loop closure seamlessly and preserves local consistency by means of relative bundle adjustment principles. SlamDunk requires a few threads, low memory consumption and runs at 30 Hz on a standard desktop computer without hardware acceleration by a GPGPU card. As such, it renders real-time dense SLAM affordable on commodity hardware. SlamDunk permits highly responsive interactive operation in a variety of workspaces and scenarios, such as scanning small objects or densely reconstructing large-scale environments. We provide quantitative and qualitative experiments in diverse settings to demonstrate the accuracy and robustness of the proposed approach.},
address = {Cham},
author = {Fioraio, Nicola and {Di Stefano}, Luigi},
booktitle = {Comput. Vis. - ECCV 2014 Work. Zurich, Switzerland, Sept. 6-7 12, 2014, Proceedings, Part I},
doi = {10.1007/978-3-319-16178-5_28},
editor = {Agapito, Lourdes and Bronstein, Michael M and Rother, Carsten},
file = {:home/chris/Documents/Mendeley Desktop/SlamDunk$\backslash$: Affordable Real-Time RGB-D SLAM.pdf:pdf},
isbn = {978-3-319-16178-5},
pages = {401--414},
publisher = {Springer International Publishing},
title = {{SlamDunk: Affordable Real-Time RGB-D SLAM}},
url = {https://doi.org/10.1007/978-3-319-16178-5{\_}28},
year = {2015}
}
@inproceedings{Strasdat2011,
abstract = {We present a novel and general optimisation framework for visual SLAM, which scales for both local, highly accu- rate reconstruction and large-scale motion with long loop closures. We take a two-level approach that combines accu- rate pose-point constraints in the primary region of interest with a stabilising periphery of pose-pose soft constraints. Our algorithm automatically builds a suitable connected graph of keyposes and constraints, dynamically selects in- ner and outer window membership and optimises both si- multaneously. We demonstrate in extensive simulation ex- periments that our method approaches the accuracy of off- line bundle adjustment while maintaining constant-time op- eration, even in the hard case of very loopy monocular cam- era motion. Furthermore, we present a set of real experi- ments for various types of visual sensor and motion, includ- ing large scale SLAM with both monocular and stereo cam- eras, loopy local browsing with either monocular or RGB-D cameras, and dense RGB-D object model building},
author = {Strasdat, Hauke and Davison, Andrew J. and Montiel, J. M M and Konolige, Kurt},
booktitle = {Int. Conf. Comput. Vis.},
file = {:home/chris/Documents/Mendeley Desktop/Double Window Optimisation for Constant Time Visual SLAM.pdf:pdf},
pages = {2352--2359},
title = {{Double Window Optimisation for Constant Time Visual SLAM}},
url = {http://ieeexplore.ieee.org/document/6126517},
year = {2011}
}
@article{Whelan2015a,
abstract = {We present a new simultaneous localization and mapping (SLAM) system capable of producing high-quality globally con- sistent surface reconstructions over hundreds of meters in real time with only a low-cost commodity RGB-D sensor. By using a fused volumetric surface reconstruction we achieve a much higher quality map over what would be achieved using raw RGB-D point clouds. In this paper we highlight three key techniques associated with applying a volumetric fusion- based mapping system to the SLAM problem in real time. First, the use of a GPU-based 3D cyclical buffer trick to efficiently extend dense every-frame volumetric fusion of depth maps to function over an unbounded spatial region. Second, overcoming camera pose estimation limitations in a wide variety of environments by combining both dense geometric and photometric camera pose constraints. Third, efficiently updating the dense map according to place recognition and subsequent loop closure constraints by the use of an ‘as-rigid-as-possible' space deformation. We present results on a wide variety of aspects of the system and show through evaluation on de facto standard RGB-D benchmarks that our system performs strongly in terms of trajectory estimation, map quality and computational performance in comparison to other state-of-the-art systems.},
author = {Whelan, Thomas and Kaess, Michael and Johannsson, Hordur and Fallon, Maurice and Leonard, John J. and McDonald, John},
doi = {10.1177/0278364914551008},
file = {:home/chris/Documents/Mendeley Desktop/Real-time large scale dense RGB-D SLAM with volumetric fusion.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {Int. J. Rob. Res.},
keywords = {camera pose estimation,dense methods,gpu,large scale,real-time,rgb-d,slam,volumetric fusion},
number = {4-5},
pages = {598--626},
pmid = {280437},
title = {{Real-time large-scale dense RGB-D SLAM with volumetric fusion}},
url = {http://journals.sagepub.com/doi/10.1177/0278364914551008},
volume = {34},
year = {2015}
}
@article{Whelan2013a,
abstract = {This paper describes extensions to the Kintinu- ous [1] algorithm for spatially extended KinectFusion, incor- porating the following additions: (i) the integration of multiple 6DOF camera odometry estimation methods for robust track- ing; (ii) a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm; (iii) advanced fused real- time surface coloring. These extensions are validated with ex- tensive experimental results, both quantitative and qualitative, demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Whelan, Thomas and Johannsson, Hordur and Kaess, Michael and Leonard, John J. and McDonald, John},
doi = {10.1109/ICRA.2013.6631400},
eprint = {9605103},
file = {:home/chris/Documents/Mendeley Desktop/Robust Real-Time Visual Odometry for Dense RGB-D Mapping.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
number = {i},
pages = {5724--5731},
pmid = {17255001},
primaryClass = {cs},
title = {{Robust real-time visual odometry for dense RGB-D mapping}},
year = {2013}
}
@article{Karlsson2005,
abstract = { This paper presents the Visual Simultaneous Localization and Mapping (vSLAMTM) algorithm, a novel algorithm for simultaneous localization and mapping (SLAM). The algorithm is vision-and odometry-based, and enables low-cost navigation in cluttered and populated environments. No initial map is required, and it satisfactorily handles dynamic changes in the environment, for example, lighting changes, moving objects and/or people. Typically, vSLAM recovers quickly from dramatic disturbances, such as {\{}{\&}{\}}{\{}{\#}{\}}8220;kidnapping{\{}{\&}{\}}{\{}{\#}{\}}8221;. },
author = {Karlsson, Niklas and {Di Bernardo}, Enrico and Ostrowski, Jim and Goncalves, Luis and Pirjanian, Paolo and Munich, Mario E.},
doi = {10.1109/ROBOT.2005.1570091},
file = {:home/chris/Documents/Mendeley Desktop/The vSLAM Algorithm for Robust Localization and Mapping.pdf:pdf},
isbn = {078038914X},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
keywords = {Kalman filter,Mixed proposal distribution,Particle filter,SLAM,Vision},
pages = {24--29},
title = {{The vSLAM algorithm for robust localization and mapping}},
volume = {2005},
year = {2005}
}
@article{Kerl2013,
abstract = {In this paper, we propose a dense visual SLAM method for RGB-D cameras that minimizes both the photometric and the depth error over all pixels. In contrast to sparse, feature-based methods, this allows us to better exploit the available information in the image data which leads to higher pose accuracy. Furthermore, we propose an entropy-based similarity measure for keyframe selection and loop closure detection. From all successful matches, we build up a graph that we optimize using the g2o framework. We evaluated our approach extensively on publicly available benchmark datasets, and found that it performs well in scenes with low texture as well as low structure. In direct comparison to several state-of-the-art methods, our approach yields a significantly lower trajectory error. We release our software as open-source.},
author = {Kerl, Christian and Sturm, Jurgen and Cremers, Daniel},
doi = {10.1109/IROS.2013.6696650},
file = {:home/chris/Documents/Mendeley Desktop/Dense Visual SLAM for RGB-D Cameras.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {2100--2106},
pmid = {6696650},
title = {{Dense visual SLAM for RGB-D cameras}},
year = {2013}
}
@article{Whelan2012,
abstract = {In this paper we present an extension to the KinectFusion algorithm that permits dense mesh-based mapping of extended scale environments in real-time. This is achieved through (i) altering the original algorithm such that the region of space being mapped by the KinectFusion algorithm can vary dynamically, (ii) extracting a dense point cloud from the regions that leave the KinectFusion volume due to this variation, and, (iii) incrementally adding the resulting points to a triangular mesh representation of the environment. The system is implemented as a set of hierarchical multi-threaded components which are capable of operating in real-time. The architecture facilitates the creation and integration of new modules with minimal impact on the performance on the dense volume tracking and surface reconstruction modules. We provide experimental results demonstrating the system's ability to map areas considerably beyond the scale of the original KinectFusion algorithm including a two story apartment and an extended sequence taken from a car at night. In order to overcome failure of the iterative closest point (ICP) based odometry in areas of low geometric features we have evaluated the Fast Odometry from Vision (FOVIS) system as an alternative. We provide a comparison between the two approaches where we show a trade off between the reduced drift of the visual odometry approach and the higher local mesh quality of the ICP-based approach. Finally we present ongoing work on incorporating full simultaneous localisation and mapping (SLAM) pose-graph optimisation. I.},
author = {Whelan, Thomas and Kaess, Michael and Fallon, Maurice},
file = {:home/chris/Documents/Mendeley Desktop/Kintinuous$\backslash$: Spatially Extended KinectFusion.pdf:pdf},
journal = {RSS Work. RGB-D Adv. Reason. with Depth Cameras},
keywords = {H. Johannsson,J.J. Leonard and J.B. McDonald,M. Kaess,M.F. Fallon,T. Whelan},
pages = {7},
title = {{Kintinuous: Spatially extended kinectfusion}},
year = {2012}
}
@article{Whelan2013,
abstract = {In this paper we present a system for capturing large scale dense maps in an online setting with a low cost RGB-D sensor. Central to this work is the use of an "as-rigid-as-possible" space deformation for efficient dense map correction in a pose graph optimisation framework. By combining pose graph optimisation with non-rigid deformation of a dense map we are able to obtain highly accurate dense maps over large scale trajectories that are both locally and globally consistent. With low latency in mind we derive an incremental method for deformation graph construction, allowing multi-million point maps to be captured over hundreds of metres in real-time. We provide benchmark results on a well established RGB-D SLAM dataset demonstrating the accuracy of the system and also provide a number of our own datasets which cover a wide range of environments, both indoors, outdoors and across multiple floors.},
author = {Whelan, Thomas and Kaess, Michael and Leonard, John J. and McDonald, John},
doi = {10.1109/IROS.2013.6696405},
file = {:home/chris/Documents/Mendeley Desktop/Deformation-based Loop Closure for Large Scale Dense RGB-D SLAM.pdf:pdf},
isbn = {978-1-4673-6358-7},
issn = {2153-0858},
journal = {2013 IEEE/RSJ Int. Conf. Intell. Robot. Syst.},
pages = {548--555},
title = {{Deformation-based loop closure for large scale dense RGB-D SLAM}},
url = {http://ieeexplore.ieee.org/document/6696405/},
year = {2013}
}
@article{Klingensmith2016,
abstract = {A robot with a hand-mounted depth sensor scans a scene. When the robot's joint angles are not known with certainty, how can it best reconstruct the scene? In this work, we simultaneously estimate the joint angles of the robot and reconstruct a dense volumetric model of the scene. In this way, we perform simultaneous localization and mapping in the configuration space of the robot, rather than in the pose space of the camera. We show using simulations and robot experiments that our approach greatly reduces both 3D reconstruction error and joint angle error over simply using the forward kinematics. Unlike other approaches, ours directly reasons about robot joint angles, and can use these to constrain the pose of the sensor. Because of this, it is more robust to missing or ambiguous depth data than approaches that are unconstrained by the robot's kinematics.},
author = {Klingensmith, Matthew and Sirinivasa, Siddartha S. and Kaess, Michael},
doi = {10.1109/LRA.2016.2518242},
file = {:home/chris/Documents/Mendeley Desktop/Articulated Robot Motion for SimultaneousLocalization and Mapping (ARM-SLAM).pdf:pdf},
isbn = {9781467380256},
issn = {23773766},
journal = {IEEE Robot. Autom. Lett.},
keywords = {Kinematics,Mapping,RGBD Perception,SLAM,Visual-Based navigation},
number = {2},
pages = {1156--1163},
title = {{Articulated Robot Motion for Simultaneous Localization and Mapping (ARM-SLAM)}},
volume = {1},
year = {2016}
}
@article{Siam2017,
abstract = {— Loop closure detection or place recognition is a fundamental problem in robot simultaneous localization and mapping (SLAM). SeqSLAM is considered to be one of the most successful algorithms for loop closure detection as it has been demonstrated to be able to handle significant environmental condition changes including those due to illumination, weather, and time of the day. However, SeqSLAM relies heavily on exhaustive sequence matching, a computationally expensive process that prevents the algorithm from being used in dealing with large maps. In this paper, we propose Fast-SeqSLAM, an efficient version of SeqSLAM. Fast-SeqSLAM has a much reduced time complexity without degrading the accuracy, and this is achieved by using an approximate nearest neighbor (ANN) algorithm to match the current image with those in the robot map and extending the idea of SeqSLAM to greedily search a sequence of images that best match with the current sequence. We demonstrate the effectiveness of our Fast-SeqSLAM algorithm in appearance based loop closure detection.},
author = {Siam, Sayem Mohammad and Zhang, Hong},
doi = {10.1109/ICRA.2017.7989671},
file = {:home/chris/Documents/Mendeley Desktop/Fast-SeqSLAM$\backslash$: A Fast Appearance Based Place Recognition Algorithm.pdf:pdf},
isbn = {9781509046324},
issn = {10504729},
journal = {2017 IEEE Int. Conf. Robot. Autom.},
pages = {5702--5708},
title = {{Fast-SeqSLAM: A Fast Appearance Based Place Recognition Algorithm}},
url = {http://ieeexplore.ieee.org/document/7989671/},
year = {2017}
}
@article{Olson2011,
abstract = {While the use of naturally-occurring features is a central focus of machine perception, artificial features (fiducials) play an important role in creating controllable experiments, ground truthing, and in simplifying the development of systems where perception is not the central objective. We describe a new visual fiducial system that uses a 2D bar code style {\&}{\#}x201C;tag{\&}{\#}x201D;, allowing full 6 DOF localization of features from a single image. Our system improves upon previous systems, incorporating a fast and robust line detection system, a stronger digital coding system, and greater robustness to occlusion, warping, and lens distortion. While similar in concept to the ARTag system, our method is fully open and the algorithms are documented in detail.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Olson, Edwin},
doi = {10.1109/ICRA.2011.5979561},
eprint = {arXiv:1011.1669v3},
file = {:home/chris/Documents/Mendeley Desktop/AprilTag$\backslash$: A robust and flexible visual fiducial system.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3400--3407},
pmid = {15991970},
title = {{AprilTag: A robust and flexible visual fiducial system}},
year = {2011}
}
@article{Wang2016,
abstract = {AprilTags and other passive fiducial markers require specialized algorithms to detect markers among other features in a natural scene. The vision processing steps generally dominate the computation time of a tag detection pipeline, so even small improvements in marker detection can translate to a faster tag detection system. We incorporated lessons learned from implementing and supporting the AprilTag system into this improved system. This work describes AprilTag 2, a completely redesigned tag detector that improves robustness and efficiency compared to the original AprilTag system. The tag coding scheme is unchanged, retaining the same robustness to false positives inherent to the coding system. The new detector improves performance with higher detection rates, fewer false positives, and lower computational time. Improved performance on small images allows the use of decimated input images, resulting in dramatic gains in detection speed.},
author = {Wang, John and Olson, Edwin},
doi = {10.1109/IROS.2016.7759617},
file = {:home/chris/Documents/Mendeley Desktop/AprilTag 2$\backslash$: Efficient and robust fiducial detection.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {4193--4198},
title = {{AprilTag 2: Efficient and robust fiducial detection}},
volume = {2016-Novem},
year = {2016}
}
@article{DeGregorio2017,
abstract = {We present a novel mapping framework for robot navigation which features a multi-level querying system capable to obtain rapidly representations as diverse as a 3D voxel grid, a 2.5D height map and a 2D occupancy grid. These are inherently embedded into a memory and time efficient core data structure organized as a Tree of SkipLists. Compared to the well-known Octree representation, our approach exhibits a better time efficiency, thanks to its simple and highly parallelizable computational structure, and a similar memory footprint when mapping large workspaces. Peculiarly within the realm of mapping for robot navigation, our framework supports realtime erosion and re-integration of measurements upon reception of optimized poses from the sensor tracker, so as to improve continuously the accuracy of the map.},
archivePrefix = {arXiv},
arxivId = {1704.05832},
author = {{De Gregorio}, Daniele and {Di Stefano}, Luigi},
doi = {10.1109/ICRA.2017.7989299},
eprint = {1704.05832},
file = {:home/chris/Documents/Mendeley Desktop/De Gregorio, Di Stefano - 2017 - SkiMap An Efficient Mapping Framework for Robot Navigation.pdf:pdf;:home/chris/Documents/Mendeley Desktop/SkiMap$\backslash$: An Efficient Mapping Framework for Robot Navigation.pdf:pdf},
isbn = {9781509046324},
issn = {10504729},
title = {{SkiMap: An Efficient Mapping Framework for Robot Navigation}},
url = {http://arxiv.org/abs/1704.05832},
year = {2017}
}
@article{Wang2012,
abstract = {The Extend Kalman Filter based algorithm for simultaneous localization and mapping cannot satisfy the requirement of real time map updating because of the increasing number of landmarks and the heavy calculating cost while AUV working for long time endurance. The Compressed EKF based SLAM is introduced in this paper. And the method of map management and the local map switch strategy are addressed, which divide the AUV navigating area into several local sub-maps. The navigation error calculating based on landmarks in sub-map is completed in local area by using Extend Kalman filter, and the global map updating is done only when the condition satisfied the switch rule of the sub-map. Finally the CEKF-SLAM based navigating method is tested with the trial data, and by comparing with the dead reckoning navigating result, the test results show that the navigation error of CEKF-SLAM algorithm is less than that of dead reckoning algorithm, and on the same time, the former reduces the calculation cost for AUV navigation.},
author = {Wang, Hongjian and Li, Cun and Lv, Hongli and Chen, Xinghua},
doi = {10.1109/ITSC.2012.6338834},
file = {:home/chris/Documents/Mendeley Desktop/Research on Compressed EKF Based SLAM Algorithm for Unmanned Underwater Vehicle.pdf:pdf},
isbn = {9781467330640},
issn = {2153-0009},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
number = {86},
pages = {1402--1406},
title = {{Research on compressed EKF based SLAM algorithm for unmanned underwater vehicle}},
year = {2012}
}
@article{Steckel2013,
abstract = {We propose to combine a biomimetic navigation model which solves a simultaneous localization and mapping task with a biomimetic sonar mounted on a mobile robot to address two related questions. First, can robotic sonar sensing lead to intelligent interactions with complex environments? Second, can we model sonar based spatial orientation and the construction of spatial maps by bats? To address these questions we adapt the mapping module of RatSLAM, a previously published navigation system based on computational models of the rodent hippocampus. We analyze the performance of the proposed robotic implementation operating in the real world. We conclude that the biomimetic navigation model operating on the information from the biomimetic sonar allows an autonomous agent to map unmodified (office) environments efficiently and consistently. Furthermore, these results also show that successful navigation does not require the readings of the biomimetic sonar to be interpreted in terms of individual objects/landmarks in the environment. We argue that the system has applications in robotics as well as in the field of biology as a simple, first order, model for sonar based spatial orientation and map building.},
author = {Steckel, Jan and Peremans, Herbert},
doi = {10.1371/journal.pone.0054076},
file = {:home/chris/Documents/Mendeley Desktop/BatSLAM$\backslash$: Simultaneous Localization and Mapping Using Biomimetic Sonar.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS One},
number = {1},
pmid = {23365647},
title = {{BatSLAM: Simultaneous Localization and Mapping Using Biomimetic Sonar}},
volume = {8},
year = {2013}
}
@article{Steckel2015,
abstract = {In this paper, we present a solution to the Simultaneous Localization and Mapping problem by combining a novel 3D in-air sonar sensor with techniques for estimating the egomotion of a mobile platform (called acoustic flow odometry) and a bio-inspired mapping module (called BatSLAM). This combination eliminates the need for odometric information originating from the robot's motor controller, enabling applications where that information is difficult to obtain, e.g. many electric wheelchairs. The proposed method exploits the programmable spatial sampling capabilities of the 3D sonar system to derive from the same set of received echo signals both an accurate representation of reflectors in the horizontal plane (2D energyscape) and a more coarse representation of reflectors in the frontal hemisphere (3D energyyscape). Using a mapping experiment we demonstrate that the motion estimates originating from the acoustic flow module combined with the coarse frontal hemisphere data provide sufficient information for the mapping module to reconstruct the robot's trajectory.},
author = {Steckel, Jan and Peremans, Herbert},
doi = {10.1109/IROS.2015.7353452},
file = {:home/chris/Documents/Mendeley Desktop/Spatial sampling strategy for a 3D Sonar Sensor supporting BatSLAM.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
keywords = {Acoustics,Arrays,Microphones,Robot sensing systems,Sonar,Three-dimensional displays},
pages = {723--728},
title = {{Spatial sampling strategy for a 3D sonar sensor supporting BatSLAM}},
volume = {2015-Decem},
year = {2015}
}
@article{Dong2017,
abstract = {Continuous-time trajectory representations are a powerful tool that can be used to address several issues in many practical simultaneous localization and mapping (SLAM) scenarios, like continuously collected measurements distorted by robot motion, or during with asynchronous sensor measurements. Sparse Gaussian processes (GP) allow for a probabilistic non-parametric trajectory representation that enables fast trajectory estimation by sparse GP regression. However, previous approaches are limited to dealing with vector space representations of state only. In this technical report we extend the work by Barfoot et al. [1] to general matrix Lie groups, by applying constant-velocity prior, and defining locally linear GP. This enables using sparse GP approach in a large space of practical SLAM settings. In this report we give the theory and leave the experimental evaluation in future publications.},
archivePrefix = {arXiv},
arxivId = {1705.06020},
author = {Dong, Jing and Boots, Byron and Dellaert, Frank},
eprint = {1705.06020},
file = {:home/chris/Documents/Mendeley Desktop/Sparse Gaussian Processes for Continuous-Time Trajectory Estimationon Matrix Lie Groups.pdf:pdf},
title = {{Sparse Gaussian Processes for Continuous-Time Trajectory Estimation on Matrix Lie Groups}},
url = {http://arxiv.org/abs/1705.06020},
year = {2017}
}
@article{Russo2014,
abstract = {Computer vision approaches are increasingly used in mobile robotic systems, since they allow to obtain a very good representation of the environment by using low-power and cheap sensors. In particular it has been shown that they can compete with standard solutions based on laser range scanners when dealing with the problem of simultaneous localization and mapping (SLAM), where the robot has to explore an unknown environment while building a map of it and localizing in the same map. We present a package for simultaneous localization and mapping in ROS (Robot Operating System) using a monocular camera sensor only. Experimental results in real scenarios as well as on standard datasets show that the algorithm is able to track the trajectory of the robot and build a consistent map of small environments, while running in near real-time on a standard PC.},
author = {Russo, Ludovico and Rosa, Stefano and Bona, Basilio and Matteucci, Matteo},
doi = {10.1.1.401.8518},
file = {:home/chris/Documents/Mendeley Desktop/A{\_}ROS{\_}Implementation{\_}of{\_}the{\_}Mono-Slam{\_}Algorithm.pdf:pdf},
journal = {Int. J. Comput. Vis.},
pages = {339--351},
title = {{A ROS Implementation of the Mono-Slam Algorithm}},
url = {http://www.airccj.org/CSCP/vol4/csit41831.pdf},
year = {2014}
}
@unpublished{Pirovano2012,
author = {Pirovano, Michele},
file = {:home/chris/Documents/Mendeley Desktop/Kinfu – an open source implementation of KinectFusion + case study $\backslash$: implementing a 3D scanner with PCL.pdf:pdf},
number = {October 2011},
title = {{Kinfu - an open source implementation of Kinect Fusion + case study: implementing a 3D scanner with PCL}},
url = {https://homes.di.unimi.it/borghese/Teaching/IntelligentSystems/Documents/PirovanoMichele-VisualReconstructionReport.pdf},
year = {2012}
}
@article{He2011,
abstract = {Mobile autonomous systems are very important for marine scientific investigation and military applications. Many algorithms have been studied to deal with the computational efficiency problem required for large scale simultaneous localization and mapping (SLAM) and its related accuracy and consistency. Among these methods, submap-based SLAM is a more effective one. By combining the strength of two popular mapping algorithms, the Rao-Blackwellised particle filter (RBPF) and extended information filter (EIF), this paper presents a combined SLAM-an efficient submap-based solution to the SLAM problem in a large scale environment. RBPF-SLAM is used to produce local maps, which are periodically fused into an EIF-SLAM algorithm. RBPF-SLAM can avoid linearization of the robot model during operating and provide a robust data association, while EIF-SLAM can improve the whole computational speed, and avoid the tendency of RBPF-SLAM to be over-confident. In order to further improve the computational speed in a real time environment, a binary-tree-based decision-making strategy is introduced. Simulation experiments show that the proposed combined SLAM algorithm significantly outperforms currently existing algorithms in terms of accuracy and consistency, as well as the computing efficiency. Finally, the combined SLAM algorithm is experimentally validated in a real environment by using the Victoria Park dataset.},
author = {He, Bo and Zhang, Shujing and Yan, Tianhong and Zhang, Tao and Liang, Yan and Zhang, Hongjin},
doi = {10.3390/s111110197},
file = {:home/chris/Documents/Mendeley Desktop/A Novel Combined SLAM Based on RBPF-SLAM and EIF-SLAM for Mobile System Sensing in a Large Scale Environment.pdf:pdf},
isbn = {1424-8220},
issn = {14248220},
journal = {Sensors},
keywords = {Computational efficiency,Consistency,Eif-slam,Rbpf-slam,Submap},
number = {11},
pages = {10197--10219},
pmid = {22346639},
title = {{A novel combined SLAM based on RBPF-SLAM and EIF-SLAM for mobile system sensing in a large scale environment}},
volume = {11},
year = {2011}
}
@article{AuatCheein2011,
abstract = {Precision agricultural maps are required for agricultural machinery navigation, path planning and plantation supervision. In this work we present a Simultaneous Localization and Mapping (SLAM) algorithm solved by an Extended Information Filter (EIF) for agricultural environments (olive groves). The SLAM algorithm is implemented on an unmanned non-holonomic car-like mobile robot. The map of the environment is based on the detection of olive stems from the plantation. The olive stems are acquired by means of both: a range sensor laser and a monocular vision system. A support vector machine (SVM) is implemented on the vision system to detect olive stems on the images acquired from the environment. Also, the SLAM algorithm has an optimization criterion associated with it. This optimization criterion is based on the correction of the SLAM system state vector using only the most meaningful stems - from an estimation convergence perspective - extracted from the environment information without compromising the estimation consistency. The optimization criterion, its demonstration and experimental results within real agricultural environments showing the performance of our proposal are also included in this work. {\textcopyright} 2011 Elsevier B.V.},
author = {{Auat Cheein}, F. and Steiner, G. and {Perez Paina}, G. and Carelli, R.},
doi = {10.1016/j.compag.2011.07.007},
file = {:home/chris/Documents/Mendeley Desktop/Optimized EIF-SLAM algorithm for precision agriculture mapping based on stems detection.pdf:pdf},
isbn = {0168-1699},
issn = {01681699},
journal = {Comput. Electron. Agric.},
keywords = {Agricultural mapping,Mobile robot,SLAM},
number = {2},
pages = {195--207},
publisher = {Elsevier B.V.},
title = {{Optimized EIF-SLAM algorithm for precision agriculture mapping based on stems detection}},
url = {http://dx.doi.org/10.1016/j.compag.2011.07.007},
volume = {78},
year = {2011}
}
@article{Dai2017,
abstract = {Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed reality and robotic applications. However, scalability brings challenges of drift in pose estimation, introducing significant errors in the accumulated model. Approaches often require hours of offline processing to globally correct model errors. Recent online methods demonstrate compelling results, but suffer from: (1) needing minutes to perform online correction preventing true real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation resulting in many tracking failures; or (3) supporting only unstructured point-based representations, which limit scan quality and applicability. We systematically address these issues with a novel, real-time, end-to-end reconstruction framework. At its core is a robust pose estimation strategy, optimizing per frame for a global set of camera poses by considering the complete history of RGB-D input with an efficient hierarchical approach. We remove the heavy reliance on temporal tracking, and continually localize to the globally optimized frames instead. We contribute a parallelizable optimization framework, which employs correspondences based on sparse features and dense geometric and photometric matching. Our approach estimates globally optimized (i.e., bundle adjusted) poses in real-time, supports robust tracking with recovery from gross tracking failures (i.e., relocalization), and re-estimates the 3D model in real-time to ensure global consistency; all within a single framework. Our approach outperforms state-of-the-art online systems with quality on par to offline methods, but with unprecedented speed and scan completeness. Our framework leads to a comprehensive online scanning solution for large indoor environments, enabling ease of use and high-quality results.},
archivePrefix = {arXiv},
arxivId = {1604.01093},
author = {Dai, Angela and Nie{\ss}ner, Matthias and Zollh{\"{o}}fer, Michael and Izadi, Shahram and Theobalt, Christian},
doi = {10.1145/nnnnnnn.nnnnnnn},
eprint = {1604.01093},
file = {:home/chris/Documents/Mendeley Desktop/BundleFusion$\backslash$: Real-Time Globally Consistent 3D Reconstruction Using On-the-Fly Surface Reintegration.pdf:pdf},
isbn = {9781450335492},
issn = {15232867},
number = {3},
pmid = {397311},
title = {{BundleFusion: Real-time Globally Consistent 3D Reconstruction using On-the-fly Surface Re-integration}},
url = {http://arxiv.org/abs/1604.01093},
volume = {36},
year = {2017}
}
@article{Newcombe2011a,
abstract = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
author = {Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
doi = {10.1109/ISMAR.2011.6092378},
file = {:home/chris/Documents/Mendeley Desktop/KinectFusion$\backslash$: Real-Time Dense Surface Mapping and Tracking.pdf:pdf},
isbn = {9781457721830},
issn = {{\textless}null{\textgreater}},
journal = {2011 10th IEEE Int. Symp. Mix. Augment. Reality, ISMAR 2011},
keywords = {AR,Dense Reconstruction,Depth Cameras,GPU,Real-Time,SLAM,Tracking,Volumetric Representation},
pages = {127--136},
pmid = {6162880},
title = {{KinectFusion: Real-time dense surface mapping and tracking}},
year = {2011}
}
@article{Izadi2011,
abstract = {Abstract KinectFusion enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct , geometrically precise, ... $\backslash$n},
author = {Izadi, S and Kim and Hilliges, O and Molyneaux, D and Newcombe, R and Kohli, P and Shotton, J and Hodges, S and Freeman, D and Davison, A and Fitzgibbon, A},
doi = {10.1145/2047196.2047270},
file = {:home/chris/Documents/Mendeley Desktop/KinectFusion$\backslash$: Real-time 3D Reconstruction and InteractionUsing a Moving Depth Camera.pdf:pdf},
isbn = {9781450307161},
issn = {9781450307161},
journal = {Proc. 24th Annu. ACM User Interface Softw. Technol. Symp. - UIST '11},
keywords = {3d,all or part of,ar,depth cameras,geometry-aware interactions,gpu,or hard copies of,permission to make digital,physics,research cambridge,research conducted at microsoft,surface reconstruction,this work for,tracking,uk},
pages = {559--568},
pmid = {6162880},
title = {{KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera}},
url = {http://dl.acm.org/citation.cfm?id=2047270{\%}5Cnpapers://c80d98e4-9a96-4487-8d06-8e1acc780d86/Paper/p5008},
year = {2011}
}
@article{Eliazar2004,
abstract = {Probabilistic approaches have proved very successful at addressing the basic problems of robot localization and mapping and they have shown great promise on the combined problem of simultaneous localization and mapping (SLAM). One approach to SLAM assumes relatively sparse, relatively unambiguous landmarks and builds a Kalman filter over landmark positions. Other approaches assume dense sensor data which individually are not very distinctive, such as those available from a laser range finder. In earlier work, we presented an algorithm called DP-SLAM, which provided a very accurate solution to the latter case by efficiently maintaining a joint distribution over robot maps and poses. The approach assumed an extremely accurate laser range finder and a deterministic environment. In this work we demonstrate an improved map representation and laser penetration model, an improvement in the asymptotic efficiency of the algorithm, and empirical results of loop closing on a high resolution map of a very challenging domain.},
author = {a.I. Eliazar and Parr, R.},
doi = {10.1109/ROBOT.2004.1308006},
file = {:home/chris/Documents/Mendeley Desktop/DP-SLAM2-0.pdf:pdf},
isbn = {0-7803-8232-3},
issn = {1050-4729},
journal = {IEEE Int. Conf. Robot. Autom. 2004. Proceedings. ICRA '04. 2004},
pages = {1314--1320},
title = {{Dp-Slam 2.0}},
volume = {2},
year = {2004}
}
@article{Davison2007,
abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the "pure vision" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to Structure from Motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera.},
author = {Davison, Andrew and Reid, Ian and Molton, Nicholas and Stasse, Olivier},
doi = {10.1109/TPAMI.2007.1049},
file = {:home/chris/Documents/Mendeley Desktop/MonoSLAM$\backslash$: Real-Time Single Camera SLAM.pdf:pdf},
issn = {0162-8828},
journal = {Pattern Anal. Mach. Intell. (PAMI), IEEE Trans.},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Computer Systems,Computer-Assisted,Computer-Assisted: methods,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Information Storage and Retrieval,Information Storage and Retrieval: methods,Numerical Analysis,Pattern Recognition,Photogrammetry,Photogrammetry: methods,Signal Processing,Three-Dimensional,Three-Dimensional: methods,Video Recording,Video Recording: methods},
number = {6},
pages = {1052--67},
pmid = {17431302},
title = {{MonoSLAM: real-time single camera SLAM.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17431302},
volume = {29},
year = {2007}
}
@article{Yan2017,
abstract = {Recent work on simultaneous trajectory estimation and mapping (STEAM) for mobile robots has used Gaussian processes (GPs) to efficiently represent the robot's trajectory through its environment. GPs have several advantages over discrete-time trajectory representations: they can represent a continuous-time trajectory, elegantly handle asynchronous and sparse measurements, and allow the robot to query the trajectory to recover its estimated position at any time of interest. A major drawback of the GP approach to STEAM is that it is formulated as a batch trajectory estimation problem. In this paper we provide the critical extensions necessary to transform the existing GP-based batch algorithm for STEAM into an extremely efficient incremental algorithm. In particular, we are able to vastly speed up the solution time through efficient variable reordering and incremental sparse updates, which we believe will greatly increase the practicality of Gaussian process methods for robot mapping and localization. Finally, we demonstrate the approach and its advantages on both synthetic and real datasets.},
archivePrefix = {arXiv},
arxivId = {1504.02696},
author = {Yan, Xinyan and Indelman, Vadim and Boots, Byron},
doi = {10.1016/j.robot.2016.10.004},
eprint = {1504.02696},
file = {:home/chris/Documents/Mendeley Desktop/Incremental Sparse GP Regression for Continuous-time Trajectory Estimation {\&} Mapping.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {Continuous time,Gaussian process regression,Localization,SLAM,State estimation},
pages = {120--132},
title = {{Incremental sparse GP regression for continuous-time trajectory estimation and mapping}},
volume = {87},
year = {2017}
}
@inproceedings{Endres2012a,
abstract = {Page 1. An Evaluation of the RGB-D SLAM System Felix Endres1 J{\"{u}}rgen Hess1 Nikolas Engelhard1 J{\"{u}}rgen Sturm2 Daniel Cremers2 Wolfram Burgard1 Abstract—We present an approach to simultaneous local- ization and ... $\backslash$n},
author = {Endres, F and Hess, J and Engelhard, N and {\ldots}, J Sturm},
booktitle = {IEEE Int. Conf. Robot. Autom.},
doi = {10.1109/ICRA.2012.6225199},
file = {:home/chris/Documents/Mendeley Desktop/An Evaluation of the RGB-D SLAM System.pdf:pdf},
isbn = {9781467314046},
issn = {978-1-4673-1403-9},
number = {c},
pages = {1691--1696},
title = {{An evaluation of the RGB-D SLAM system}},
volume = {3},
year = {2012}
}
@inproceedings{Endres2012,
abstract = {In this article we present a novel mapping system that robustly generates highly accurate 3D maps using an RGB-D camera. Our approach does not require any further sensors or odometry. With the availability of low-cost and light-weight RGB-D sensors such as the Microsoft Kinect, our approach applies to small domestic robots such as vacuum cleaners as well as flying robots such as quadrocopters. Furthermore, our system can also be used for free-hand reconstruction of detailed 3D models. In addition to the system itself, we present a thorough experimental evaluation on a publicly available bench- mark dataset. We analyze and discuss the influence of several parameters such as the choice of the feature descriptor, the number of visual features, and validation methods. The results of the experiments demonstrate that our system can robustly deal with challenging scenarios such as fast cameras motions and feature-poor environments while being fast enough for online operation. Our system is fully available as open-source and has already been widely adopted by the robotics community.},
author = {Endres, Felix and Hess, Jurgen J{\"{u}}rgen and Sturm, J{\"{u}}rgen Jurgen J{\"{u}}rgen and Cremers, Daniel and Burgard, Wolfram},
booktitle = {IEEE Trans. Robot.},
doi = {10.1109/TRO.2013.2279412},
file = {:home/chris/Documents/Mendeley Desktop/3D Mapping with an RGB-D Camera.pdf:pdf},
issn = {1552-3098},
number = {1},
pages = {1--11},
title = {{3D Mapping with an RGB-D Camera}},
volume = {30},
year = {2012}
}
@article{Faugeras1988,
author = {Faugeras, O.D. and Lustman, F.},
doi = {10.1142/S0218001488000285},
file = {:home/chris/Documents/Mendeley Desktop/Motion and structure from motion in a piecewise planar environment.pdf:pdf},
issn = {0218-0014},
journal = {Int. J. Pattern Recognit. Artif. Intell.},
number = {03},
pages = {485--508},
title = {{Motion and Structure From Motion in a Piecewise Planar Environment}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001488000285},
volume = {02},
year = {1988}
}
@article{Garcia2011,
author = {Garcia, Rafael and Gracias, Nuno},
doi = {978-1-61284-4577-0088-0/11/$26.00},
file = {:home/chris/Documents/Mendeley Desktop/Detection of interest points in turbid underwater images.pdf:pdf},
title = {{Detection of Interest Points in Turbid Underwater Images}},
year = {2011}
}
@article{Song2014,
author = {Song, Dalei and Sun, Weicheng and Ji, Zehui and Hou, Guojia and Li, Xiufang and Liu, Liang},
doi = {10.1109/InfoSEEE.2014.6947890},
file = {:home/chris/Documents/Mendeley Desktop/Color Model Selection for Underwater Object Recognition.pdf:pdf},
isbn = {978-1-4799-3197-2},
journal = {2014 Int. Conf. Inf. Sci. Electron. Electr. Eng.},
keywords = {and engineering,college of information science,color model,guojia hou,illumination invariant,liang liu,primary colors,underwater object recognition,xiufang li},
pages = {1339--1342},
title = {{Color model selection for underwater object recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6947890},
year = {2014}
}
@article{Shiela2005,
author = {Shiela, Ma and Marcos, Angeli C and Soriano, Maricor N and Saloma, Caesar A},
file = {:home/chris/Documents/Mendeley Desktop/Classification of coral reef images from underwater video using neural networks.pdf:pdf},
number = {22},
pages = {8766--8771},
title = {{Classification of coral reef images from underwater video using neural networks}},
volume = {13},
year = {2005}
}
@article{Gong,
archivePrefix = {arXiv},
arxivId = {arXiv:1607.05967v1},
author = {Gong, Han and Finlayson, Graham},
eprint = {arXiv:1607.05967v1},
file = {:home/chris/Documents/Mendeley Desktop/Interactive Illumination Invariance.pdf:pdf},
title = {{Interactive Illumination Invariance}}
}
@article{Maimone2005,
abstract = {NASA's Mars Exploration Rovers (MER) were designed to traverse in Viking Lander-I style terrains: mostly flat, with many small non-obstacle rocks and occasional obstacles. During actual operations in such terrains, on- board position estimates derived solely from the onboard In- ertial Measurement Unit and wheel encoder-based odome- try achieved well within the design goal of at most 10{\%} er- ror. However, MER vehicles were also driven along slippery slopes tilted as high as 31 degrees. In such conditions an ad- ditional capability was employed to maintain a sufficiently accurate onboard position estimate: Visual Odometry. The MER Visual Odometry system comprises onboard software for comparing stereo pairs taken by the pointable mast-mounted 45 degree FOV Navigation cameras (NAV- CAMs). The system computes an update to the 6-DOF rover pose (x, y, z, roll, pitch, yaw) by tracking the motion of autonomously-selected ”interesting” terrain features be- tween two pairs of stereo images, in both 2D pixel and 3D world coordinates. A maximum likelihood estimator is ap- plied to the computed 3D offsets to produce a final, corrected estimate of vehicle motion between the two pairs. In this paper we describe the Visual Odometry algorithm used on the Mars Exploration Rovers, and summarize its re- sults from the first year of operations on Mars.},
author = {Maimone, Mark and Cheng, Yang and Matthies, Larry},
doi = {10.1002/rob.20184},
file = {:home/chris/Documents/Mendeley Desktop/Two Years of Visual Odometry on the Mars Exploration Rovers.pdf:pdf},
isbn = {0-7803-9298-1},
issn = {1070-9932},
journal = {2005 IEEE Int. Conf. Syst. Man Cybern.},
pages = {903--910},
title = {{Visual Odometry on the Mars Exploration Rovers}},
url = {http://ieeexplore.ieee.org/document/1571261/},
volume = {1},
year = {2007}
}
@article{Kaeli2013,
author = {Kaeli, Jeffrey W and Engineering, Mechanical and Tech, Virginia},
file = {:home/chris/Documents/Mendeley Desktop/Computational Strategies for Understanding Underwater Optical Image Datasets.pdf:pdf},
number = {2007},
title = {{Computational Strategies for Understanding Underwater Optical Image Datasets by}},
year = {2013}
}
@article{Science2012,
author = {Science, Computer and Science, Computer},
file = {:home/chris/Documents/Mendeley Desktop/LBP-SURF Descriptor with Color Invariant and Texture Based Features for Underwater Images.pdf:pdf},
isbn = {9781450316606},
keywords = {cs-,feature description,feature detection,sift,surf},
title = {{LBP-SURF Descriptor with Color Invariant and Texture Based Features for Underwater Images}},
year = {2012}
}
@article{Noriega,
author = {Noriega, Philippe and Bernier, Olivier and Marzin, Pierre},
file = {:home/chris/Documents/Mendeley Desktop/Real Time Illumination Invariant Background Subtraction Using Local Kernel Histograms.pdf:pdf},
pages = {1--10},
title = {{Real Time Illumination Invariant Background Subtraction Using Local Kernel Histograms}}
}
@article{Park,
abstract = {— Direct visual odometry and Simultaneous Lo-calization and Mapping (SLAM) methods determine camera poses by means of direct image alignment. This optimizes a photometric cost term based on the Lucas-Kanade method. Many recent works use the brightness constancy assumption in the alignment cost formulation and therefore cannot cope with significant illumination changes. Such changes are especially likely to occur for loop closures in SLAM. Alternatives exist which attempt to match images more robustly. In our paper, we perform a systematic evaluation of real-time capable methods. We determine their accuracy and robustness in the context of odometry and of loop closures, both on real images as well as synthetic datasets with simulated lighting changes. We find that for real images, a Census-based method outperforms the others. We make our new datasets available online 3 .},
author = {Park, Seonwook and Sch{\"{o}}ps, Thomas and Pollefeys, Marc},
file = {:home/chris/Documents/Mendeley Desktop/Illumination Change robustness in Direct Visual SLAM.pdf:pdf},
isbn = {9781509046324},
title = {{Illumination Change Robustness in Direct Visual SLAM}}
}
@article{Kerl,
author = {Kerl, Christian and Souiai, Mohamed and Cremers, Daniel},
file = {:home/chris/Documents/Mendeley Desktop/Towards Illumination-invariant 3D Reconstruction using ToF RGB-D Cameras.pdf:pdf},
title = {{Towards Illumination-invariant 3D Reconstruction using ToF RGB-D Cameras}}
}
@article{Dubbelman2010,
abstract = {In robotic applications the absolute pose is often obtained as the integral of successive relative rigid-body motions. As each relative rigid-body motion is typically the product of statistical inference, the integrated absolute pose will exhibit error build-up and the estimated trajectory will differ from the true trajectory undertaken by the system. Some application areas allow the system to receive additional information about its current absolute pose, for example from loop detection, which is more accurate than the integral of the relative rigid-body motions. The availability of this absolute information is usually less frequent than the information underlying the relative rigid-body motions. This contribution addresses an efficient closed form algorithm which minimally bends a trajectory such that the integrated pose is exactly equal to any particular desired pose. The manner in which the bending is distributed over the trajectory is controllable using weights. The proposed method will be compared against a maximum likelihood solution on simulated trajectories as well as on trajectories estimated from binocular and monocular data. The results indicate that the performance differences between the closed form approach and the maximum likelihood solution are negligible while the closed form approach is significantly more efficient.},
author = {Dubbelman, Gijs and Esteban, Isaac and Schutte, Klamer},
doi = {10.1109/IROS.2010.5652656},
file = {:home/chris/Documents/Mendeley Desktop/Dubbelman, Esteban, Schutte - 2010 - Efficient Trajectory Bending with Applications to Loop Closure.pdf:pdf},
isbn = {9781424466757},
issn = {2153-0858},
journal = {IEEE/RSJ 2010 Int. Conf. Intell. Robot. Syst. IROS 2010 - Conf. Proc.},
keywords = {Loop closure,Visual odometry,Visual reconstruction},
pages = {4836--4842},
title = {{Efficient trajectory bending with applications to loop closure}},
year = {2010}
}
@article{Felzenszwalb,
author = {Felzenszwalb, Pedro F and Huttenlocher, Daniel P},
file = {:home/chris/Documents/Mendeley Desktop/Felzenszwalb, Huttenlocher - Unknown - Efficient Graph-Based Image Segmentation.pdf:pdf},
keywords = {clustering,graph algorithm,image segmentation,perceptual organization},
pages = {1--26},
title = {{Efficient Graph-Based Image Segmentation}}
}
@article{Maimone,
author = {Maimone, Mark and Cheng, Yang and Matthies, Larry},
file = {:home/chris/Documents/Mendeley Desktop/Two Years of Visual Odometry on the Mars Exploration Rovers.pdf:pdf},
title = {{Two Years of Visual Odometry on the Mars Exploration Rovers}}
}
@article{Kummerle2011,
author = {K{\"{u}}mmerle, Rainer and Rainer, K and Grisetti, Giorgio and Konolige, Kurt},
doi = {10.1109/ICRA.2011.5979949},
file = {:home/chris/Documents/Mendeley Desktop/K{\"{u}}mmerle et al. - 2011 - G2o A general framework for graph optimization g 2 o A General Framework for Graph Optimization.pdf:pdf},
number = {September 2014},
title = {{G2o : A general framework for graph optimization g 2 o : A General Framework for Graph Optimization}},
year = {2011}
}
@article{Niko2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.04158v3},
author = {Niko, S and Jul, R O},
eprint = {arXiv:1501.04158v3},
file = {:home/chris/Documents/Mendeley Desktop/Niko, Jul - 2015 - On the Performance of ConvNet Features for Place Recognition.pdf:pdf},
title = {{On the Performance of ConvNet Features for Place Recognition}},
year = {2015}
}
@article{Kendall,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.05909v2},
author = {Kendall, Alex and Cipolla, Roberto and Feb, C V},
eprint = {arXiv:1509.05909v2},
file = {:home/chris/Documents/Mendeley Desktop/Kendall, Cipolla, Feb - Unknown - Modelling Uncertainty in Deep Learning for Camera Relocalization.pdf:pdf},
title = {{Modelling Uncertainty in Deep Learning for Camera Relocalization}}
}
@article{Sunderhauf2013,
abstract = {When operating over extended periods of time, an autonomous system will inevitably be faced with severe changes in the appearance of its environment. Coping with such changes is more and more in the focus of current robotics research. In this paper, we foster the development of robust place recognition algorithms in changing environments by describing a new dataset that was recorded during a 728km long journey in spring, summer, fall, and winter. Approximately 40 hours of full-HD video cover extreme seasonal changes over almost 3000 km in both natural and man-made environments. Furthermore, accurate ground truth information are provided. To our knowledge, this is by far the largest SLAM dataset available at the moment. In addition, we introduce an open source Matlab implementation of the recently published SeqSLAM algorithm and make it available to the community. We benchmark SeqSLAM using the novel dataset and analyse the influence of important parameters and algorithmic steps.},
author = {S{\"{u}}nderhauf, N and Neubert, Peer and Protzel, Peter},
file = {:home/chris/Documents/Mendeley Desktop/Niko, Neubert, Protzel - 2013 - Are We There Yet Challenging SeqSLAM on a 3000 km Journey Across All Four Seasons.pdf:pdf},
journal = {Int. Conf. Robot. Autom.},
pages = {1--3},
title = {{Are we there yet? Challenging SeqSLAM on a 3000 km journey across all four seasons}},
url = {http://www.tu-chemnitz.de/etit/proaut/rsrc/openseqslam.pdf},
year = {2013}
}
@article{Forster2014a,
abstract = {We propose a semi-direct monocular visual odometry algorithm that is precise, robust, and faster than current state-of-the-art methods. The semi-direct approach eliminates the need of costly feature extraction and robust matching techniques for motion estimation. Our algorithm operates directly on pixel intensities, which results in subpixel precision at high frame-rates. A probabilistic mapping method that explicitly models outlier measurements is used to estimate 3D points, which results in fewer outliers and more reliable points. Precise and high frame-rate motion estimation brings increased robustness in scenes of little, repetitive, and high-frequency texture. The algorithm is applied to micro-aerial-vehicle state- estimation in GPS-denied environments and runs at 55 frames per second on the onboard embedded computer and at more than 300 frames per second on a consumer laptop. We call our approach SVO (Semi-direct Visual Odometry) and release our implementation as open-source software.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.05830v2},
author = {Forster, Christian and Pizzoli, Matia and Scaramuzza, Davide},
doi = {10.1109/ICRA.2014.6906584},
eprint = {arXiv:1606.05830v2},
file = {:home/chris/Documents/Mendeley Desktop/SVO$\backslash$: Fast Semi-Direct Monocular Visual Odometry.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
number = {May},
pages = {15--22},
title = {{SVO: Fast semi-direct monocular visual odometry}},
year = {2014}
}
@article{S??nderhauf2015,
abstract = {After the incredible success of deep learning in the computer vision domain, there has been much interest in applying Convolutional Network (ConvNet) features in robotic fields such as visual navigation and SLAM. Unfortunately, there are fundamental differences and challenges involved. Computer vision datasets are very different in character to robotic camera data, real-time performance is essential, and performance priorities can be different. This paper comprehensively evaluates and compares the utility of three state-of-the-art ConvNets on the problems of particular relevance to navigation for robots; viewpoint-invariance and condition-invariance, and for the first time enables real-time place recognition performance using ConvNets with large maps by integrating a variety of existing (locality-sensitive hashing) and novel (semantic search space partitioning) optimization techniques. We present extensive experiments on four real world datasets cultivated to evaluate each of the specific challenges in place recognition. The results demonstrate that speed-ups of two orders of magnitude can be achieved with minimal accuracy degradation, enabling real-time performance. We confirm that networks trained for semantic place categorization also perform better at (specific) place recognition when faced with severe appearance changes and provide a reference for which networks and layers are optimal for different aspects of the place recognition problem.},
archivePrefix = {arXiv},
arxivId = {1501.04158},
author = {S??nderhauf, Niko and Shirazi, Sareh and Dayoub, Feras and Upcroft, Ben and Milford, Michael},
doi = {10.1109/IROS.2015.7353986},
eprint = {1501.04158},
file = {:home/chris/Documents/Mendeley Desktop/Niko, Jul - 2015 - On the Performance of ConvNet Features for Place Recognition.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
keywords = {Computer vision,Feature extraction,Real-time systems,Robustness,Semantics,Visualization},
pages = {4297--4304},
title = {{On the performance of ConvNet features for place recognition}},
volume = {2015-Decem},
year = {2015}
}
@article{Kuemmerle2011,
abstract = {Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g2o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g2o offers a performance comparable to implementations of state of-the-art approaches for the specific problems.},
author = {K{\"{u}}mmerle, Rainer and Grisetti, Giorgio and Strasdat, Hauke and Konolige, Kurt and Burgard, Wolfram},
doi = {10.1109/ICRA.2011.5979949},
file = {:home/chris/Documents/Mendeley Desktop/K{\"{u}}mmerle et al. - 2011 - G2o A general framework for graph optimization g 2 o A General Framework for Graph Optimization.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
number = {June},
pages = {3607--3613},
pmid = {5979949},
title = {{G2o: A general framework for graph optimization}},
year = {2011}
}
@article{Bai2017,
abstract = {Loop closure detection (LCD) is an indispensable part of simultaneous localization and mapping systems (SLAM); it enables robots to produce a consistent map by recognizing previously visited places. When robots operate over extended periods, robustness to viewpoint and condition changes as well as satisfactory real-time performance become essential requirements for a practical LCD system. This paper presents an approach to directly utilize the outputs at the intermediate layer of a pre-trained convolutional neural network (CNN) as image descriptors. The matching location is determined by matching the image sequences through a method called SeqCNNSLAM. The utility of SeqCNNSLAM is comprehensively evaluated in terms of viewpoint and condition invariance. Experiments show that SeqCNNSLAM outperforms state-of-the-art LCD systems, such as SeqSLAM and Change Removal, in most cases. To allow for the real-time performance of SeqCNNSLAM, an acceleration method, A-SeqCNNSLAM, is established. This method exploits the location relationship between the matching images of adjacent images to reduce the matching range of the current image. Results demonstrate that acceleration of 4-6 is achieved with minimal accuracy degradation, and the method's runtime satisfies the real-time demand. To extend the applicability of A-SeqCNNSLAM to new environments, a method called O-SeqCNNSLAM is established for the online adjustment of the parameters of A-SeqCNNSLAM.},
archivePrefix = {arXiv},
arxivId = {1704.05016},
author = {Bai, Dongdong and Wang, Chaoqun and Zhang, Bo and Yi, Xiaodong and Yang, Xuejun},
eprint = {1704.05016},
file = {:home/chris/Documents/Mendeley Desktop/Bai et al. - 2017 - CNN Feature boosted SeqSLAM for Real-Time Loop Closure Detection.pdf:pdf},
keywords = {a-seqcnnslam,cnn,loop closure detection,o-seqcnnslam,seqcnnslam},
title = {{CNN Feature boosted SeqSLAM for Real-Time Loop Closure Detection}},
url = {http://arxiv.org/abs/1704.05016},
year = {2017}
}
@article{Pascoe2017,
author = {Pascoe, G and Maddern, W and Tanner, M and Pini{\'{e}}s, P and Newman, P},
file = {:home/chris/Documents/Mendeley Desktop/NID-SLAM$\backslash$: Robust Monocular SLAM using Normalised Information Distance.pdf:pdf},
journal = {Comput. Vis. Pattern Recognit.},
title = {{NID-SLAM: Robust Monocular SLAM using Normalised Information Distance}},
url = {http://www.robots.ox.ac.uk/{~}mobile/Papers/2017CVPR{\_}pascoe.pdf},
year = {2017}
}
@article{Cummins2008,
abstract = {This paper describes a probabilistic approach to the problem of recognizing places based on their appearance. The system we present is not limited to localization, but can determine that a new observation comes from a previously unseen place, and so augment its map. Effectively this is a SLAM system in the space of appearance. Our probabilistic approach allows us to explicitly account for perceptual aliasing in the environment--identical but indistinctive observations receive a low probability of having come from the same place. We achieve this by learning a generative model of place appearance. By partitioning the learning problem into two parts, new place models can be learned online from only a single observation of a place. The algorithm complexity is linear in the number of places in the map, and is particularly suitable for online loop closure detection in mobile robotics.},
author = {Cummins, M. and Newman, P.},
doi = {10.1177/0278364908090961},
file = {:home/chris/Documents/Mendeley Desktop/FAB-MAP$\backslash$: Probabilistic Localization and Mapping in the Space of Appearance.pdf:pdf},
isbn = {0278364908090},
issn = {0278-3649},
journal = {Int. J. Rob. Res.},
keywords = {ap-,pearance based navigation,place recognition,topological slam},
number = {6},
pages = {647--665},
title = {{FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364908090961},
volume = {27},
year = {2008}
}
@article{Paul2010,
abstract = {This paper describes a probabilistic framework for appearance based navigation and mapping using spatial and visual appearance data. Like much recent work on appearance based navigation we adopt a bag-of-words approach in which positive or negative observations of visual words in a scene are used to discriminate between already visited and new places. In this paper we add an important extra dimension to the approach. We explicitly model the spatial distribution of visual words as a random graph in which nodes are visual words and edges are distributions over distances. Care is taken to ensure that the spatial model is able to capture the multi-modal distributions of inter-word spacing and account for sensor errors both in word detection and distances. Crucially, these inter-word distances are viewpoint invariant and collectively constitute strong place signatures and hence the impact of using both spatial and visual appearance is marked. We provide results illustrating a tremendous increase in precision-recall area compared to a state-of-the-art visual appearance only systems.},
author = {Paul, Rohan and Newman, Paul},
doi = {10.1109/ROBOT.2010.5509587},
file = {:home/chris/Documents/Mendeley Desktop/FAB-MAP 3D$\backslash$: Topological Mapping with Spatial and Visual Appearance.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {2649--2656},
title = {{FAB-MAP 3D: Topological mapping with spatial and visual appearance}},
year = {2010}
}
@article{Glover2010,
abstract = {Appearance-based mapping and localisation is especially challenging when separate processes of mapping and localisation occur at different times of day. The problem is exacerbated in the outdoors where continuous change in sun angle can drastically affect the appearance of a scene. We confront this challenge by fusing the probabilistic local feature based data association method of FAB-MAP with the pose cell filtering and experience mapping of RatSLAM. We evaluate the effectiveness of our amalgamation of methods using five datasets captured throughout the day from a single camera driven through a network of suburban streets. We show further results when the streets are re-visited three weeks later, and draw conclusions on the value of the system for lifelong mapping.},
author = {Glover, Arren J. and Maddern, William P. and Milford, Michael J. and Wyeth, Gordon F.},
doi = {10.1109/ROBOT.2010.5509547},
file = {:home/chris/Documents/Mendeley Desktop/FAB-MAP + RatSLAM$\backslash$: Appearance-based SLAM for Multiple Times of Day.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3507--3512},
title = {{FAB-MAP + RatSLAM: Appearance-based SLAM for multiple times of day}},
year = {2010}
}
@article{Konolige2008,
abstract = {—Many successful indoor mapping techniques employ frame-to-frame matching of laser scans to produce detailed local maps, as well as closing large loops. In this paper, we propose a framework for applying the same techniques to visual imagery. We match visual frames with large numbers of point features, using classic bundle adjustment techniques from computational vision, but keep only relative frame pose information (a skeleton). The skeleton is a reduced nonlinear system that is a faithful approximation of the larger system, and can be used to solve large loop closures quickly, as well as forming a backbone for data association and local registration. We illustrate the working of the system with large outdoor datasets (10 km), showing large-scale loop closure and precise localization in real time.},
author = {Konolige, Kurt and Agrawal, Motilal},
doi = {10.1109/TRO.2008.2004832},
file = {:home/chris/Documents/Mendeley Desktop/FrameSLAM$\backslash$: from Bundle Adjustment to Realtime Visual Mappping.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Trans. Robot.},
number = {5},
pages = {1--11},
title = {{FrameSLAM : from Bundle Adjustment to Realtime Visual Mappping}},
volume = {24},
year = {2008}
}
@article{Abuhashim2016,
abstract = {—This paper presents a new robustification procedure for nonlinear least-squares optimisation problems. In particular, we focus on the robustness of view-graph SLAM against outlier correspondences in the images and outlier geometries in the graph. Our method utilises revised measurements model linearisation and decision making to detect and remove outliers during data fusion. We utilise innovations and residuals gating to decide which observations were affected, given the most recent model linearisation point. By exploiting the inherited locality of measurements and states and the sparsity structure of nonlinear least-squares formulation we can check which measurement is affected before and after data fusion. This locality is the whole basis of robustification. To be efficient, we carry out the estimation using the information form. By doing so, we can include and remove information by individual measurements at each step using simple information addition and subtraction operations. Our results demonstrate the robustness of our method against outliers with respect to the use of kinematics alone and RANSAC with Levenberg-Marquardt algorithm.},
author = {Abuhashim, Tariq and Natale, Lorenzo},
file = {:home/chris/Documents/Mendeley Desktop/Abuhashim, Natale - 2016 - Robustness in View-Graph SLAM.pdf:pdf},
isbn = {9780996452748},
journal = {Int. Conf. Inf. Fusion},
title = {{Robustness in View-Graph SLAM}},
year = {2016}
}
@article{Bloesch2015,
abstract = {In this paper, we present a monocular visual- inertial odometry algorithm which, by directly using pixel intensity errors of image patches, achieves accurate tracking performance while exhibiting a very high level of robustness. After detection, the tracking of the multilevel patch features is closely coupled to the underlying extended Kalman filter (EKF) by directly using the intensity errors as innovation term during the update step. We follow a purely robocentric approach where the location of 3D landmarks are always estimated with respect to the current camera pose. Furthermore, we decompose landmark positions into a bearing vector and a distance parametrization whereby we employ a minimal representation of differences on a corresponding -Algebra in order to achieve better consistency and to improve the computational performance. Due to the robocentric, inverse- distance landmark parametrization, the framework does not require any initialization procedure, leading to a truly power- up-and-go state estimation system. The presented approach is successfully evaluated in a set of highly dynamic hand-held experiments as well as directly employed in the control loop of a multirotor unmanned aerial vehicle (UAV).},
author = {Bloesch, Michael and Omari, Sammy and Hutter, Marco and Siegwart, Roland},
doi = {10.1109/IROS.2015.7353389},
file = {:home/chris/Documents/Mendeley Desktop/Bloesch et al. - Unknown - Robust Visual Inertial Odometry Using a Direct EKF-Based Approach.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
keywords = {Cameras,Estimation,Feature extraction,Robots,Technological innovation,Three-dimensional displays,Uncertainty},
pages = {298--304},
title = {{Robust visual inertial odometry using a direct EKF-based approach}},
volume = {2015-Decem},
year = {2015}
}
@article{Mei2011,
abstract = {Large scale exploration of the environment requires a constant time estimation engine. Bundle adjustment or pose relaxation do not fulfil these requirements as the number of parameters to solve grows with the size of the environment. We describe a relative simultaneous localisation and mapping system (RSLAM) for the constant-time estimation of structure and motion using a binocular stereo camera system as the sole sensor. Achieving robustness in the presence of difficult and changing lighting conditions and rapid motion requires careful engineering of the visual processing, and we describe a number of innovations which we show lead to high accuracy and robustness. In order to achieve real-time performance without placing severe limits on the size of the map that can be built, we use a topo-metric representation in terms of a sequence of relative locations. When combined with fast and reliable loop-closing, we mitigate the drift to obtain highly accurate global position estimates without any global minimisation. We discuss some of the issues that arise from using a relative representation, and evaluate our system on long sequences processed at a constant 30–45 Hz, obtaining precisions down to a few meters over distances of a few kilometres.},
author = {Mei, Christopher and Sibley, Gabe and Cummins, Mark and Newman, Paul and Reid, Ian},
doi = {10.1007/s11263-010-0361-7},
file = {:home/chris/Documents/Mendeley Desktop/RSLAM$\backslash$: A System for Large-Scale Mapping in Constant-Time using Stereo.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {Int. J. Comput. Vis.},
keywords = {Loop closing,SIFT,SLAM,Stereo,Tracking},
number = {2},
pages = {198--214},
title = {{RSLAM: A system for large-scale mapping in constant-time using stereo}},
volume = {94},
year = {2011}
}
@article{Cummins2009,
abstract = {We describe a new formulation of appearance-only SLAM suitable for very large scale navigation. The system navi- gates in the space of appearance, assigning each new observation to either a new or previously visited location, without reference to metric position. The system is demonstrated performing reliable online appearance mapping and loop closure detection over a 1,000km trajectory, with mean filter update times of 14 ms. The 1,000km experiment is more than an order of magnitude larger than any previously reported result. The scalability of the system is achieved by defining a sparse approximation to the FAB-MAP model suitable for implementation using an inverted index. Our formulation of the problem is fully probabilistic and naturally incorporates robustness against perceptual aliasing. The 1,000km data set comprising almost a terabyte of omni-directional and stereo imagery is available for use, and we hope that it will serve as a benchmark for future systems.},
author = {Cummins, Mark and Newman, Paul},
doi = {10.1109/ROBOT.2008.4543473},
file = {:home/chris/Documents/Mendeley Desktop/Cummins, Newman - Unknown - Highly Scalable Appearance-Only SLAM –.pdf:pdf},
isbn = {978-1-4244-1646-2},
issn = {10504729},
journal = {Rss},
pages = {1--8},
title = {{Highly Scalable Appearance-Only SLAM - FAB-MAP 2.0}},
year = {2009}
}
@article{Tarrio2016,
abstract = {In this work we present a novel algorithm for realtime visual odometry for a monocular camera. The main idea is to develop an approach between classical feature-based vi-sual odometry systems and modern direct dense/semi-dense methods, trying to benefit from the best attributes of both. Similar to feature-based systems, we extract information from the images, instead of working with raw image inten-sities as direct methods. In particular, the information ex-tracted are the edges present in the image, while the rest of the algorithm is designed to take advantage of the struc-tural information provided when pixels are treated as edges. Edge extraction is an efficient and higly parallelizable op-eration. The edge depth information extracted is dense enough to allow acceptable surface fitting, similar to mod-ern semi-dense methods. This is a valuable attribute that feature-based odometry lacks. Experimental results show that the proposed method has similar drift than state of the art feature-based and direct methods, and is a simple algo-rithm that runs at realtime and can be parallelized. Finally, we have also developed an inertial aided version that suc-cessfully stabilizes an unmanned air vehicle in complex in-door environments using only a frontal camera, while run-ning the complete solution in the embedded hardware on board the vehicle.},
author = {Tarrio, Juan Jose and Pedre, Sol},
doi = {10.1109/ICCV.2015.87},
file = {:home/chris/Documents/Mendeley Desktop/Jos, Pedre, Cnea - Unknown - Realtime edge-based visual odometry for a monocular camera.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {702--710},
pmid = {6130318},
title = {{Realtime edge-based visual odometry for a monocular camera}},
volume = {11-18-Dece},
year = {2016}
}
@article{Mur-Artal2016a,
abstract = {We present ORB-SLAM2 a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time in standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our backend based on Bundle Adjustment with monocular and stereo observations allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches to map points that allow for zero-drift localization. The evaluation in 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
archivePrefix = {arXiv},
arxivId = {1610.06475},
author = {Mur-Artal, Raul and Tardos, Juan D.},
doi = {10.1109/TRO.2012.2197158},
eprint = {1610.06475},
file = {:home/chris/Documents/Mendeley Desktop/ORB-SLAM2$\backslash$: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
pmid = {309728700020},
title = {{ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras}},
url = {http://arxiv.org/abs/1610.06475},
year = {2016}
}
@article{Kendall2016a,
abstract = {We present a robust and real-time monocular six degree of freedom visual relocalization system. We use a Bayesian convolutional neural network to regress the 6-DOF camera pose from a single RGB image. It is trained in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking under 6ms to compute. It obtains approximately 2m and 6 degrees accuracy for very large scale outdoor scenes and 0.5m and 10 degrees accuracy indoors. Using a Bayesian convolutional neural network implementation we obtain an estimate of the model's relocalization uncertainty and improve state of the art localization accuracy on a large scale outdoor dataset. We leverage the uncertainty measure to estimate metric relocalization error and to detect the presence or absence of the scene in the input image. We show that the model's uncertainty is caused by images being dissimilar to the training dataset in either pose or appearance.},
archivePrefix = {arXiv},
arxivId = {1509.05909},
author = {Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/ICRA.2016.7487679},
eprint = {1509.05909},
file = {:home/chris/Documents/Mendeley Desktop/Kendall, Cipolla, Feb - Unknown - Modelling Uncertainty in Deep Learning for Camera Relocalization.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {4762--4769},
title = {{Modelling uncertainty in deep learning for camera relocalization}},
volume = {2016-June},
year = {2016}
}
@article{Felzenszwalb2015,
author = {Felzenszwalb, Pedro F and Huttenlocher, Daniel P},
doi = {10.1023/B:VISI.0000022288.19776.77},
file = {:home/chris/Documents/Mendeley Desktop/Felzenszwalb, Huttenlocher - Unknown - Efficient Graph-Based Image Segmentation.pdf:pdf},
isbn = {0920-5691},
issn = {1573-1405},
keywords = {clustering,graph algorithm,image segmentation,perceptual organization},
pages = {1--26},
pmid = {1000198747},
title = {{Efficient Graph-Based Image Segmentation}},
url = {papers3://publication/uuid/D1250C05-2FC7-4954-A734-E33EBBEECB95},
year = {2015}
}
@article{Daniel2014,
abstract = {Obtaining a good baseline between different video frames is one of the key elements in vision-based monocular SLAM systems. However, if the video frames contain only a few 2D feature correspondences with a good baseline, or the camera only rotates without sufficient translation in the beginning, tracking and mapping becomes unstable. We introduce a real-time visual SLAM system that incrementally tracks individual 2D features, and estimates camera pose by using matched 2D features, regardless of the length of the baseline. Triangulating 2D features into 3D points is deferred until key frames with sufficient baseline for the features are available. Our method can also deal with pure rotational motions, and fuse the two types of measurements in a bundle adjustment step. Adaptive criteria for key frame selection are also introduced for efficient optimization and dealing with multiple maps. We demonstrate that our SLAM system improves camera pose estimates and robustness, even with purely rotational motions.},
author = {Daniel, Herrera C. and Kim, Kihwan and Kannala, Juho and Pulli, Kari and Heikkil??, Janne},
doi = {10.1109/3DV.2014.49},
file = {:home/chris/Documents/Mendeley Desktop/DT-SLAM$\backslash$: Deferred Triangulation for Robust SLAM.pdf:pdf},
isbn = {9781479970018},
journal = {Proc. - 2014 Int. Conf. 3D Vision, 3DV 2014},
pages = {609--616},
title = {{DT-SLAM: Deferred triangulation for robust SLAM}},
year = {2014}
}
@phdthesis{Leutenegger2014,
author = {Leutenegger, Stefan},
file = {:home/chris/Documents/Mendeley Desktop/Operation - 2014 - Unmanned solar airplanes.pdf:pdf},
school = {ETH Zurich},
title = {{Unmanned solar airplanes - Design and Algorithms for Efficient and RobustAutonomous Operation}},
type = {Dissertation},
url = {https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/90524/eth-46751-02.pdf},
year = {2014}
}
@article{Leutenegger2015,
abstract = {Combining visual and inertial measurements has become popular in mobile robotics, since the two sensing modalities offer complementary characteristics that make them the ideal choice for accurate visual–inertial odometry or simultaneous localization and mapping (SLAM). While historically the problem has been addressed with filtering, advancements in visual estimation suggest that nonlinear optimization offers superior accuracy, while still tractable in complexity thanks to the sparsity of the underlying problem. Taking inspiration from these findings, we formulate a rigorously probabilistic cost function that combines reprojection errors of landmarks and inertial terms. The problem is kept tractable and thus ensuring real-time operation by limiting the optimization to a bounded window of keyframes through marginalization. Keyframes may be spaced in time by arbitrary intervals, while still related by linearized inertial terms. We present evaluation results on complementary datasets recorded with our custom-built stereo visual–inertial hardware that accurately synchronizes accelerometer and gyroscope measurements with imagery. A comparison of both a stereo and monocular version of our algorithm with and without online extrinsics estimation is shown with respect to ground truth. Furthermore, we compare the performance to an implementation of a state-of-the-art stochastic cloning sliding-window filter. This competitive reference implementation performs tightly coupled filtering-based visual–inertial odometry. While our approach declaredly demands more computation, we show its superior performance in terms of accuracy.},
author = {Leutenegger, Stefan and Lynen, Simon and Bosse, Michael and Siegwart, Roland and Furgale, Paul},
doi = {10.1177/0278364914554813},
file = {:home/chris/Documents/Mendeley Desktop/Leutenegger et al. - 2015 - Keyframe-based visual – inertial odometry using nonlinear optimization.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {Int. J. Rob. Res.},
keywords = {bundle adjustment,imu,inertial measurement unit,inertial odometry,keyframes,robotics,sensor fusion,simultaneous localization and mapping,slam,stereo camera,visual},
number = {3},
pages = {314--334},
title = {{Keyframe-based visual-inertial odometry using nonlinear optimization}},
url = {http://journals.sagepub.com/doi/10.1177/0278364914554813},
volume = {34},
year = {2015}
}
@article{Whelan2015,
abstract = {We present a novel approach to real-time dense visual SLAM. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments explored using an RGB-D camera in an incremental online fashion, without pose graph optimisation or any postprocessing steps. This is accomplished by using dense frame-tomodel camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimisations as often as possible to stay close to the mode of the map distribution, while utilising global loop closure to recover from arbitrary drift and maintain global consistency},
author = {Whelan, Thomas and Leutenegger, Stefan and {Salas Moreno}, Renato and Glocker, Ben and Davison, Andrew},
doi = {10.15607/RSS.2015.XI.001},
file = {:home/chris/Documents/Mendeley Desktop/ElasticFusion$\backslash$: Dense SLAM Without A Pose Graph.pdf:pdf},
isbn = {9780992374716},
issn = {2330765X},
journal = {Robot. Sci. Syst. XI},
pmid = {397311},
title = {{ElasticFusion: Dense SLAM Without A Pose Graph}},
url = {http://www.roboticsproceedings.org/rss11/p01.pdf},
year = {2015}
}
@article{Concha2015a,
abstract = {This paper proposes a direct monocular SLAM algorithm that estimates a dense reconstruction of a scene in real-time on a CPU. Highly textured image areas are mapped using standard direct mapping techniques, that minimize the photometric error across different views. We make the assumption that homogeneous-color regions belong to approximately planar areas. Our contribution is a new algorithm for the estimation of such planar areas, based on the information of a superpixel segmentation and the semidense map from highly textured areas. We compare our approach against several alternatives using the public TUM dataset and additional live experiments with a hand-held camera.We demonstrate that our proposal for piecewise planar monocular SLAM is faster, more accurate and more robust than the piecewise planar baseline. In addition, our experimental results show how the depth regularization of monocular maps can damage its accuracy, being the piecewise planar assumption a reasonable option in indoor scenarios.},
author = {Concha, Alejo and Civera, Javier},
doi = {10.1109/IROS.2015.7354184},
file = {:home/chris/Documents/Mendeley Desktop/DPPTAM$\backslash$: Dense Piecewise Planar Tracking and Mapping from a Monocular Sequence.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
number = {June},
pages = {5686--5693},
title = {{DPPTAM: Dense piecewise planar tracking and mapping from a monocular sequence}},
volume = {2015-Decem},
year = {2015}
}
@article{Engel2016,
abstract = {We propose a novel direct sparse visual odometry formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry -- represented as inverse depth in a reference frame -- and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on mostly white walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
archivePrefix = {arXiv},
arxivId = {1607.02565},
author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
doi = {10.1109/TPAMI.2017.2658577},
eprint = {1607.02565},
file = {:home/chris/Documents/Mendeley Desktop/Engel, Cremers - Unknown - Direct Sparse Odometry.pdf:pdf},
issn = {0162-8828},
pmid = {28060704},
title = {{Direct Sparse Odometry}},
url = {http://arxiv.org/abs/1607.02565},
year = {2016}
}
@article{Concha2015b,
abstract = {This paper proposes a direct monocular SLAM algorithm that estimates a dense reconstruction of a scene in real-time on a CPU. Highly textured image areas are mapped using standard direct mapping techniques, that minimize the photometric error across different views. We make the assumption that homogeneous-color regions belong to approximately planar areas. Our contribution is a new algorithm for the estimation of such planar areas, based on the information of a superpixel segmentation and the semidense map from highly textured areas. We compare our approach against several alternatives using the public TUM dataset and additional live experiments with a hand-held camera.We demonstrate that our proposal for piecewise planar monocular SLAM is faster, more accurate and more robust than the piecewise planar baseline. In addition, our experimental results show how the depth regularization of monocular maps can damage its accuracy, being the piecewise planar assumption a reasonable option in indoor scenarios.},
author = {Concha, Alejo and Civera, Javier},
doi = {10.1109/IROS.2015.7354184},
file = {:home/chris/Documents/Mendeley Desktop/DPPTAM$\backslash$: Dense Piecewise Planar Tracking and Mapping from a Monocular Sequence(2).pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {5686--5693},
title = {{DPPTAM: Dense piecewise planar tracking and mapping from a monocular sequence}},
volume = {2015-Decem},
year = {2015}
}
@article{Pirker2011a,
abstract = {We propose a novel, hybrid SLAM system to construct a dense occupancy grid map$\backslash$nbased on sparse visual features and dense depth information. While previous approaches$\backslash$ndeemed the occupancy grid usable only in 2D mapping, and in combination with a probabilistic$\backslash$napproach, we show that geometric SLAM can produce consistent, robust and$\backslash$ndense occupancy information, and maintain it even during erroneous exploration and$\backslash$nloop closure. We require only a single hypothesis of the occupancy map and employ a$\backslash$nweighted inverse mapping scheme to align it to sparse geometric information. We propose$\backslash$na novel map-update criterion to prevent inconsistencies, and a robust measure to$\backslash$ndiscriminate exploration from localization.},
author = {Pirker, Katrin and R{\"{u}}ther, Matthias and Schweighofer, Gerald and Bischof, Horst},
doi = {10.5244/C.25.115},
file = {:home/chris/Documents/Mendeley Desktop/GPSlam$\backslash$: Marrying Sparse Geometric and Dense Probabilistic Visual Mapping.pdf:pdf},
isbn = {1-901725-43-X},
journal = {Procedings Br. Mach. Vis. Conf. 2011},
pages = {115.1--115.12},
title = {{GPSlam: Marrying Sparse Geometric and Dense Probabilistic Visual Mapping}},
url = {http://www.bmva.org/bmvc/2011/proceedings/paper115/index.html},
year = {2011}
}
@article{Pirker2010,
abstract = {Simultaneous localization and mapping (SLAM) is a basic prerequisite in autonomous mobile robotics. Most existing visual SLAM approaches either assume a static environ- ment, or simply 'forget' old parts of the map to cope with map size constraints and scene dynamics. We present a novel map representation for sparse visual features. A new 3D point descriptor called Histogram of Oriented Cameras (HOC) encodes anisotropic spa- tial visibility information and the importance of each three-dimensional landmark. Each feature holds and updates a histogram of the poses of observing cameras. It is hereby able to estimate its probability of occlusion and importance for localization from a given viewpoint. In a series of simulated and real-world experiments we prove that the pro- posed descriptor allows to cope with dynamic changes in the map, improves localization accuracy and enables reasonable control of the map size. 1},
author = {Pirker, Katrin and R{\"{u}}ther, Matthias and Bischof, Horst},
doi = {10.5244/C.24.76},
file = {:home/chris/Documents/Mendeley Desktop/Histogram of oriented cameras - a new descriptor for visual slam in dynamic environments.pdf:pdf},
isbn = {1-901725-40-5},
journal = {Br. Mach. Vis. Conf.},
pages = {76.1--76.12},
title = {{Histogram of Oriented Cameras - A New Descriptor for Visual SLAM in Dynamic Environments}},
year = {2010}
}
@article{Wu2016,
author = {Wu, Xiu-zhen and Gang, Liu and Gong, Wei-si and Shi, Yan},
file = {:home/chris/Documents/Mendeley Desktop/An Improved Monocular ORB-SLAM Method.pdf:pdf},
isbn = {9781605954110},
journal = {Int. Conf. Artif. Intell. Comput. Sci.},
keywords = {abstract,adaptive threshold,an improved algorithm is,application of monocular vision,ensure the validity of,feature points are detected,multiple solution selection,orb-slam,orb-slam algorithm,proposed for the practical,the adaptive gray level,the feature points detection,threshold is designed to,vision localization,when the image contrast,when the image fast},
number = {Aics},
pages = {253--258},
title = {{An Improved Monocular ORB-SLAM Method}},
year = {2016}
}
@article{Milford2012,
abstract = { Abstract— Learning and then recognizing a route, whether travelled during the day or at night, in clear or inclement weather, and in summer or winter is a challenging task for state of the art algorithms in computer vision and robotics. In this paper, we present a new approach to visual navigation under changing conditions dubbed SeqSLAM. Instead of calculating the single location most likely given a current image, our approach calculates the best candidate matching location within every local navigation sequence. Localization is then achieved by recognizing coherent sequences of these " local best matches " . This approach removes the need for global matching performance by the vision front-end – instead it must only pick the best match within any short sequence of images. The approach is applicable over environment changes that render traditional feature-based techniques ineffective. Using two car-mounted camera datasets we demonstrate the effectiveness of the algorithm and compare it to one of the most successful feature-based SLAM algorithms, FAB-MAP. The perceptual change in the datasets is extreme; repeated traverses through environments during the day and then in the middle of the night, at times separated by months or years and in opposite seasons, and in clear weather and extremely heavy rain. While the feature-based method fails, the sequence-based algorithm is able to match trajectory segments at 100{\%} precision with recall rates of up to 60{\%}.},
author = {Milford, Michael J. and Wyeth, Gordon F.},
doi = {10.1109/ICRA.2012.6224623},
file = {:home/chris/Documents/Mendeley Desktop/SeqSLAM$\backslash$: Visual Route-Based Navigation for Sunny Summer Days and Stormy Winter Nights.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {1643--1649},
title = {{SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights}},
year = {2012}
}
@article{Pirker2011,
author = {Pirker, Katrin and Ruther, M and Bischof, Horst},
file = {:home/chris/Documents/Mendeley Desktop/CD SLAM - Continuous Localization and Mapping in a Dynamic World.pdf:pdf},
isbn = {9781612844565},
journal = {IEEE/RSJ Int. Conf. Intell. Robot. Syst. IROS},
pages = {3990--3997},
title = {{CD SLAM - continuous localization and mapping in a dynamic world}},
year = {2011}
}
@article{Frese2010,
abstract = {This paper gives a brief overview on the Simultaneous Localization and Mapping (SLAM) problem from the perspective of using SLAM for an application as opposed to the common view in SLAM research papers that focus on investigating SLAM itself. We discuss different ways of using SLAM with increasing difficulty: for creating a map prior to operation, as a black-box localization system, and for providing a growing online map during operation. We also discuss the common variants of SLAM based on 2-D evidence grids, 2-D pose graphs, 2-D features, 3-D visual features, and 3-D pose graphs together with their pros and cons for applications. We point to implementations available on the Internet and give advice on which approach suits which application from our experience.},
author = {Frese, Udo and Wagner, Ren{\'{e}} and R{\"{o}}fer, Thomas},
doi = {10.1007/s13218-010-0040-4},
file = {:home/chris/Documents/Mendeley Desktop/A SLAM Overview from a User's Perspective.pdf:pdf},
isbn = {4942121864207},
issn = {09331875},
journal = {KI - K{\"{u}}nstliche Intelligenz},
keywords = {Localization,Navigation,SLAM},
pages = {1--8},
title = {{A SLAM Overview from a User's Perspective}},
year = {2010}
}
@article{Civera2008,
abstract = {Abstract?We present a new parametrization for point features$\backslash$n$\backslash$nwithin monocular simultaneous localization and mapping$\backslash$n$\backslash$n(SLAM) that permits efficient and accurate representation of uncertainty$\backslash$n$\backslash$nduring undelayed initialization and beyond, all within$\backslash$n$\backslash$nthe standard extended Kalman filter (EKF). The key concept$\backslash$n$\backslash$nis direct parametrization of the inverse depth of features relative$\backslash$n$\backslash$nto the camera locations from which they were first viewed,$\backslash$n$\backslash$nwhich produces measurement equations with a high degree of$\backslash$n$\backslash$nlinearity. Importantly, our parametrization can cope with features$\backslash$n$\backslash$nover a huge range of depths, even those that are so far fromthe camera$\backslash$n$\backslash$nthat they present little parallax during motion?maintaining$\backslash$n$\backslash$nsufficient representative uncertainty that these points retain the$\backslash$nopportunity$\backslash$n$\backslash$nto ?come in? smoothly from infinity if the camera makes$\backslash$n$\backslash$nlarger movements. Feature initialization is undelayed in the sense$\backslash$n$\backslash$nthat even distant features are immediately used to improve camera$\backslash$n$\backslash$nmotion estimates, acting initially as bearing references but not$\backslash$n$\backslash$npermanently labeled as such. The inverse depth parametrization$\backslash$n$\backslash$nremains well behaved for features at all stages of SLAM processing,$\backslash$n$\backslash$nbut has the drawback in computational terms that each point is$\backslash$n$\backslash$nrepresented by a 6-D state vector as opposed to the standard three$\backslash$n$\backslash$nof a Euclidean XYZ representation. We show that once the depth$\backslash$n$\backslash$nestimate of a feature is sufficiently accurate, its representation$\backslash$ncan$\backslash$n$\backslash$nsafely be converted to the Euclidean XYZ form, and propose a$\backslash$n$\backslash$nlinearity index that allows automatic detection and conversion to$\backslash$n$\backslash$nmaintain maximum efficiency?only low parallax features need be$\backslash$n$\backslash$nmaintained in inverse depth form for long periods. We present a$\backslash$n$\backslash$nreal-time implementation at 30 Hz, where the parametrization is$\backslash$n$\backslash$nvalidated in a fully automatic 3-D SLAM system featuring a handheld$\backslash$n$\backslash$nsingle camera with no additional sensing. Experiments show$\backslash$n$\backslash$nrobust operation in challenging indoor and outdoor environments$\backslash$n$\backslash$nwith a very large ranges of scene depth, varied motion, and also$\backslash$n$\backslash$nreal time 360? loop closing.},
author = {Civera, Javier and Davison, Andrew J and Montiel, J M Martinez},
doi = {10.1109/TRO.2008.2003276},
file = {:home/chris/Documents/Mendeley Desktop/Inverse Depth Parametrization for Monocular SLAM.pdf:pdf},
isbn = {1552-3098 VO - 24},
issn = {15523098},
journal = {IEEE Trans. Robot.},
keywords = {2D,EKF-SLAM,SLAM,computer vision,localization,robotics},
number = {5},
pages = {932--945},
title = {{Inverse Depth Parameterization for Monocular {\{}SLAM{\}}}},
volume = {24},
year = {2008}
}
@article{Strasdat2010a,
abstract = {State of the art visual SLAM systems have recently been presented which are capable of accurate, large-scale and real-time performance, but most of these require stereo vision. Important application areas in robotics and beyond open up if similar performance can be demonstrated using monocular vision, since a single camera will always be cheaper, more compact and easier to calibrate than a multi-camera rig. With high quality estimation, a single camera moving through a static scene of course effectively provides its own stereo geometry via frames distributed over time. However, a classic issue with monocular visual SLAM is that due to the purely projective nature of a single camera, motion estimates and map structure can only be recovered up to scale. Without the known inter-camera distance of a stereo rig to serve as an anchor, the scale of locally constructed map portions and the corresponding motion estimates is therefore liable to drift over time. In this paper we describe a new near real-time visual SLAM system which adopts the continuous keyframe optimisation ap- proach of the best current stereo systems, but accounts for the additional challenges presented by monocular input. In particular, we present a new pose-graph optimisation technique which allows for the efficient correction of rotation, translation and scale drift at loop closures. Especially, we describe the Lie group of similarity transformations and its relation to the corresponding Lie algebra.We also present in detail the systems new image processing front-end which is able accurately to track hundreds of features per frame, and a filter-based approach for feature initialisation within keyframe-based SLAM. Our approach is proven via large-scale simulation and real-world experiments where a camera completes large looped trajectories.},
author = {Strasdat, H. and {M. M. Montiel}, J. and Davison, A.},
doi = {10.15607/RSS.2010.VI.010},
file = {:home/chris/Documents/Mendeley Desktop/Scale Drift-Aware Large Scale Monocular SLAM.pdf:pdf},
isbn = {9780262516815},
issn = {2330765X},
journal = {Robot. Sci. Syst. VI},
title = {{Scale Drift-Aware Large Scale Monocular SLAM}},
url = {http://www.roboticsproceedings.org/rss06/p10.pdf},
year = {2010}
}
@misc{,
title = {{Photometric error discussion}},
url = {https://groups.google.com/forum/{\#}!topic/ceres-solver/2sMxmaSWMDo}
}
@article{Younes2016,
abstract = {Extensive research in the field of Visual SLAM for the past fifteen years has yielded workable systems that found their way into various applications, such as robotics and augmented reality. Although filter-based (e.g., Kalman Filter, Particle Filter) Visual SLAM systems were common at some time, non-filter based (i.e., akin to SfM solutions), which are more efficient, are becoming the de facto methodology for building a Visual SLAM system. This paper presents a survey that covers the various non-filter based Visual SLAM systems in the literature, detailing the various components of their implementation, while critically assessing the specific strategies made by each proposed system in implementing its components.},
archivePrefix = {arXiv},
arxivId = {1607.00470},
author = {Younes, Georges and Asmar, Daniel and Shammas, Elie},
eprint = {1607.00470},
file = {:home/chris/Documents/Mendeley Desktop/A survey on non-filter-based monocular Visual SLAM systems.pdf:pdf},
keywords = {monocular,non-filter based,visual slam},
number = {2012},
title = {{A survey on non-filter-based monocular Visual SLAM systems}},
url = {http://arxiv.org/abs/1607.00470},
year = {2016}
}
@article{Eade2013,
abstract = {Representation, exponential map, adjoint and jacobian of different manifolds with Lie algebra Different operations when sampling and statistics results when working with manifolds},
author = {Eade, Ethan},
file = {:home/chris/Documents/Mendeley Desktop/Lie Groups for 2D and 3D Transformations.pdf:pdf},
pages = {1--24},
title = {{Lie Groups for 2D and 3D Transformations}},
url = {http://ethaneade.com/lie.pdf},
year = {2013}
}
@inproceedings{Glover2012,
abstract = {Appearance-based loop closure techniques, which leverage the high information content of visual images and can be used independently of pose, are now widely used in robotic applications. The current state-of-the-art in the field is Fast Appearance-Based Mapping (FAB-MAP) having been demonstrated in several seminal robotic mapping experiments. In this paper, we describe OpenFABMAP, a fully open source implementation of the original FAB-MAP algorithm. Beyond the benefits of full user access to the source code, OpenFABMAP provides a number of configurable options including rapid codebook training and interest point feature tuning. We demonstrate the performance of OpenFABMAP on a number of published datasets and demonstrate the advantages of quick algorithm customisation. We present results from OpenFABMAP's application in a highly varied range of robotics research scenarios.},
address = {Saint Paul, MN},
author = {Glover, Arren and Maddern, William and Warren, Michael and Reid, Stephanie and Milford, Michael and Wyeth, Gordon},
booktitle = {Int. Conf. Robot. Autom.},
doi = {10.1109/ICRA.2012.6224843},
file = {:home/chris/Documents/Mendeley Desktop/OpenFABMAP$\backslash$: An Open Source Toolbox for Appearance-based Loop Closure Detection.pdf:pdf},
number = {May},
title = {{OpenFABMAP: An Open Source Toolbox for Appearance-based Loop Closure Detection}},
url = {https://eprints.qut.edu.au/50317/1/glover{\_}ICRA2012{\_}final.pdf},
year = {2012}
}
@misc{,
title = {{Scale Drift}},
url = {https://www.kudan.eu/kudan-news/scale-simultaneous-localisation-mapping/}
}
@article{Civera2006,
author = {Civera, Javier and Montiel, J M M},
file = {:home/chris/Documents/Mendeley Desktop/SLAM Summer School 2006.pdf:pdf},
journal = {Robotics},
pages = {1--5},
title = {{SLAM Summer School 2006 Practical 2 : SLAM using Monocular Vision}},
year = {2006}
}
@article{Thrun2008,
abstract = {This article provides a comprehensive introduction into the simultaneous localization and mapping problem, better known in its abbreviated form as SLAM. SLAM addresses the problem of a robot navigating an unknown environment. While navigating the environment, the robot seeks to acquire a map thereof, and at the same time it wishes to localize itself using its map. The use of SLAM problems can be motivated in two different ways. One might be interested in detailed environment models, or one might seek to maintain an accurate sense of a mobile robot's location. SLAM servers both of these purposes. We review three major paradigms of algorithms from which a huge number of recently published methods are derived. First comes the traditional approach, which relies in the extended Kalman filter (EKF) for representing the robot's best estimate. The second paradigm draws its intuition from the fact that the SLAM problem can be viewed as a sparse graph of constraints, and it applies nonlinear optimization for recovering the map and the robot's locations. Finally, we survey the particle filter paradigm, which applies non-parametric density estimation and efficient factorization methods to the SLAM problem. This article discusses extensions of these basic methods. 11, elucidates variants of the SLAM problem and poses a taxonomy for the field. Relevant research is referenced extensively, and open research problems are discussed.},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Thrun, S},
doi = {10.1109/MRA.2006.1638022},
eprint = {there is not},
file = {:home/chris/Documents/Mendeley Desktop/SLAM slides.pdf:pdf},
isbn = {1610-7438},
issn = {1070-9932},
journal = {Robot. Cogn. Approaches to Spat. Mapp.},
pages = {13--41},
pmid = {1638022},
title = {{CSC 2514 Lecture 6 SLAM}},
volume = {38},
year = {2008}
}
@article{Civera2008a,
abstract = {Abstract?We present a new parametrization for point features$\backslash$n$\backslash$nwithin monocular simultaneous localization and mapping$\backslash$n$\backslash$n(SLAM) that permits efficient and accurate representation of uncertainty$\backslash$n$\backslash$nduring undelayed initialization and beyond, all within$\backslash$n$\backslash$nthe standard extended Kalman filter (EKF). The key concept$\backslash$n$\backslash$nis direct parametrization of the inverse depth of features relative$\backslash$n$\backslash$nto the camera locations from which they were first viewed,$\backslash$n$\backslash$nwhich produces measurement equations with a high degree of$\backslash$n$\backslash$nlinearity. Importantly, our parametrization can cope with features$\backslash$n$\backslash$nover a huge range of depths, even those that are so far fromthe camera$\backslash$n$\backslash$nthat they present little parallax during motion?maintaining$\backslash$n$\backslash$nsufficient representative uncertainty that these points retain the$\backslash$nopportunity$\backslash$n$\backslash$nto ?come in? smoothly from infinity if the camera makes$\backslash$n$\backslash$nlarger movements. Feature initialization is undelayed in the sense$\backslash$n$\backslash$nthat even distant features are immediately used to improve camera$\backslash$n$\backslash$nmotion estimates, acting initially as bearing references but not$\backslash$n$\backslash$npermanently labeled as such. The inverse depth parametrization$\backslash$n$\backslash$nremains well behaved for features at all stages of SLAM processing,$\backslash$n$\backslash$nbut has the drawback in computational terms that each point is$\backslash$n$\backslash$nrepresented by a 6-D state vector as opposed to the standard three$\backslash$n$\backslash$nof a Euclidean XYZ representation. We show that once the depth$\backslash$n$\backslash$nestimate of a feature is sufficiently accurate, its representation$\backslash$ncan$\backslash$n$\backslash$nsafely be converted to the Euclidean XYZ form, and propose a$\backslash$n$\backslash$nlinearity index that allows automatic detection and conversion to$\backslash$n$\backslash$nmaintain maximum efficiency?only low parallax features need be$\backslash$n$\backslash$nmaintained in inverse depth form for long periods. We present a$\backslash$n$\backslash$nreal-time implementation at 30 Hz, where the parametrization is$\backslash$n$\backslash$nvalidated in a fully automatic 3-D SLAM system featuring a handheld$\backslash$n$\backslash$nsingle camera with no additional sensing. Experiments show$\backslash$n$\backslash$nrobust operation in challenging indoor and outdoor environments$\backslash$n$\backslash$nwith a very large ranges of scene depth, varied motion, and also$\backslash$n$\backslash$nreal time 360? loop closing.},
author = {Montiel, J M Martinez and Civera, Javier and Davison, Andrew J},
doi = {10.1109/TRO.2008.2003276},
file = {:home/chris/Documents/Mendeley Desktop/Unifed Inverse Depth Parametrization for Monocular SLAM.pdf:pdf},
isbn = {1552-3098 VO - 24},
issn = {15523098},
journal = {IEEE Trans. Robot.},
keywords = {2D,EKF-SLAM,SLAM,computer vision,localization,robotics},
number = {5},
pages = {932--945},
title = {{Unified Inverse Depth Parametrization for Monocular SLAM}},
volume = {24},
year = {2006}
}
@article{Engel2013,
abstract = {We propose a fundamentally novel approach to real-time visual odometry for a monocular camera. It allows to ben- efit from the simplicity and accuracy of dense tracking – which does not depend on visual features – while running in real-time on a CPU. The key idea is to continuously estimate a semi-dense inverse depth map for the current frame, which in turn is used to track the motion of the camera using dense image alignment. More specifically, we estimate the depth of all pixels which have a non-negligible image gradient. Each estimate is represented as a Gaussian probability distribution over the inverse depth. We propagate this information over time, and update it with new measurements as new images arrive. In terms of tracking accuracy and computational speed, the proposed method compares favorably to both state-of-the-art dense and feature-based visual odometry and SLAM algorithms. As our method runs in real-time on a CPU, it is of large practical value for robotics and augmented reality applications. 1.Towards},
author = {Engel, Jakob and Sturm, Jurgen and Cremers, Daniel},
doi = {10.1109/ICCV.2013.183},
file = {:home/chris/Documents/Mendeley Desktop/Semi-Dense Visual Odometry for a Monocular Camera.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
keywords = {SLAM,dense,monocular,stereo,visual odometry},
pages = {1449--1456},
pmid = {6130318},
title = {{Semi-dense visual odometry for a monocular camera}},
year = {2013}
}
@article{Engel2014a,
abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct meth- ods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on sim(3), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
author = {Engel, Jakob and Sch, Thomas and Cremers, Daniel},
doi = {10.1007/978-3-319-10605-2_54},
file = {:home/chris/Documents/Mendeley Desktop/LSD-SLAM Slides.pdf:pdf},
isbn = {978-3-319-10604-5},
issn = {16113349},
journal = {Eur. Conf. Comput. Vis.},
number = {PART 2},
pages = {834--849},
pmid = {638263},
title = {{LSD-SLAM: Large-Scale Direct Monocular SLAM (Slides)}},
volume = {8690 LNCS},
year = {2014}
}
@misc{,
title = {{The Future of Real-Time SLAM and Deep Learning vs SLAM}},
url = {http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html}
}
@unpublished{Mur-Artal2017,
abstract = {In recent years there have been excellent results in Visual-Inertial Odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However these approaches lack the capability to close loops, and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this work we present a novel tightly-coupled Visual-Inertial Simultaneous Localization and Mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We test our system in the 11 sequences of a recent micro-aerial vehicle public dataset achieving a typical scale factor error of 1{\%} and centimeter precision. We compare to the state-of-the-art in visual-inertial odometry in sequences with revisiting, proving the better accuracy of our method due to map reuse and no drift accumulation.},
archivePrefix = {arXiv},
arxivId = {1610.05949},
author = {Mur-Artal, Raul and Tardos, Juan D.},
booktitle = {IEEE Robot. Autom. Lett.},
doi = {10.1109/LRA.2017.2653359},
eprint = {1610.05949},
file = {:home/chris/Documents/Mendeley Desktop/Visual-Inertial Monocular SLAM with Map Reuse.pdf:pdf},
issn = {2377-3766},
title = {{Visual-Inertial Monocular SLAM with Map Reuse}},
url = {http://arxiv.org/abs/1610.05949{\%}0Ahttp://dx.doi.org/10.1109/LRA.2017.2653359},
year = {2017}
}
@techreport{Agarwal,
author = {Agarwal, Sameer and Snavely, Noah and Seitz, Steven M and Szeliski, Richard},
file = {:home/chris/Documents/Mendeley Desktop/Bundle Adjustment in the Large.pdf:pdf},
keywords = {bundle adjustment,precondi-,structure from motion,tioned conjugate gradients},
title = {{Bundle Adjustment in the Large}},
url = {https://homes.cs.washington.edu/{~}sagarwal/bal.pdf}
}
@article{Tardos,
author = {Tardos, Juan D},
file = {:home/chris/Documents/Mendeley Desktop/ORB SLAM 2 $\backslash$: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras.pdf:pdf},
title = {{ORB SLAM 2 : an Open-Source SLAM System for Monocular , Stereo and RGB-D Cameras}}
}
@article{Nerurkar2014,
abstract = {In this paper, we present C-KLAM, a Maximum A Posteriori (MAP) estimator-based keyframe approach for SLAM. Instead of discarding information from non-keyframes for reducing the computational complexity, the proposed C-KLAM presents a novel, elegant, and computationally-efficient technique for incorporating most of this information in a consistent manner, resulting in improved estimation accuracy. To achieve this, C-KLAM projects both proprioceptive and exteroceptive information from the non-keyframes to the keyframes, using marginalization, while maintaining the sparse structure of the associated information matrix, resulting in fast and efficient solutions. The performance of C-KLAM has been tested in experiments, using visual and inertial measurements, to demonstrate that it achieves performance comparable to that of the computationally-intensive batch MAP-based 3D SLAM, that uses all available measurement information.},
author = {Nerurkar, Esha D. and Wu, Kejian J. and Roumeliotis, Stergios I.},
doi = {10.1109/ICRA.2014.6907385},
file = {:home/chris/Documents/Mendeley Desktop/C-KLAM$\backslash$: Constrained Keyframe-Based Localization and Mapping.pdf:pdf},
isbn = {9781479936854},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3638--3643},
title = {{C-KLAM: Constrained keyframe-based localization and mapping}},
year = {2014}
}
@article{MassotM.;Bonin-Font2016,
author = {{Massot M.; Bonin-Font}, F.; Negre P.Ll.; Guerrero E.; Martorell A.; Oliver G},
file = {:home/chris/Documents/Mendeley Desktop/Massot M. Bonin-Font - 2016 - A 3D Mapping, Obstacle Avoidance and Acoustic Communication Payload for the AUV SPARUS II.pdf:pdf},
isbn = {9788461741526},
journal = {Seventh Int. Work. Mar. Technol.},
keywords = {autonomous underwater vehicle,sonar,stereo vi-,usbl},
pages = {29--32},
title = {{A 3D Mapping, Obstacle Avoidance and Acoustic Communication Payload for the AUV SPARUS II}},
year = {2016}
}
@article{Li,
author = {Li, A Quattrini and Coskun, A and Doherty, S M and Ghasemlou, S and Jagtap, A S and Rahman, S and Rekleitis, I},
file = {:home/chris/Documents/Mendeley Desktop/Li et al. - Unknown - On Understanding the Challenges in Vision-Based Shipwreck Mapping.pdf:pdf},
pages = {1--3},
title = {{On Understanding the Challenges in Vision-Based Shipwreck Mapping}}
}
@article{King2013,
abstract = {Autonomous underwater vehicles (AUV)s traversing a path will incur positional error drift over time while submerged. We are developing a route following system which is based upon features extracted from the seabed using sidescan sonar collected in a training phase. Through matching of sonar images, this system navigates over a path without the need for a continual global position estimate. At the core of this system is the need to reliably extract features and match images derived from the sonar. At our disposal is an array of algorithms which implement the OpenCV common interface for feature extraction and matching. Using pre-collected sets of data we compare the performance of several of these algorithms in the context of matching sonar image tiles. Our results compare the performance of various feature types over two common sets of data. The feature types tested include SIFT[12], SURF[3], MSER[13], STAR[1], ORB[15], and BRIEF[4]. View full abstract},
author = {King, P and Anstey, B and Vardy, a},
file = {:home/chris/Documents/Mendeley Desktop/King, Anstey, Vardy - 2013 - Comparison of feature detection techniques for AUV navigation along a trained route.pdf:pdf},
isbn = {9780933957404},
issn = {0197-7385},
journal = {Ocean. - San Diego, 2013},
keywords = {AUVs},
pages = {1--8},
title = {{Comparison of feature detection techniques for AUV navigation along a trained route}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=6741023{\&}matchBoolean=true{\&}rowsPerPage=30{\&}searchField=Search{\_}All{\&}queryText=(p{\_}Title:{\%}22Comparison+of+feature+detection+techniques+for+auv+navigation+along+a+trained+route{\%}22)},
year = {2013}
}
@article{Concha2015,
abstract = {In this paper we present an algorithm that estimates in real-time a 3D dense reconstruction of an underwater scene and the vehicle pose, being the only input a monocular image sequence. Our algorithm selects a set of keyframes from a seabed sequence and estimates a depth for every pixel from the information contained in the images using direct mapping methods. The procedure does not require extra sensing input or assumptions about the scene. Our experimental results in a pool and a seabed sequence show that such minimal sensing configuration can achieve a high degree of accuracy.},
author = {Concha, Alejo and Drews-Jr, Paulo and Campos, Mario and Civera, Javier},
doi = {10.1109/OCEANS-Genova.2015.7271476},
file = {:home/chris/Documents/Mendeley Desktop/Concha et al. - 2015 - Real-time localization and dense mapping in underwater environments from a monocular sequence.pdf:pdf},
isbn = {9781479987368},
journal = {MTS/IEEE Ocean. 2015 - Genova Discov. Sustain. Ocean Energy a New World},
title = {{Real-time localization and dense mapping in underwater environments from a monocular sequence}},
volume = {1},
year = {2015}
}
@article{Jung2016,
abstract = {In this paper, we propose a method of AUV localization and mapping using visual measurement of underwater structures. Since the inertial navigation system (INS) of AUV suffers from drift, visual observation of fixed objects can enhance the localization performance. In a framework of pose graph optimization, depth map estimation of underwater structures and tracing of AUV trajectory are concurrently handled. Estimation of absolute scale is also resolved by exploiting incremental measurements of inertial sensors. The proposed method is validated by experiments performed in a structured basin environment.},
author = {Jung, Jongdae and Choi, Jinwoo and Lee, Yeongjun and Choi, Hyun Taek},
doi = {10.1109/OCEANS.2016.7761113},
file = {:home/chris/Documents/Mendeley Desktop/Jung et al. - 2016 - AUV localization using depth perception of underwater structures from a monocular camera.pdf:pdf},
isbn = {9781509015375},
journal = {Ocean. 2016 MTS/IEEE Monterey, OCE 2016},
keywords = {auv,localization,vision},
pages = {444--446},
title = {{AUV localization using depth perception of underwater structures from a monocular camera}},
volume = {3},
year = {2016}
}
@article{Li2016,
abstract = {The problem of state estimation using primarily visual data has received a lot of attention in the last decade. Several open source packages have appeared addressing the problem, each supported by im- pressive demonstrations. Applying any of these packages on a new dataset however, has been proven extremely challenging. Suboptimal perfor- mance, loss of localization, and challenges in customization have not produced a clear winner. Several other research groups have presented superb performance without releasing the code, sometimes materializing as commercial products. In this paper, ten of the most promising open source packages are evaluated, by cross validating them on the datasets provided for each package and by testing them on eight different datasets collected over the years in our laboratory. Indoor and outdoor, terrestrial and flying vehicles, in addition to underwater robots, cameras, and buoys were used to collect data. An analysis on the motions required for the different approaches and an evaluation of their performance is presented.},
author = {Li, A. Quattrini and Coskun, A. and Doherty, S. M. and Ghasemlou, S.},
file = {:home/chris/Documents/Mendeley Desktop/Li et al. - 2016 - Experimental Comparison of open source Vision based State Estimation Algorithms.pdf:pdf},
keywords = {localization,slam,vision based state estimation},
number = {January},
title = {{Experimental Comparison of open source Vision based State Estimation Algorithms}},
year = {2016}
}
@article{Negre2016,
abstract = {— This paper reports on a novel technique to visually detect loop closings in feature-poor underwater environments in order to increase the accuracy of vision-based localization systems. The main problem of the classical visual Simultaneous Localization and Mapping (SLAM) for underwater vehicles is the lack of robust, stable and matchable features in certain aquatic environments. The presence of sandbanks, seagrass or other underwater phenomena cause the visual features to concentrate in regions heavily textured, leaving great image areas completely free of visual information. In this situation, the classical loop closing detection algorithms fail, resulting in no corrections for the SLAM system. Our novel method proposes to reinforce the loop closing detection by clustering visual keypoints present in multiple keyframes and to match features of clusters instead of features of keyframes. This new technique is assessed on the particular application of navigating an Autonomous Underwater Vehicle (AUV) in ma-rine environments colonized with seagrass or with the presence of sandbanks. Experiments conducted in several coastal zones on the Balearic Islands show a high degree of success in the visual registration of overlapping areas.},
author = {Negre, Pep Lluis and Bonin-Font, Francisco and Oliver, Gabriel},
doi = {10.1109/ICRA.2016.7487416},
file = {:home/chris/Documents/Mendeley Desktop/Negre, Bonin-Font, Oliver - 2016 - Cluster-based loop closing detection for underwater slam in feature-poor regions.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {2589--2595},
title = {{Cluster-based loop closing detection for underwater slam in feature-poor regions}},
volume = {2016-June},
year = {2016}
}
@article{Altuntas2016,
abstract = {? T?B?TAK.In recent decades, reinforcement learning (RL) has been widely used in different research fields ranging from psychology to computer science. The unfeasibility of sampling all possibilities for continuous-state problems and the absence of an explicit teacher make RL algorithms preferable for supervised learning in the machine learning area, as the optimal control problem has become a popular subject of research. In this study, a system is proposed to solve mobile robot navigation by opting for the most popular two RL algorithms, Sarsa(?) and Q(?) . The proposed system, developed in MATLAB, uses state and action sets, defined in a novel way, to increase performance. The system can guide the mobile robot to a desired goal by avoiding obstacles with a high success rate in both simulated and real environments. Additionally, it is possible to observe the effects of the initial parameters used by the RL methods, e.g., ?, on learning, and also to make comparisons between the performances of Sarsa(?) and Q(?) algorithms.},
author = {Altuntas, Nihal and Imal, Erkan and Emanet, Nahit and ??zt??rk, Ceyda Nur},
doi = {10.3906/elk-1311-129},
file = {:home/chris/Documents/Mendeley Desktop/Reinforcement learning-based mobile robot navigation.pdf:pdf},
issn = {13036203},
journal = {Turkish J. Electr. Eng. Comput. Sci.},
keywords = {Eligibility traces,Mobile robot navigation,Obstacle avoidance,Q-learning,Reinforcement learning,Sarsa,Temporal difference},
number = {3},
pages = {1747--1767},
title = {{Reinforcement learning-based mobile robot navigation}},
volume = {24},
year = {2016}
}
@article{Arana-Daniel2012,
abstract = {In this work, we propose the integration of two of the most widely used approaches for the implementation of autonomous navigation systems: the reinforcement learning for path finding, along with SLAM (Simultaneous Localization and Mapping) type algorithms for localization and mapping of the environment. These two approaches are integrated to address the problem of how a robot should explore an unknown and dynamic environment while it collects perception features in order to locate itself and, at the same time, to obtain information clues about cost traversability of an area. So, when a robot is exploring and mapping with a SLAM algorithm it is also learning to associate perception features with costs and actions to find optimal paths from the starting point to the goal point in dynamical environments.},
author = {Arana-Daniel, N},
file = {:home/chris/Documents/Mendeley Desktop/Reinforcement Learning-SLAM for finding minimum cost path and mapping.pdf:pdf},
isbn = {9781467344975},
issn = {2154-4824},
journal = {World Autom. {\ldots}},
pages = {1--6},
title = {{Reinforcement Learning-SLAM for finding minimum cost path and mapping}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6320898},
year = {2012}
}
@article{Prasad2016,
abstract = {Effective SLAM using a single monocular camera is highly preferred due to its simplicity. However, when compared to trajectory planning methods using depth-based SLAM, Monocular SLAM in loop does need additional considerations. One main reason being that for the optimization, in the form of Bundle Adjustment (BA), to be robust, the SLAM system needs to scan the area for a reasonable duration. Most monocular SLAM systems do not tolerate large camera rotations between successive views and tend to breakdown. Other reasons for Monocular SLAM failure include ambiguities in decomposition of the Essential Matrix, feature-sparse scenes and more layers of non linear optimization apart from BA. This paper presents a novel formulation based on Reinforcement Learning (RL) that generates fail safe trajectories wherein the SLAM generated outputs (scene structure and camera motion) do not deviate largely from their true values. Quintessentially, the RL framework successfully learns the otherwise complex relation between motor actions and perceptual inputs that result in trajectories that do not cause failure of SLAM, which are almost intractable to capture in an obvious mathematical formulation. We show systematically in simulations how the quality of the SLAM map and trajectory dramatically improves when trajectories are computed by using RL.},
archivePrefix = {arXiv},
arxivId = {1607.07558},
author = {Prasad, Vignesh and Singh, Saurabh and Pareekutty, Nahas and Ravindran, Balaraman and Krishna, Madhava},
doi = {10.1103/PhysRevA.94.013830},
eprint = {1607.07558},
file = {:home/chris/Documents/Mendeley Desktop/SLAM-Safe Planner$\backslash$: Preventing Monocular SLAM Failureusing Reinforcement Learning.pdf:pdf},
title = {{SLAM-Safe Planner: Preventing Monocular SLAM Failure using Reinforcement Learning}},
url = {http://arxiv.org/abs/1607.07558},
year = {2016}
}
@article{Leutenegger2013,
abstract = {The fusion of visual and inertial cues has become popular in robotics due to the complementary nature of the two sensing modalities. While most fusion strategies to date rely on filtering schemes, the visual robotics community has recently turned to non-linear optimization approaches for tasks such as visual Simultaneous Localization And Mapping (SLAM), following the discovery that this comes with signifi- cant advantages in quality of performance and computational complexity. Following this trend, we present a novel approach to tightly integrate visual measurements with readings from an Inertial Measurement Unit (IMU) in SLAM. An IMU error term is integrated with the landmark reprojection error in a fully probabilistic manner, resulting to a joint non-linear cost function to be optimized. Employing the powerful concept of ‘keyframes' we partially marginalize old states to maintain a bounded-sized optimization window, ensuring real-time opera- tion. Comparing against both vision-only and loosely-coupled visual-inertial algorithms, our experiments confirm the benefits of tight fusion in terms of accuracy and robustness.},
author = {Leutenegger, S and Furgale, P and Rabaud, V and Chli, M and Konolige, K and Siegwart, R},
doi = {10.1177/0278364914554813},
file = {:home/chris/Documents/Mendeley Desktop/Keyframe-Based Visual-Inertial SLAM Using Nonlinear Optimization.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {Proc. Robot. Sci. Syst.},
pages = {0},
title = {{Keyframe Based Visual Inertial SLAM Using Nonlinear Optimization}},
year = {2013}
}
@article{Tan2013,
author = {Tan, Wei},
doi = {10.1109/ISMAR.2013.6671781},
file = {:home/chris/Documents/Mendeley Desktop/RDSLAM SLIDES.pdf:pdf},
isbn = {9781479928699},
issn = {09295593},
pmid = {6671781},
title = {{Robust Monocular SLAM in Dynamic Environments}},
year = {2013}
}
@article{Hornung2009,
author = {Hornung, a.},
file = {:home/chris/Documents/Mendeley Desktop/Learning Policies for Reliable Mobile Robot Localization.pdf:pdf},
journal = {Arminhornung.De},
number = {January},
title = {{Learning Policies for Reliable Mobile Robot Localization}},
url = {http://www.arminhornung.de/Study/files/diplomarbeit{\_}hornung.pdf},
year = {2009}
}
@article{Augmented2015,
author = {Augmented, Qualcomm and Lecture, Reality},
file = {:home/chris/Documents/Mendeley Desktop/orb-slam-a-real-time-accurate-monocular-slam-system.pdf:pdf},
title = {{ORB-SLAM: a Real-Time Accurate Monocular SLAM System}},
year = {2015}
}
@article{Ahtiainen,
author = {Ahtiainen, Juhana},
file = {:home/chris/Documents/Mendeley Desktop/Exploration and Other Applications of Reinforcement Learning in Robotics.pdf:pdf},
journal = {Automation.Tkk.Fi},
title = {{Exploration and Other Applications of Reinforcement Learning in Robotics}},
url = {http://automation.tkk.fi/attach/AS-84-4340/Exploration.pdf}
}
@article{Dong,
author = {Dong, Zilong},
file = {:home/chris/Documents/Mendeley Desktop/Keyframe-Based{\_}Real-Time{\_}Camera{\_}Tracking.pdf:pdf},
title = {{Keyframe-Based Real-Time Camera Tracking}}
}
@article{Knopp2013,
author = {Knopp, Martin},
file = {:home/chris/Documents/Mendeley Desktop/Reinforcement{\_}Learning{\_}for{\_}Robotic{\_}Navigation SLIDES.pdf:pdf},
pages = {1--17},
title = {{Reinforcement Learning for Robotic Navigation}},
year = {2013}
}
@inproceedings{Tan2013a,
author = {Tan, Wei},
booktitle = {IEEE Int. Symp. Mix. Augment. Real. 2013},
doi = {978-1-4799-2869-9/13},
file = {:home/chris/Documents/Mendeley Desktop/Robust Monocular SLAM in Dynamic Environments.pdf:pdf},
title = {{Robust Monocular SLAM in Dynamic Environments}},
year = {2013}
}
@article{Samsonovich1997,
abstract = {A minimal synaptic architecture is proposed for how the brain might perform path integration by computing the next internal representation of self-location from the current representation and from the perceived velocity of motion. In the model, a place-cell assembly called a "chart" contains a two-dimensional attractor set called an "attractor map" that can be used to represent coordinates in any arbitrary environment, once associative binding has occurred between chart locations and sensory inputs. In hippocampus, there are different spatial relations among place fields in different environments and behavioral contexts. Thus, the same units may participate in many charts, and it is shown that the number of uncorrelated charts that can be encoded in the same recurrent network is potentially quite large. According to this theory, the firing of a given place cell is primarily a cooperative effect of the activity of its neighbors on the currently active chart. Therefore, it is not particularly useful to think of place cells as encoding any particular external object or event. Because of its recurrent connections, hippocampal field CA3 is proposed as a possible location for this "multichart" architecture; however, other implementations in anatomy would not invalidate the main concepts. The model is implemented numerically both as a network of integrate-and-fire units and as a "macroscopic" (with respect to the space of states) description of the system, based on a continuous approximation defined by a system of stochastic differential equations. It provides an explanation for a number of hitherto perplexing observations on hippocampal place fields, including doubling, vanishing, reshaping in distorted environments, acquiring directionality in a two-goal shuttling task, rapid formation in a novel environment, and slow rotation after disorientation. The model makes several new predictions about the expected properties of hippocampal place cells and other cells of the proposed network.},
author = {Samsonovich, a and McNaughton, B L},
doi = {10.1146/ANNUREV.PSYCH.53.100901.135114},
file = {:home/chris/Documents/Mendeley Desktop/Path Integration and Cognitive Mapping in a Continuous AttractorNeural Network Model.pdf:pdf},
isbn = {0270-6474 (Print)$\backslash$n0270-6474 (Linking)},
issn = {0270-6474},
journal = {J. Neurosci.},
keywords = {allocentric,attractor,ca3,cognitive map,dead reckoning,head direction,hippocampus,idiothetic,individual and multiple parallel,integrate-and-fire,it is known from,learning,path integration,place cells,recordings of,spatial},
number = {15},
pages = {5900--5920},
pmid = {9221787},
title = {{Path integration and cognitive mapping in a continuous attractor neural network model.}},
volume = {17},
year = {1997}
}
@article{Lopez2011,
abstract = {In this paper we propose a new approach for$\backslash$nactive SLAM (Simultaneous Localization And Mapping) of$\backslash$nnonholonomic vehicles. Both the environment and the vehicle$\backslash$nmodel are unknown in advance, thus the path planner uses$\backslash$nreinforcement learning to acquire the vehicle model, which is$\backslash$nestimated by a reduced set of transitions. At the same time,$\backslash$nthe vehicle explores the environment creating a consistent map$\backslash$nthrough optimal path motion. The mapping is represented$\backslash$nby sets of ordered and weighted data points, named objects,$\backslash$nthat provide some advantages with respect to conventional$\backslash$nmethods. In order to guide the navigation and to build a map$\backslash$nof the environment the planner employs a three-dimensional$\backslash$ncontroller based on the concept of virtual wall following. Both$\backslash$nsimulation and experimental results are reported to show the$\backslash$nsatisfactory performance of the method.},
author = {Lopez, Eduardo and {De Bernadis}, Caleb and Martinez-Marin, Tomas},
file = {:home/chris/Documents/Mendeley Desktop/An Active SLAM Approach for Autonomous Navigation of Nonholonomic Vehicles.pdf:pdf},
pages = {291----296},
title = {{An Active SLAM Approach for Autonomous Navigation of$\backslash$nNonholonomic Vehicles}},
url = {http://aass.oru.se/Agora/ECMR2011/proceedings/papers/ECMR2011{\_}0040.pdf},
year = {2011}
}
@article{Bhatti2016,
abstract = {A number of recent approaches to policy learning in 2D game domains have been successful going directly from raw input images to actions. However when employed in complex 3D environments, they typically suffer from challenges related to partial observability, combinatorial exploration spaces, path planning, and a scarcity of rewarding scenarios. Inspired from prior work in human cognition that indicates how humans employ a variety of semantic concepts and abstractions (object categories, localisation, etc.) to reason about the world, we build an agent-model that incorporates such abstractions into its policy-learning framework. We augment the raw image input to a Deep Q-Learning Network (DQN), by adding details of objects and structural elements encountered, along with the agent's localisation. The different components are automatically extracted and composed into a topological representation using on-the-fly object detection and 3D-scene reconstruction.We evaluate the efficacy of our approach in Doom, a 3D first-person combat game that exhibits a number of challenges discussed, and show that our augmented framework consistently learns better, more effective policies.},
archivePrefix = {arXiv},
arxivId = {1612.00380},
author = {Bhatti, Shehroze and Desmaison, Alban and Miksik, Ondrej and Nardelli, Nantas and Siddharth, N. and Torr, Philip H. S.},
eprint = {1612.00380},
file = {:home/chris/Documents/Mendeley Desktop/Playing Doom with SLAM-Augmented Deep Reinforcement Learning.pdf:pdf},
title = {{Playing Doom with SLAM-Augmented Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1612.00380},
year = {2016}
}
@inproceedings{Liu2016,
abstract = {Keyframe-based SLAM has achieved great success in terms of ac- curacy, efficiency and scalability. However, due to parallax re- quirement and delay of map expansion, traditional keyframe-based methods easily encounter the robustness problem in the challeng- ing cases especially for fast motion with strong rotation. For AR applications in practice, these challenging cases are easily encoun- tered, since a home user may not carefully move the camera to avoid potential problems. With the above motivation, in this paper, we present RKSLAM, a robust keyframe-based monocular SLAM system that can reliably handle fast motion and strong rotation, ensuring good AR experiences. First, we propose a novel multi- homography based feature tracking method which is robust and ef- ficient for fast motion and strong rotation. Based on it, we propose a real-time local map expansion scheme to triangulate the observed 3D points immediately without delay. A sliding-window based camera pose optimization framework is proposed, which imposes the motion prior constraints between consecutive frames through simulated or real IMU data. Qualitative and quantitative compar- isons with the state-of-the-art methods, and an AR application on mobile devices demonstrate the effectiveness of the proposed ap- proach. Index},
author = {Liu, Haomin and Zhang, Guofeng and Bao, Hujun},
booktitle = {IEEE Int. Symp. Mix. Augment. Real. Robust},
doi = {10.1109/ISMAR.2016.24},
file = {:home/chris/Documents/Mendeley Desktop/Robust Keyframe-based Monocular SLAM for Augmented Reality.pdf:pdf},
isbn = {9781509036417},
title = {{Robust Keyframe-based Monocular SLAM for Augmented Reality}},
year = {2016}
}
@article{Niko2012a,
abstract = {Current SLAM back-ends are based on least squares optimization and thus are not robust against outliers like data association errors and false positive loop closure detections. Our paper presents and evaluates a robust back-end formulation for SLAM using switchable constraints. Instead of proposing yet another appearance-based data association technique, our system is able to recognize and reject outliers during the optimization. This is achieved by making the topology of the underlying factor graph representation subject to the optimization instead of keeping it fixed. The evaluation shows that the approach can deal with up to 1000 false positive loop closure constraints on various datasets. This largely increases the robustness of the overall SLAM system and closes a gap between the sensor-driven front-end and the back-end optimizers.},
author = {Sunderhauf, Niko and Protzel, Peter},
doi = {10.1109/IROS.2012.6385590},
file = {:home/chris/Documents/Mendeley Desktop/Niko, Protzel - 2012 - Towards a Robust Back-End for Pose Graph SLAM.pdf:pdf},
isbn = {9781467317375},
issn = {21530858},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {1879--1884},
title = {{Switchable constraints for robust pose graph SLAM}},
year = {2012}
}
@inproceedings{Suenderhauf2012,
abstract = {Current SLAM back-ends are based on least squares optimization and thus are not robust against outliers like data association errors and false positive loop closure detections. Our paper presents and evaluates a robust back-end formulation for SLAM using switchable constraints. Instead of proposing yet another appearance-based data association technique, our system is able to recognize and reject out- liers during the optimization. This is achieved by making the topology of the underlying factor graph representation subject to the optimization instead of keeping it fixed. The evaluation shows that the approach can deal with up to 1000 false positive loop closure constraints on various datasets. This largely increases the robustness of the overall SLAM system and closes a gap between the sensor-driven front-end and the back-end optimizers},
author = {S{\"{u}}nderhauf, Niko and Protzel, Peter},
booktitle = {IEEE Conf. Intell. Robot. Syst.},
file = {:home/chris/Documents/Mendeley Desktop/Niko, Protzel - 2012 - Towards a Robust Back-End for Pose Graph SLAM.pdf:pdf},
title = {{Switchable Constraints for Robust Pose Graph SLAM}},
year = {2012}
}
@article{Strasdat2010,
abstract = {— While the most accurate solution to off-line struc-ture from motion (SFM) problems is undoubtedly to extract as much correspondence information as possible and perform global optimisation, sequential methods suitable for live video streams must approximate this to fit within fixed computational bounds. Two quite different approaches to real-time SFM — also called monocular SLAM (Simultaneous Localisation and Mapping) — have proven successful, but they sparsify the problem in different ways. Filtering methods marginalise out past poses and summarise the information gained over time with a probability distribution. Keyframe methods retain the optimisation approach of global bundle adjustment, but computationally must select only a small number of past frames to process. In this paper we perform the first rigorous analysis of the relative advantages of filtering and sparse optimisation for sequential monocular SLAM. A series of experiments in simulation as well using a real image SLAM system were performed by means of covariance propagation and Monte Carlo methods, and comparisons made using a combined cost/accuracy measure. With some well-discussed reservations, we conclude that while filtering may have a niche in systems with low processing resources, in most modern applications keyframe optimisation gives the most accuracy per unit of computing time.},
author = {Strasdat, Hauke and Montiel, J. M M and Davison, Andrew J.},
doi = {10.1109/ROBOT.2010.5509636},
file = {:home/chris/Documents/Mendeley Desktop/Real-time Monocular SLAM$\backslash$: Why Filter?.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {2657--2664},
pmid = {5509636},
title = {{Real-time monocular SLAM: Why filter?}},
year = {2010}
}
@inproceedings{Dubbelman2013,
abstract = {— A novel closed-form solution for pose-graph SLAM is presented. It optimizes pose-graphs of particular structure called pose-chains by employing an extended version of trajectory bending. Our solution is designed as a back-end optimizer to be used within systems whose front-end performs state-of-the-art visual odometry and appearance based loop detection. The optimality conditions of our closed-form method and that of state-of-the-art iterative methods are discussed. The practical relevance of their theoretical differences is investigated by extensive experiments using simulated and real data. It is shown using 49 kilometers of challenging binocular data that the accuracy obtained by our closed-form solution is comparable to that of state-of-the-art iterative solutions while the time it needs to compute its solution is a factor 50 to 200 times lower. This makes our approach relevant to a broad range of applications and computational platforms.},
author = {Dubbelman, Gijs and Browning, Brett},
booktitle = {IEEE Int. Conf. Robot. Autom.},
file = {:home/chris/Documents/Mendeley Desktop/Closed-form Online Pose-chain SLAM.pdf:pdf},
title = {{Closed-form Online Pose-chain SLAM}},
url = {http://gijsdubbelman.com/joomla/media/pdf/icra2013.pdf},
year = {2013}
}
@article{Klein2007,
abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.5736v1},
author = {Klein, Georg and Murray, David},
doi = {10.1109/ISMAR.2007.4538852},
eprint = {arXiv:1407.5736v1},
file = {:home/chris/Documents/Mendeley Desktop/Parallel Tracking and Mapping for Small AR Workspaces.pdf:pdf},
isbn = {9781424417506},
issn = {00472778},
journal = {2007 6th IEEE ACM Int. Symp. Mix. Augment. Reality, ISMAR},
pmid = {21736739},
title = {{Parallel tracking and mapping for small AR workspaces}},
year = {2007}
}
@article{Colares2012,
author = {Colares, Rafael Goncalves and Chaimowicz, Luiz},
doi = {10.1109/SBR-LARS.2012.42},
file = {:home/chris/Documents/Mendeley Desktop/Planning for Simultaneous Localization and Mapping using Topological Information.pdf:pdf},
isbn = {978-0-7695-4906-4},
journal = {2012 Brazilian Robot. Symp. Lat. Am. Robot. Symp.},
pages = {214--219},
title = {{Planning for Simultaneous Localization and Mapping Using Topological Information}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6363345},
year = {2012}
}
@article{Eliazar2003,
abstract = {We present a novel, laser range finder based algorithm for simultaneous localization and mapping (SLAM) for mobile robots. SLAM addresses the problem of con- structing an accurate map in real time despite imperfect information about the robot's trajectory through the en- vironment. Unlike other approaches that assume prede- termined landmarks (and must deal with a resulting data- association problem) our algorithm is purely laser based. Our algorithm uses a particle filter to represent both robot poses and possible map configurations. By using a new map representation, which we call distributed particle (DP) mapping, we are able to maintain and update hun- dreds of candidate maps and robot poses efficiently. The worst-case complexity of our algorithm per laser sweep is log-quadratic in the number of particles we maintain and linear in the area swept out by the laser. However, in practice our run time is usually much less than that. Our technique contains essentially no assumptions about the environment yet it is accurate enough to close loops of 60m in length with crisp, perpendicular edges on corri- dors and minimal or no misalignment errors.},
author = {Eliazar, Austin and Parr, Ronald},
doi = {10.1109/IROS.2009.5354248},
file = {:home/chris/Documents/Mendeley Desktop/DP-SLAM Fast, Robust Simultaneous Localization and Mapping Without Predetermined Landmarks.pdf:pdf},
isbn = {978-1-4244-3803-7},
issn = {10450823},
journal = {IJCAI Int. Jt. Conf. Artif. Intell.},
pages = {1135--1142},
title = {{DP-SLAM: Fast, robust simultaneous localization and mapping without predetermined landmarks}},
year = {2003}
}
@article{Chakravorty2008,
abstract = {In this paper, the problem of mapping and planning in an uncertain environment is studied. A hybrid Bayesian/ frequentist formulation of the simultaneous planning, localization and mapping (SPLAM) problem is presented wherein the environment is modeled as a stationary, spatially uncorrelated random process whose stationary probabilities are fixed but unknown, and have to be estimated as the autonomous system moves through the environment and makes observations using its sensors. The environmental random process is estimated using stochastic approximation algorithms. Under a certain "reliable sensor assumption", it is shown that the mapping algorithms converge with probability one, and that the convergence of the mapping algorithms is independent of the planning policy, as long as it is non-anticipative, akin to the celebrated "Separation Principle" in Classical Linear Control theory. Further, the computational burden of the mapping algorithms is significantly reduced when compared to Bayesian SPLAM techniques.},
author = {Chakravorty, Suman and Saha, R.},
doi = {10.1109/ACC.2008.4586660},
file = {:home/chris/Documents/Mendeley Desktop/Simultaneous Planning Localization and Mapping$\backslash$:A Hybrid Bayesian Frequentist Approach.pdf:pdf},
isbn = {9781424420797},
issn = {07431619},
journal = {Proc. Am. Control Conf.},
pages = {1226--1231},
title = {{Simultaneous planning localization and mapping: A hybrid Bayesian/frequentist approach}},
year = {2008}
}
@article{Shukla2015,
abstract = {In this paper, we solve the traditional problem of autonomous Localization and Mapping by inter- leaving planning for exploring unknown environments by a mobile robot and Simultaneous Localization and Mapping (SLAM). We denote such planned SLAM systems as SPLAM (Simultaneous Planning Lo- calization and Mapping). The main aim of SPLAM is to plan paths for the SLAM process such that the robot and map uncertainty upon execution of the path remains minimum and tractable. The planning is interleaved with SLAM and hence the terminology SPLAM. While typical SPLAM routines find paths when the robot traverses amidst known regions of the constructed map, herein we use the SPLAM for- mulation for an exploration like situation. Exploration is carried out through a frontier based approach where we identify multiple frontiers in the known map. Using Randomized Trajectory Generation tech- niques we calculate various possible trajectories to all the known frontiers. A practical SPLAM system must be fast and also be able to handle presence of dynamic objects in the environment. We've provided solutions to these two problems separately. For developing a fast planning system we introduce a novel strategy for selecting the trajectory which mimics Fast SLAM, selects a trajectory for robot motion that will minimize the map and robot state covariance. By using a Fast SLAM like approach for selecting frontiers we are able to decouple the robot and landmark covariance resulting in a faster selection of next best location, while maintaining the same kind of robustness of an EKF based SPLAM framework. We then compare our results with Shortest Path Algorithm and EKF based Planning. We show significant reduction in covariance when compared with shortest frontier first approach, while the uncertainties are comparable to EKF-SPLAM albeit at much faster planning times. For planning in Dynamic Environments while the typical planning methods for Dynamic Environ- ments involve filtering the dynamic objects and then planning for the static environment thereafter ob- tained, we've developed a novel planning algorithm which is intelligently able to decide whether to keep or discard the dynamic objects. If we are able to make use of them to aide localization they are kept in the SLAM framework, but if found hindering the localization they are discarded. For this, along with selecting one of the trajectories generated through the Trajectory Generation system, we also comes up with an action policy. This action policy dictates what action, from a given set of actions, will the robot take at each node of the trajectory. Also for the process of SLAM which is intermediate to planning, we devise a EKF-SLAM based framework which is capable of incorporating Dynamic Objects as opposed to the traditional SLAM methods. We then compare our results with a method that unilaterally filters all moving objects, which we call as the dynamic object filtering method or simply filtering method in short. We show significant reduction in robot and map uncertainty when compared with the dynamic object filtering planning method, for both simulated and real environments.},
author = {Shukla, Piyush},
file = {:home/chris/Documents/Mendeley Desktop/SPLAM.pdf:pdf},
number = {March},
title = {{Simultaneous Planning Localization and Mapping}},
year = {2015}
}
@article{Terejanu2003,
abstract = {Consider the following nonlinear system, described by the difference equation and the observation model with additive noise: x k = f(x k−1) + w k−1 (1) z k = h(x k) + v k (2) The initial state x 0 is a random vector with known mean µ 0 = E[x 0 ] and covariance P 0 = E[(x 0 − µ 0)(x 0 − µ 0) T ]. In the following we assume that the random vector w k captures uncertainties in the model and v k denotes the measurement noise. Both are temporally uncorrelated (white noise), zero-mean random sequences with known covariances and both of them are uncorrelated with the initial state x 0 . E[w k ] = 0 E[w k w T k ] = Q k E[w k w T j ] = 0 for k = j E[w k x T 0 ] = 0 for all k (3) E[v k ] = 0 E[v k v T k ] = R k E[v k v T j ] = 0 for k = j E[v k x T 0 ] = 0 for all k (4) Also the two random vectors w k and v k are uncorrelated: E[w k v T j ] = 0 for all k and j (5) Vectorial functions f({\textperiodcentered}) and h({\textperiodcentered}) are assumed to be C 1 functions (the function and its first derivative are continuous on the given domain). Dimension and description of variables: x k n × 1 − State vector w k n × 1 − Process noise vector z k m × 1 − Observation vector v k m × 1 − Measurement noise vector f({\textperiodcentered}) n × 1 − Process nonlinear vector function h({\textperiodcentered}) m × 1 − Observation nonlinear vector function Q k n × n − Process noise covariance matrix R k m × m − Measurement noise covariance matrix},
author = {Terejanu, Ga},
file = {:home/chris/Documents/Mendeley Desktop/Tutorial EKF.pdf:pdf},
journal = {Tech. Rep. Ext. Kalman Filter Tutor.},
pages = {7},
title = {{Extended kalman filter tutorial}},
url = {http://homes.cs.washington.edu/{~}todorov/courses/cseP590/readings/tutorialEKF.pdf},
year = {2003}
}
@inproceedings{Abouzahir2014,
author = {Abouzahir, Mohamed and Elouardi, Abdelhafid and Bouaziz, Samir and Latif, Rachid and Tajer, Abdelouahed},
booktitle = {2014 13th Int. Conf. Control Autom. Robot. Vision, ICARCV 2014},
doi = {10.1109/ICARCV.2014.7064524},
file = {:home/chris/Documents/Mendeley Desktop/FastSLAM 2.0 Running On a Low-Cost Embedded Architecture.pdf:pdf},
isbn = {9781479951994},
keywords = {Embedded systems,FastSLAM 2.0,Parallel implementation},
number = {December},
pages = {1421--1426},
title = {{FastSLAM 2.0 running on a low-cost embedded architecture}},
year = {2014}
}
@article{Montemerlo2003,
author = {Montemerlo, Michael and Thrun, Sebastian and phne Koller, Da and Wegbreit, Ben},
file = {:home/chris/Documents/Mendeley Desktop/FastSLAM 2.0$\backslash$: An Improved Particle Filtering Algorithm for Simultaneous Localization and Mapping that Provably Converges .pdf:pdf},
journal = {Int. Jt. Conf. Artif. Intell. IJCAI},
pages = {1151--1156},
title = {{Fast SLAM 2.0 : an improved particle filtering algorithm for simultaneous localization and mapping that provably converges}},
year = {2003}
}
@article{Calonder,
author = {Calonder, Michael},
file = {:home/chris/Documents/Mendeley Desktop/EKF SLAM vs. FastSLAM – A Comparison.pdf:pdf},
pages = {1--5},
title = {{EKF SLAM vs. FastSLAM - A Comparison}}
}
@article{Paz2008,
abstract = {In this paper, we show that all processes associated with the move-sense-update cycle of extended Kalman filter (EKF) Simultaneous Localization and Mapping (SLAM) can be carried out in time linear with the number of map features. We describe Divide and Conquer SLAM, which is an EKF SLAM algorithm in which the computational complexity per step is reduced from O(n 2) to O(n), and the total cost of SLAM is reduced from O(n 3) to O(n 2). Unlike many current large-scale EKF SLAM techniques, this algorithm computes a solution without relying on approximations or simplifications (other than linearizations) to reduce computational complexity. Also, estimates and covariances are available when needed by data association without any further computation. Furthermore, as the method works most of the time in local maps, where angular errors remain small, the effect of linearization errors is limited. The resulting vehicle and map estimates are more precise than those obtained with standard EKF SLAM. The errors with respect to the true value are smaller, and the computed state covariance is consistent with the real error in the estimation. Both simulated experiments and the Victoria Park dataset are used to provide evidence of the advantages of this algorithm.},
author = {Paz, L.M. and Tardos, J.D. and Neira, J.},
doi = {10.1109/TRO.2008.2004639},
file = {:home/chris/Documents/Mendeley Desktop/Divide and Conquer$\backslash$: EKF SLAM in O(n).pdf:pdf},
isbn = {9781612843858},
issn = {1552-3098},
journal = {IEEE Trans. Robot.},
number = {5},
pages = {1107--1120},
title = {{Divide and Conquer: EKF SLAM in O(n)}},
volume = {24},
year = {2008}
}
@article{Thrun2006,
abstract = {This article presentsGraphSLAM,a unifying algorithm for the offline SLAM problem. GraphSLAM is closely related to a recent sequence of research papers on applying optimization techniques to SLAM problems. It transforms the SLAM posterior into a graphical net- work, representing the log-likelihood of the data. It then reduces this graph using variable elimination techniques, arriving at a lower- dimensional problems that is then solved using conventional opti- mization techniques. As a result, GraphSLAM can generate maps with 108 or more features. The paper discusses a greedy algorithm for data association, and presents results for SLAM in urban envi- ronments with occasional GPS measurements.},
author = {Thrun, Sebastian and Montemerlo, Michael},
doi = {10.1177/0278364906065387},
file = {:home/chris/Documents/Mendeley Desktop/The Graph SLAM Algorithm with Applications to Large-Scale Mapping of Urban Structures.pdf:pdf},
isbn = {0278364906065},
journal = {Int. J. Rob. Res.},
keywords = {localization,mapping,robot navigation},
pages = {403--429},
title = {{The GraphSLAM Algorithm with Applications to Large-Scale Mapping of Urban Structures}},
volume = {25},
year = {2006}
}
@article{Stachniss2016,
author = {Stachniss, Cyrill},
file = {:home/chris/Documents/Mendeley Desktop/Graph SLAM SLIDES.pdf:pdf},
title = {{Graph-Based SLAM ??}},
year = {2016}
}
@techreport{Wang,
abstract = {Pose graphs have become an attractive representa- tion for solving Simultaneous Localization and Mapping (SLAM) problems. In this paper, we analyze the structure of the nonlinear- ities in the 2D SLAM problem formulated as the optimizing of a pose graph. First, we prove that finding the optimal configuration of a very basic pose graph with 3 nodes (poses) and 3 edges (relative pose constraints) with spherical covariance matrices, which can be formulated as a six dimensional least squares optimization problem, is equivalent to solving a one dimensional optimization problem. Then we show that the same result can be extended to the optimizing of a pose graph with “two anchor nodes” where every edge is connecting to one of the two anchor nodes. Furthermore, we prove that the global minimum of the resulting one dimensional optimization problem must belong to a certain interval and there are at most 3 minima in that interval. Thus the globally optimal pose configuration of the pose graph can be obtained very easily through the bisection method and closed-form formulas.},
address = {Beijing, China},
author = {Wang, Heng and Hu, Gibson and Huang, Shoudong and Dissanayake, Gamini},
file = {:home/chris/Documents/Mendeley Desktop/On the Structure of Nonlinearitiesin Pose Graph SLAM.pdf:pdf},
institution = {Beijing University of Technology},
title = {{On the Structure of Nonlinearities in Pose Graph SLAM}}
}
@article{Montemerlo2002,
author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
file = {:home/chris/Documents/Mendeley Desktop/FastSLAM$\backslash$: A Factored Solution to the Simultaneous Localization and Mapping Problem.pdf:pdf},
title = {{FastSLAM : A Factored Solution to the Simultaneous Localization and Mapping Problem}},
url = {http://www.cs.cmu.edu/{~}mmde/mmdeaaai2002.pdf},
year = {2002}
}
@misc{Abbeel,
author = {Abbeel, Pieter},
file = {:home/chris/Documents/Mendeley Desktop/Graph SLAM SLIDES - 2.pdf:pdf},
publisher = {UC Berkeley EECS},
title = {{GraphSLAM (Slides)}},
url = {https://people.eecs.berkeley.edu/{~}pabbeel/cs287-fa15/slides/lecture24-Graph-SLAM.pdf},
urldate = {2017-08-20}
}
@inproceedings{Olson2006,
abstract = {A robot exploring an environment can estimate its own motion and the relative positions of features in the environment. Simultaneous Localization and Mapping (SLAM) algorithms attempt to fuse these estimates to produce a map and a robot trajectory. The constraints are generally non-linear, thus SLAM can be viewed as a non-linear optimization problem. The optimization can be difficult, due to poor initial estimates arising from odometry data, and due to the size of the state space. We present a fast non-linear optimization algorithm that rapidly recovers the robot trajectory, even when given a poor initial estimate. Our approach uses a variant of Stochastic Gradient Descent on an alternative state-space representation that has good stability and computational properties.We compare our algorithm to several others, using both real and synthetic data sets.},
author = {Olson, Edwin and Leonard, John and Teller, Seth},
booktitle = {ICRA(International Conf. Robot. Autom.},
file = {:home/chris/Documents/Mendeley Desktop/Fast Iterative Alignment of Pose Graphswith Poor Initial Estimates.pdf:pdf},
number = {May},
pages = {2262--2269},
title = {{Fast Iterative Alignment of Pose Graphs with Poor Initial Estimates}},
year = {2006}
}
@article{Samsuri2015,
author = {{S. B. Samsuri, H. Zamzuri, M. A. A. Rahman, S. A. Mazlan}, A. H. A. Rahman},
file = {:home/chris/Documents/Mendeley Desktop/COMPUTATIONAL COST ANALYSIS OF EXTENDED KALMAN FILTER IN SIMULTANEOUS LOCALIZATION {\&} MAPPING (EKF-SLAM) PROBLEM FOR AUTONOMOUS VEHICLE.pdf:pdf},
journal = {Appl. Mech. Mater.},
keywords = {autonomous vehicle,computational cost,extended kalman filter,slam},
number = {17},
pages = {1--8},
title = {{Computational Cost Analysis Of Extended Kalman Filter In Simultaneous Localization $\backslash${\&} Mapping (EKF-SLAM) Problem For Autonomous Vehicle}},
volume = {10},
year = {2015}
}
@article{Pizzoli2014,
abstract = {In this paper, we solve the problem of estimating dense and accurate depth maps from a single moving camera. A probabilistic depth measurement is carried out in real time on a per-pixel basis and the computed uncertainty is used to reject erroneous estimations and provide live feedback on the reconstruction progress. Our contribution is a novel approach to depth map computation that combines Bayesian estimation and recent development on convex optimization for image processing.We demonstrate that our method outperforms state- of-the-art techniques in terms of accuracy, while exhibiting high efficiency in memory usage and computing power. We call our approach REMODE (REgularized MOnocular Depth Estimation). Our CUDA-based implementation runs at 30Hz on a laptop computer and is released as open-source software.},
author = {Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide},
doi = {10.1109/ICRA.2014.6907233},
file = {:home/chris/Documents/Mendeley Desktop/REMODE$\backslash$: Probabilistic, Monocular Dense Reconstruction in Real Time.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {2609--2616},
title = {{REMODE: Probabilistic, monocular dense reconstruction in real time}},
year = {2014}
}
@article{Liu2003,
abstract = {We present an algorithm for the multi-robot simultaneous localization and mapping (SLAM) problem. Our algorithm enables teams of robots to build joint maps, even if their relative starting locations are unknown and landmarks are ambiguous—$\backslash$r$\backslash$nwhich is presently an open problem in robotics. It achieves this capability through a sparse information filter technique, which represents maps and robot poses by Gaussian Markov random fields. The alignment of local maps into a single global maps is achieved by a tree-based algorithm for searching similar-looking local landmark configurations, paired with a hill climbing algorithm that maximizes the overall likelihood by search in the space of correspondences. We report favorable results obtained with a real-world benchmark data set.},
author = {Liu, Yufeng and Thrun, Sebastian},
file = {:home/chris/Documents/Mendeley Desktop/Gaussian Multi-Robot SLAM.pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst.},
title = {{Gaussian Multi-Robot SLAM}},
year = {2003}
}
@article{Howard2006,
abstract = {This paper describes an on-line algorithm for multi-robot simultaneous localization and mapping (SLAM). The starting point is the single-robot Rao-Blackwellized particle filter described by Hahnel et al., and three key generalizations are made. First, the particle filter is extended to handle multi-robot SLAM problems in which the initial pose of the robots is known (such as occurs when all robots start from the same location). Second, an approximation is introduced to solve the more general problem in which the initial pose of robots is not known a priori (such as occurs when the robots start from widely separated locations). In this latter case, it is assumed that pairs of robots will eventually encounter one another, thereby determining their relative pose. This relative attitude is used to initialize the filter, and subsequent observations from both robots are combined into a common map. Third and finally, a method is introduced to integrate observations collected prior to the first robot encounter, using the notion of a virtual robot travelling backwards in time. This novel approach allows one to integrate all data from all robots into a single common map.},
author = {Howard, A.},
doi = {10.1177/0278364906072250},
file = {:home/chris/Documents/Mendeley Desktop/Multi-robot Simultaneous Localization and Mapping using Particle Filters.pdf:pdf},
isbn = {0278364906072},
issn = {0278-3649},
journal = {Int. J. Rob. Res.},
number = {12},
pages = {1243--1256},
title = {{Multi-robot Simultaneous Localization and Mapping using Particle Filters}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364906072250},
volume = {25},
year = {2006}
}
@article{Zhou2006,
abstract = {This paper presents a new approach to the multi-robot map-alignment problem that enables teams of robots to build joint maps without initial knowledge of their relative poses. The key contribution of this work is an optimal algorithm for merging (not necessarily overlapping) maps that are created by different robots independently. Relative pose measurements between pairs of robots are processed to compute the coordinate transformation between any two maps. Noise in the robot-to-robot observations, propagated through the map-alignment process, increases the error in the position estimates of the transformed landmarks, and reduces the overall accuracy of the merged map. When there is overlap between the two maps, landmarks that appear twice provide additional information, in the form of constraints, which increases the alignment accuracy. Landmark duplicates are identified through a fast nearest-neighbor matching algorithm. In order to reduce the computational complexity of this search process, a kd-tree is used to represent the landmarks in the original map. The criterion employed for matching any two landmarks is the Mahalanobis distance. As a means of validation, we present experimental results obtained from two robots mapping an area of 4,800 m{\textless}sup{\textgreater}2{\textless}/sup{\textgreater}},
author = {Zhou, Xun S. and Roumeliotis, Stergios I.},
doi = {10.1109/IROS.2006.282219},
file = {:home/chris/Documents/Mendeley Desktop/Multi-robot SLAM with Unknown Initial Correspondence$\backslash$:The Robot Rendezvous Case.pdf:pdf},
isbn = {142440259X},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {1785--1792},
title = {{Multi-robot SLAM with unknown initial correspondence: The robot rendezvous case}},
year = {2006}
}
@unpublished{Choudhary2016,
author = {Choudhary, Siddharth and Carlone, Luca and Nieto, Carlos and Rogers, John and Liu, Zhen and Christensen, Henrik I and Dellaert, Frank},
file = {:home/chris/Documents/Mendeley Desktop/Multi Robot Object-based SLAM.pdf:pdf},
pages = {1--12},
title = {{Multi Robot Object-based SLAM}},
url = {https://www.cc.gatech.edu/{~}choudhar/publications/Choudhary16iser.pdf},
year = {2016}
}
@article{Hahnel2005,
abstract = {We present a lazy data association algorithm for the simultaneous localization and mapping (SLAM) problem. Our approach uses a tree-structured Bayesian representation of map posteriors that makes it possible to revise data association decisions arbitrarily far into the past. We describe a criterion for detecting and repairing poor data association decisions. This technique makes it possible to acquire maps of large-scale environments with many loops, with a minimum of computational overhead for the management of multiple data association hypotheses. A empirical comparison with the popular FastSLAM algorithm shows the advantage of lazy over proactive data association.},
author = {H{\"{a}}hnel, Dirk and Thrun, Sebastian and Wegbreit, Ben and Burgard, Wolfram},
doi = {10.1007/11008941_45},
file = {:home/chris/Documents/Mendeley Desktop/Towards Lazy Data Association in SLAM.pdf:pdf},
isbn = {978-3-540-23214-8},
issn = {16107438},
journal = {Robot. Res.},
number = {Ml},
pages = {421--431},
title = {{Towards Lazy Data Association in SLAM}},
url = {http://www.springerlink.com/content/24l0jj2tq08c6tjm},
year = {2005}
}
@article{Neira,
author = {Neira, J},
file = {:home/chris/Documents/Mendeley Desktop/The Data Association Problem.pdf:pdf},
title = {{3 . The Data Association Problem}}
}
@phdthesis{Alexandre2013,
author = {Alexandre, Simoes Martins Joao},
file = {:home/chris/Documents/Mendeley Desktop/MRSLAM - Multi-Robot SimultaneousLocalization and Mapping.pdf:pdf},
pages = {67},
school = {Universidade de Coimbra},
title = {{MRSLAM - Multi-Robot Simultaneous Localization and Mapping}},
type = {Dissertation},
url = {http://ap.isr.uc.pt/archive/MRSLAM{\_}dissertacao{\_}Joao{\_}Martins-vfinal-040913{\_}235208.pdf},
year = {2013}
}
@article{Milford2004,
abstract = {The paper presents a new approach to the problem of Simultaneous Localization and Mapping - SLAM - inspired by computational models of the hippocampus of rodents. The rodents hippocampus has been extensively studied with respect to navigation tasks, and displays many of the properties of a desirable SLAM solution. RatSLAM is an implementation of a hippocampal model that can perform SLAM in real time on a real robot. It uses competitive attractor network to integrate odometric information with landmark sensing to form a consistent representation of the environment. Experimental results show that RatSLAM can operate with ambiguous landmark information and recover from both minor and major path integration errors.},
author = {Milford, Michael and Wyeth, Gordon and Prasser, David},
doi = {10.1109/ROBOT.2004.1307183},
file = {:home/chris/Documents/Mendeley Desktop/RatSLAM$\backslash$: A Hippocampal Model for Simultaneous Localization and Mapping.pdf:pdf},
isbn = {0780382323},
issn = {1050-4729},
journal = {Robot. Autom. {\ldots}},
keywords = {hippocampus,mobile robot,slam},
number = {May 2004},
pages = {403--408},
title = {{RatSLAM: a hippocampal model for simultaneous localization and mapping}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1307183},
year = {2004}
}
@article{Lenac2015,
author = {Lenac, Kruno},
file = {:home/chris/Documents/Mendeley Desktop/Fast Active SLAM for Accurate and CompleteCoverage Mapping of Unknown Environments.pdf:pdf},
isbn = {9781467314053},
journal = {Intell. Auton. Syst.},
keywords = {4,5,active slam,also exists dividing them,based slam systems,e,exploration,feature based and pose,g,gaussian distribution,particles with general non,path planning,slam,the distinction in representation,to,where the later},
pages = {1--8},
title = {{Fast Active SLAM for Accurate and Complete Coverage Mapping of Unknown Environments}},
url = {https://www.fer.unizg.hr/{\_}download/repository/Kruno{\_}Lenac{\_}KDI.pdf},
year = {2015}
}
@article{Mu2016,
abstract = {Active SLAM is the task of actively planning robot paths while simultaneously building a map and localizing within. Existing work has focused on planning paths with occupancy grid maps, which do not scale well and suffer from long term drift. This work proposes a Topological Feature Graph (TFG) representation that scales well and develops an active SLAM algorithm with it. The TFG uses graphical models, which utilize independences between variables, and enables a unified quantification of exploration and exploitation gains with a single entropy metric. Hence, it facilitates a natural and principled balance between map exploration and refinement. A probabilistic roadmap path-planner is used to generate robot paths in real time. Experimental results demonstrate that the proposed approach achieves better accuracy than a standard grid-map based approach while requiring orders of magnitude less computation and memory resources.},
archivePrefix = {arXiv},
arxivId = {1509.08155},
author = {Mu, Beipeng and Giamou, Matthew and Paull, Liam and Agha-Mohammadi, Ali Akbar and Leonard, John and How, Jonathan},
doi = {10.1109/CDC.2016.7799127},
eprint = {1509.08155},
file = {:home/chris/Documents/Mendeley Desktop/Information-based Active SLAM via Topological Feature Graphs.pdf:pdf},
isbn = {9781509018376},
journal = {2016 IEEE 55th Conf. Decis. Control. CDC 2016},
pages = {5583--5590},
title = {{Information-based Active SLAM via topological feature graphs}},
year = {2016}
}
@article{Leung2008,
author = {Leung, C and Huang, S and Dissanayake, G},
file = {:home/chris/Documents/Mendeley Desktop/Active SLAM in Structured Environments.pdf:pdf},
pages = {6},
title = {{Active SLAM in Structure Environments}},
url = {papers://a95032f2-422b-4b2a-927a-da2e44727e65/Paper/p12672},
year = {2008}
}
@article{Ball2013,
abstract = {RatSLAM is a navigation system based on the neural processes underlying navigation in the rodent brain, capable of operating with low resolution monocular image data. Seminal experiments using RatSLAM include mapping an entire suburb with a web camera and a long term robot delivery trial. This paper describes OpenRatSLAM, an open-source version of RatSLAM with bindings to the Robot Operating System framework to leverage advantages such as robot and sensor abstraction, networking, data playback, and visualization. OpenRatSLAM comprises connected ROS nodes to represent RatSLAM's pose cells, experience map, and local view cells, as well as a fourth node that provides visual odometry estimates. The nodes are described with reference to the RatSLAM model and salient details of the ROS implementation such as topics, messages, parameters, class diagrams, sequence diagrams, and parameter tuning strategies. The performance of the system is demonstrated on three publicly available open-source datasets.},
author = {Ball, David and Heath, Scott and Wiles, Janet and Wyeth, Gordon and Corke, Peter and Milford, Michael},
doi = {10.1007/s10514-012-9317-9},
file = {:home/chris/Documents/Mendeley Desktop/OpenRatSLAM$\backslash$: an open source brain-based SLAM system.pdf:pdf},
isbn = {0929-5593},
issn = {09295593},
journal = {Auton. Robots},
keywords = {Appearance-based,Brain-based,Hippocampus,Mapping,Navigation,Open-source,OpenRatSLAM,ROS,RatSLAM,SLAM},
number = {3},
pages = {149--176},
title = {{OpenRatSLAM: An open source brain-based SLAM system}},
volume = {34},
year = {2013}
}
@article{Trivun2015,
abstract = {In this paper, we present an algorithm for fully autonomous exploration and mapping of an unknown indoor robot environment. This algorithm is based on the active SLAM (simultaneous localization and mapping) approach. The mobile robot equipped with laser sensor builds a map of an environment, while keeping track of its current location. Autonomy is introduced to this system by automatically setting goal points so that either previously unknown space is mapped, or known landmarks are revisited in order to increase map accuracy. Final aim is to maximize both map coverage and accuracy. The proposed procedure is experimentally verified on Pioneer 3-DX mobile robot in real environment, using ROS framework for implementation.},
author = {Trivun, Darko and Salaka, Edin and Osmankovic, Dinko and Velagic, Jasmin and Osmic, Nedim},
doi = {10.1109/ICIT.2015.7125079},
file = {:home/chris/Documents/Mendeley Desktop/Active SLAM-Based Algorithm for Autonomous Exploration withMobile Robot.pdf:pdf},
isbn = {978-1-4799-7800-7},
journal = {Proc. IEEE Int. Conf. Ind. Technol.},
number = {June},
pages = {74--79},
title = {{Active SLAM-based algorithm for autonomous exploration with mobile robot}},
volume = {2015-June},
year = {2015}
}
@article{Torres-Gonzalez2014,
abstract = {This paper is motivated by schemes of robot-sensor network cooperation where sensor nodes (beacons) are used as landmarks for Range-Only (RO) Simultaneous Localization and Mapping (SLAM). Most existing RO-SLAM techniques consider beacons as passive devices disregarding the sensing, computing and communication capabilities they are actually endowed with. This paper proposes a Range-Only scheme based on Sparse Extended Information Filters (SEIF) that efficiently exploits their capabilities. The robot computes the SLAM prediction stage and distributes the update stage among beacons within its sensing area. The proposed scheme naturally integrates robot-beacon and inter-beacon measurements, significantly improving map and also robot estimations. Our scheme inherits from SEIF its efficiency and scalability and further reduces robot computational burden by exploiting the beacons computing capability. As a result, it has lower error and lower computer requirements than traditional methods. This paper presents the scheme, evaluates and compares its performance in simulations and real experiments.},
author = {Torres-Gonzalez, A. and {Martinez-De Dios}, J. R. and Ollero, A.},
doi = {10.1109/ICRA.2014.6907023},
file = {:home/chris/Documents/Mendeley Desktop/Efficient Robot-Sensor Network Distributed SEIF Range-Only SLAM.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {1319--1326},
title = {{Efficient robot-sensor network distributed SEIF range-only SLAM}},
year = {2014}
}
@article{Walter2007,
abstract = {Recent research concerning the Gaussian canonical form for Simulta- neous Localization and Mapping (SLAM) has given rise to a handful of algorithms that attempt to solve the SLAM scalability problem for arbitrarily large environments. One such estimator that has received due attention is the Sparse Extended Information Filter (SEIF) pro- posed by Thrun et al., which is reported to be nearly constant time, ir- respective of the size of the map. The key to the SEIF's scalability is to prune weak links in what is a dense information (inverse covariance) matrix to achieve a sparse approximation that allows for efficient, scalable SLAM. We demonstrate that the SEIF sparsification strat- egy yields error estimates that are overconfident when expressed in the global reference frame, while empirical results show that relative map consistency is maintained.$\backslash$nIn this paper, we propose an alternative scalable estimator based on an information form that maintains sparsity while preserving con- sistency. The paper describes a method for controlling the population of the information matrix, whereby we track a modified version of the SLAM posterior, essentially by ignoring a small fraction of temporal measurements. In this manner, the Exactly Sparse Extended Informa- tion Filter (ESEIF) performs inference over a model that is conserv- ative relative to the standard Gaussian distribution. We compare our algorithm to the SEIF and standard EKF both in simulation as well as on two nonlinear datasets. The results convincingly show that our method yields conservative estimates for the robot pose and map that are nearly identical to those of the EKF.},
author = {Walter, M. R. and Eustice, R. M. and Leonard, J. J.},
doi = {10.1177/0278364906075026},
file = {:home/chris/Documents/Mendeley Desktop/Exactly Sparse Extended Information Filters forFeature-Based SLAM.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {Int. J. Rob. Res.},
number = {4},
pages = {335--359},
title = {{Exactly Sparse Extended Information Filters for Feature-based SLAM}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364906075026},
volume = {26},
year = {2007}
}
@article{Wu2015,
abstract = {Consider the two characteristics: (1) Simultaneous localization and mapping (SLAM) is a popular algorithm for autonomous underwater vehicle, but visual SLAM is significantly influenced by weak illumination. (2)Geomagnetism-aided navigation and gravity-aided navigation are equally important methods in the field of vehicle navigation, but both are affected heavily by time-varying noises and terrain fluctuations. However, magnetic gradient vector can avoid the influence of time-varying noises, and is less affected by terrain fluctuations. To this end, we propose an adaptive SLAM-based magnetic gradient aided navigation with the following advantages: (1) Adaptive SLAM is an efficient way to deal with uncertainty of the measurement model. (2) Magnetic gradient inversion equation is a good alternative to be used as measurement equation in visual SLAM-denied environment. Experimental results show that our proposed method is an effective solution, combining magnetic gradient information with SLAM. {\textcopyright} 2015 IEEE.},
author = {Wu, Meng and Yao, Jian},
doi = {10.1007/978-3-319-22876-1_21},
file = {:home/chris/Documents/Mendeley Desktop/Adaptive UKF-SLAM based on Magnetic Gradient In-version Method for Underwater Navigation.pdf:pdf},
isbn = {9783319228754},
issn = {16113349},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {Adaptive SLAM,Magnetic gradient inversion equation,Magnetic gradient vector},
pages = {237--247},
title = {{Adaptive UKF-SLAM based on magnetic gradient inversion method for underwater navigation}},
volume = {9245},
year = {2015}
}
@techreport{Milford,
author = {Milford, Michael},
file = {:home/chris/Documents/Mendeley Desktop/RatSLAM Slides.pdf:pdf},
institution = {The University of British Columbia},
title = {{RatSLAM and Grid Cells}}
}
@article{Zaffari2016,
abstract = {—This paper presents the bio-inspired underwater 3D SLAM algorithm called DolphinSLAM. First, every module of the DolphinSLAM algorithm is explained. Then, the effects of param-eter variations regarding the parameters of the DolphinSLAM algorithm are investigated based on the use of the Underwater Simulator (UWSim). The parameters of interest are i) the image feature extractors, ii) the vocabulary size regarding the BoW model, and iii) the equations for experience map correction.},
author = {Zaffari, Guilherme B. and {Dos Santos}, Matheus M. and Duarte, Amanda C. and {De Fernandes}, Daniel A. and {Da Botelho}, Silvia S C},
doi = {10.1109/OCEANSAP.2016.7485531},
file = {:home/chris/Documents/Mendeley Desktop/Exploring the DolphinSLAM parameters.pdf:pdf},
isbn = {9781467397247},
keywords = {Bio-inspired,DolphinSLAM,SLAM},
title = {{Exploring the DolphinSLAM's parameters}},
year = {2016}
}
@article{Milford2005,
abstract = {RatSLAM is a system for vision-based Simul- taneous Localisation and Mapping (SLAM) inspired by models of the rodent hippocampus. The system can produce stable representations of large complex environments during robot experiments in both indoor and outdoor en- vironments. These representations are both topological and metric in nature, and can involve multiple representations of the same place as well as discontinuities. In this paper we describe a new technique known as experience mapping that can be used on-line with the RatSLAMsystem to produce world representa- tions known as experience maps. These maps group together multiple place representations and are spatially continuous. A number of experiments have been conducted in simulation and a real world office environment. These experiments demonstrate the high degree to which experiencemaps are representative of the spatial arrangement of the environment.},
author = {Milford, Michael J. and Prasser, David and Wyeth, Gordon F.},
file = {:home/chris/Documents/Mendeley Desktop/Experience Mapping$\backslash$: Producing Spatially Continuous EnvironmentRepresentations using RatSLAM.pdf:pdf},
isbn = {0958758379},
journal = {Proc. Australas. Conf. Robot. Autom. 2005},
pages = {1--10},
title = {{Experience Mapping : Producing Spatially Continuous Environment Representations using RatSLAM}},
url = {http://eprints.qut.edu.au/32840/},
year = {2005}
}
@book{Milford2008,
abstract = {This book describes the development of a robot mapping and navigation system inspired by models of the neural mechanisms underlying spatial navigation in the rodent hippocampus. Computational models of animal navigation systems have traditionally had limited performance when implemented on robots. The aim of the work was to determine the extent to which hippocampal models can be used to provide a robot with functional mapping and navigation capabilities in real world environments. The focus of the research was on achieving practical robot performance, rather than maintaining biological plausibility.},
author = {Milford, Michael John},
booktitle = {Springer Tracts Adv. Robot.},
doi = {10.1007/978-3-540-77520-1},
file = {:home/chris/Documents/Mendeley Desktop/Robot Navigation from Nature.pdf:pdf},
isbn = {9783540775195},
issn = {1610-7438},
pages = {196},
title = {{Robot Navigation from Nature}},
volume = {41},
year = {2008}
}
@article{Maddern2009,
abstract = {This paper investigates the use of the FAB-MAP appearance-only SLAM algorithm as a method for performing visual data association for RatSLAM, a semi-metric full SLAM system. While both systems have shown the ability to map large (60-70km) outdoor locations of approximately the same scale, for either larger areas or across longer time periods both algorithms encounter difficulties with false positive matches. By combining these algorithms using a mapping between appearance and pose space, both false positives and false negatives generated by FAB-MAP are significantly reduced during outdoor mapping using a forward-facing camera. The hybrid FAB-MAP-RatSLAM system developed demonstrates the potential for successful SLAM over large periods of time.},
author = {Maddern, William and Glover, Arren and Milford, Michael and Wyeth, Gordon},
file = {:home/chris/Documents/Mendeley Desktop/Augmenting RatSLAM using FAB-MAP-based Visual Data Association.pdf:pdf},
isbn = {9780980740400},
journal = {Proc. Australas. Conf. Robot. Autom. 2009},
number = {October},
title = {{Augmenting RatSLAM using FAB-MAP-based visual data association}},
year = {2009}
}
@article{Milford2006,
abstract = {The RatSLAM system can perform vision based SLAM using a computational model of the rodent hippocampus. When the number of pose cells used to represent space in RatSLAM is reduced, artifacts are introduced that hinder its use for goal directed navigation. This paper describes a new component for the RatSLAM system called an experience map, which provides a coherent representation for goal directed navigation. Results are presented for two sets of real world experiments, including comparison with the original goal memory system's performance in the same environment. Preliminary results are also presented demonstrating the ability of the experience map to adapt to simple short term changes in the environment},
author = {Milford, Michael and Wyeth, Gordon and Prasser, David},
doi = {10.1109/IROS.2006.281869},
file = {:home/chris/Documents/Mendeley Desktop/RatSLAM on the Edge$\backslash$: Revealing a Coherent Representation from an Overloaded Rat Brain.pdf:pdf},
isbn = {142440259X},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
keywords = {Goal,Mapping,Navigation,SLAM},
pages = {4060--4065},
title = {{RatSLAM on the edge: Revealing a coherent representation from an overloaded rat brain}},
year = {2006}
}
@article{Paz2007,
abstract = {—In this paper we show that all processes associated to the move-sense-update cycle of EKF SLAM can be carried out in time linear in the number of map features. We describe Divide and Conquer SLAM, an EKF SLAM algorithm where the computational complexity per step is reduced from O(n 2 ) to O(n) (the total cost of SLAM is reduced from O(n 3 ) to O(n 2 )). In addition, the resulting vehicle and map estimates have better consistency properties than standard EKF SLAM in the sense that the computed state covariance more adequately represents the real error in the estimation. Both simulated experiments and the Victoria Park Dataset are used to provide evidence of the advantages of this algorith},
author = {Paz, L. M. and Guivant, J. and Tard{\'{o}}s, J. D. and Neira, J.},
file = {:home/chris/Documents/Mendeley Desktop/Data Association for Divide and Conquer SLAM.pdf:pdf},
isbn = {9780262524841},
issn = {2330765X},
journal = {Robot. Sci. Syst.},
pages = {281 -- 288},
title = {{Data Association in O(n) for Divide and Conquer SLAM}},
volume = {3},
year = {2007}
}
@article{Grisetti2009,
author = {Grisetti, Giorgio and Stachniss, Cyrill and Arras, Kai and Burgard, Wolfram},
file = {:home/chris/Documents/Mendeley Desktop/Data Association Slides.pdf:pdf},
title = {{Robotics 2 Data Association}},
year = {2009}
}
@article{Slam2011,
author = {Slam, E K F},
file = {:home/chris/Documents/Mendeley Desktop/Data Association for SLAM.pdf:pdf},
pages = {1--8},
title = {{Data Association for SLAM}},
year = {2011}
}
@article{Nieto2003,
abstract = {The ability to simultaneously localise a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. This paper presents a real-world implementation of FastSLAM, an algorithm that recursively estimates the full posterior distribution of both robot pose and landmark locations. In particular, we present an extension to FastSLAM that addresses the data association problem using a nearest neighbor technique. Building on this, we also present a novel multiple hypotheses tracking implementation (MHT) to handle uncertainty in the data association. Finally an extension to the multi-robot case is introduced. Our algorithm has been run successfully using a number of data sets obtained in outdoor environments. Experimental results are presented that demonstrate the performance of the algorithms when compared with standard Kalman filter-based approaches.},
author = {Nieto, J. and Guivant, J. and Nebot, E. and Thrun, S.},
doi = {10.1109/ROBOT.2003.1241630},
file = {:home/chris/Documents/Mendeley Desktop/Real Time Data Association for FastSLAM.pdf:pdf},
isbn = {0-7803-7736-2},
issn = {1050-4729},
journal = {2003 IEEE Int. Conf. Robot. Autom. (Cat. No.03CH37422)},
keywords = {Australia,Bayes methods,Bayesian estimation,Bayesian methods,Computer science,FastSLAM,Kalman filter,Kalman filters,Particle filters,Partitioning algorithms,Recursive estimation,Robot sensing systems,Simultaneous localization and mapping,Uncertainty,data uncertainty,decentralised robot,full posterior distribution,landmark locations,mobile robots,multi-robot systems,multiple hypotheses tracking,multirobot case,navigation,nearest neighbor technique,real time data association,robot navigation,robot pose,stability,tracking},
number = {1},
pages = {412--418},
title = {{Real time data association for FastSLAM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1241630},
volume = {1},
year = {2003}
}
@article{Engel2014,
abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by ltering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on sim(3), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the e ect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
author = {Engel, Jakob and Sch, Thomas and Cremers, Daniel},
doi = {10.1007/978-3-319-10605-2_54},
file = {:home/chris/Documents/Mendeley Desktop/LSD-SLAM$\backslash$: Large-ScaleDirect Monocular SLAM.pdf:pdf},
isbn = {9783319106045},
issn = {16113349},
pages = {834--849},
title = {{LSD-SLAM: Large-Scale Direct Monocular SLAM}},
year = {2014}
}
@article{Naminski2013,
author = {Naminski, Megan R},
file = {:home/chris/Documents/Mendeley Desktop/An Analysis of Simultaneous Localization andMapping (SLAM) Algorithms.pdf:pdf},
keywords = {EKF,SLAM,Simultaneous Localization and Mapping},
title = {{An Analysis of Simultaneous Localization and Mapping ( SLAM ) Algorithms}},
url = {http://digitalcommons.macalester.edu/cgi/viewcontent.cgi?article=1030{\&}context=mathcs{\_}honors},
year = {2013}
}
@article{Riisgaard2004,
abstract = {A Tutorial Approach to Simultaneous Localization and Mapping},
archivePrefix = {arXiv},
arxivId = {321-330},
author = {Riisgaard, S{\o}ren and Blas, Morten Rufus},
doi = {10.1017/S0025315400002526},
eprint = {321-330},
file = {:home/chris/Documents/Mendeley Desktop/SLAM for Dummies.pdf:pdf},
isbn = {9781424448814},
issn = {00253154},
journal = {A Tutor. Approach to Simultaneous Localization Mapp.},
number = {June},
pages = {1--127},
title = {{SLAM for Dummies}},
url = {http://ocw.num.edu.mn/NR/rdonlyres/Aeronautics-and-Astronautics/16-412JSpring-2005/9D8DB59F-24EC-4B75-BA7A-F0916BAB2440/0/1aslam{\_}blas{\_}repo.pdf},
volume = {22},
year = {2004}
}
@article{Visatemongkolchai2007,
abstract = {This thesis describes the use of two sequential machine learning techniques for building a mobile robot's motion model, which is an essential component in SLAM algorithms. For building a static motion model, we apply the recursive least squares (RLS) algorithm to learn motion model parameters as soon as new data (the robot's pose from odometry readings and ground-truth poses) arrive. For building a dynamic motion model, our framework uses the bi-loop recursive least squares (BiRLS) algorithm to learn the parameters on the fly as the robot traverses the environment. These techniques for building motion models are integrated with FastSLAM 2.0 giving increased autonomy to the system by eliminating the human effort required to produce motion models. The performance of our newly acquired motion models are then evaluated objectively based on the robot's ground-truth poses in the context of FastSLAM 2.0.},
author = {Visatemongkolchai, Artit and Zhang, Hong},
doi = {10.1109/ROBIO.2007.4522409},
file = {:home/chris/Documents/Mendeley Desktop/Building probabalisitic motion models for SLAM.pdf:pdf},
isbn = {9781424417582},
journal = {2007 IEEE Int. Conf. Robot. Biomimetics, ROBIO},
pages = {1629--1634},
title = {{Building probabilistic motion models for SLAM}},
year = {2007}
}
@article{Sola2014,
abstract = {Simultaneous localization and mapping (SLAM) is the problem of concurrently estimat- ing in real time the structure of the surrounding world (the map), perceived by moving exteroceptive sensors, while simultaneously getting localized in it. The seminal solution to the problem by Smith and Cheeseman (1987) [2] employs an extended Kalman filter (EKF) as the central estimator, and has been used extensively.$\backslash$nThis file is an accompanying document for a SLAM course I give at ISAE in Toulouse every winter. Please find all the Matlab code generated during the course at the end of this document.},
author = {Sola, Joan},
file = {:home/chris/Documents/Mendeley Desktop/SLAM with the EKF.pdf:pdf},
journal = {Unpubl. Available http//www. joansola. eu/JoanSola/eng/JoanSola. html},
pages = {1--35},
title = {{Simulataneous localization and mapping with the extended Kalman filter}},
url = {http://www.iri.upc.edu/people/jsola/JoanSola/objectes/curs{\_}SLAM/SLAM2D/SLAM course.pdf},
year = {2014}
}
@article{Mur-Artal2015,
abstract = {This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
archivePrefix = {arXiv},
arxivId = {1502.00956},
author = {Mur-Artal, Raul and Montiel, J. M M and Tardos, Juan D.},
doi = {10.1109/TRO.2015.2463671},
eprint = {1502.00956},
file = {:home/chris/Documents/Mendeley Desktop/ORB-SLAM$\backslash$: A Versatile and Accurate MonocularSLAM System.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Trans. Robot.},
keywords = {Lifelong mapping,Simultaneous localization and mapping (SLAM),localization,monocular vision,recognition},
number = {5},
pages = {1147--1163},
pmid = {21736739},
title = {{ORB-SLAM: A Versatile and Accurate Monocular SLAM System}},
volume = {31},
year = {2015}
}
@article{Alcantarilla,
abstract = {One of the main drawbacks of standard visual EKF-SLAM techniques is the assumption of a general camera motion model. Usually this motion model has been implemented in the literature as a constant linear and angular velocity model. Because of this, most approaches cannot deal with sudden camera movements, causing them to lose accurate camera pose and leading to a corrupted 3D scene map. In this work we propose increasing the robustness of EKF-SLAM techniques by replacing this general motion model with a visual odometry prior, which provides a real-time relative pose prior by tracking many hundreds of features from frame to frame. We perform fast pose estimation using the two-stage RANSAC-based approach from [1]: a two-point algorithm for rotation followed by a one-point algorithm for translation. Then we integrate the estimated relative pose into the prediction step of the EKF. In the measurement update step, we only incorporate a much smaller number of landmarks into the 3D map to maintain real-time operation. Incorporating the visual odometry prior in the EKF process yields better and more robust localization and mapping results when compared to the constant linear and angular velocity model case. Our experimental results, using a handheld stereo camera as the only sensor, clearly show the benefits of our method against the standard constant velocity model.},
author = {Alcantarilla, Pablo F. and Bergasa, Luis M. and Dellaert, Frank},
doi = {10.1109/ROBOT.2010.5509272},
file = {:home/chris/Documents/Mendeley Desktop/Visual Odometry Priors for robust EKF-SLAM.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3501--3506},
title = {{Visual odometry priors for robust EKF-SLAM}},
year = {2010}
}
@article{Munguia2012,
abstract = {{\textless}p{\textgreater} This paper describes in a detailed manner a method to implement a simultaneous localization and mapping (SLAM) system based on monocular vision for applications of visual odometry, appearance-based sensing, and emulation of range-bearing measurements. SLAM techniques are required to operate mobile robots in {\textless}italic{\textgreater}a priori{\textless}/italic{\textgreater} unknown environments using only on-board sensors to simultaneously build a map of their surroundings; this map will be needed for the robot to track its position. In this context, the 6-DOF (degree of freedom) monocular camera case (monocular SLAM) possibly represents the harder variant of SLAM. In monocular SLAM, a single camera, which is freely moving through its environment, represents the sole sensory input to the system. The method proposed in this paper is based on a technique called delayed inverse-depth feature initialization, which is intended to initialize new visual features on the system. In this work, detailed formulation, extended discussions, and experiments with real data are presented in order to validate and to show the performance of the proposal. {\textless}/p{\textgreater}},
author = {Mungu{\'{i}}a, Rodrigo and Grau, Antoni},
doi = {10.1155/2012/676385},
file = {:home/chris/Documents/Mendeley Desktop/Monocular SLAM for Visual Odometry$\backslash$:A Full Approach to the Delayed Inverse-DepthFeature Initialization Method.pdf:pdf},
issn = {1024123X},
journal = {Math. Probl. Eng.},
title = {{Monocular SLAM for visual odometry: A full approach to the delayed inverse-depth feature initialization method}},
volume = {2012},
year = {2012}
}
@article{Ricci2013,
author = {Ricci, Luca},
file = {:home/chris/Documents/Mendeley Desktop/visual slam and visual odometry.pdf:pdf},
title = {{Monocular Visual Odometry}},
year = {2013}
}
@article{Williams2010,
abstract = {Sequential monocular SLAM systems perform drift free tracking of the pose of a camera relative to a jointly estimated map of landmarks. To allow real-time operation in moderately sized environments, the map is kept quite spare with usually only tens of landmarks visible in each frame. In contrast, visual odometry techniques track hundreds of visual features per frame. This leads to a very accurate estimate of the relative camera motion, but without a persistent map, the estimate tends to drift over time. We demonstrate a new monocular SLAM system which combines the benefits of these two techniques. In addition to maintaining a sparse map of landmarks in the world, our system finds as many inter-frame point matches as possible. These point matches provide additional constraints on the inter-frame motion of the camera leading to a more accurate pose estimate, and, since they are not maintained as full map landmarks, they do not cause a large increase in the computational cost. Our results in both a simulated environment and in real video demonstrate the improvement in estimation accuracy gained by the inclusion of visual odometry style observations. The constraints available from pairwise point matches are most naturally cast in the context of a camera-centric rather than world-centric frame. To that end we recast the usual world-centric EKF implementation of visual SLAM in a robo-centric frame. We show that this robo-centric visual SLAM, as expected, leads to the estimated uncertainty more closely matching the ideal uncertainty; i.e., that robo-centric visual SLAM yields a more consistent estimate than the traditional world-centric EKF algorithm.},
author = {Williams, Brian and Reid, Ian},
doi = {10.1109/ROBOT.2010.5509248},
file = {:home/chris/Documents/Mendeley Desktop/On Combining Visual SLAM and Visual Odometry.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3494--3500},
title = {{On combining visual SLAM and visual odometry}},
year = {2010}
}
@article{Yousif2015,
abstract = {This paper is intended to pave the way for new researchers in the field of robotics and autonomous systems, particularly thosewho are interested in robot localization and mapping. We discuss the fundamentals of robot navigation requirements and provide a reviewof the state of the art tech- niques that form the bases of established solutions formobile robots localization and mapping. The topicswediscuss range from basic localization techniques such as wheel odometry and dead reckoning, to the more advance Visual Odometry (VO) and Simultaneous Localization and Mapping (SLAM) techniques. We discuss VO in both monocular and stereo vision systems using feature matching/tracking and optical flow techniques.We discuss and compare the basics of most common SLAM methods such as the Extended Kalman Fil- ter SLAM (EKF-SLAM), Particle Filter and the most recent RGB-D SLAM. We also provide techniques that form the building blocks to those methods such as feature extraction (i.e. SIFT, SURF, FAST), feature matching, outlier removal and data association techniques.},
author = {Yousif, Khalid and Bab-Hadiashar, Alireza and Hoseinnezhad, Reza},
doi = {10.1007/s40903-015-0032-7},
file = {:home/chris/Documents/Mendeley Desktop/An-Overview-to-Visual-Odometry-and-Visual-SLAM-Applications-to-Mobile-Robotics.pdf:pdf},
isbn = {2199-854X},
issn = {2363-6912},
journal = {Intell. Ind. Syst.},
number = {4},
pages = {289--311},
title = {{An Overview to Visual Odometry and Visual SLAM: Applications to Mobile Robotics}},
url = {http://link.springer.com/10.1007/s40903-015-0032-7},
volume = {1},
year = {2015}
}
@article{Jones2011,
abstract = {We present amodel to estimatemotion frommonocular visual and inertialmeasurements. We analyze the model and characterize the conditions under which its state is observable, and its parameters are identifiable. These include the unknown gravity vector, and the unknown transformation between the camera coordinate frame and the inertial unit. We show that it is possible to estimate both state and parameters as part of an on-line procedure, but only provided that the motion sequence is “rich enough,” a condition that we characterize explicitly. We then describe an efficient implementation of a filter to estimate the state and parameters of this model, including gravity and camera-to-inertial calibration. It runs in real-time on an embedded platform, and its performance has been tested extensively. We report experiments of continuous operation, without failures, re-initialization, or re-calibration, on paths of length up to 30Km. We also describe an integrated approach to “loop-closure,” that is the recognition of previously-seen locations and the topological re-adjustment of the traveled path. It represents visual features relative to the global orientation reference provided by the gravity vector estimated by the filter, and relative to the scale provided by their known position within the map; these features are organized into “locations” defined by visibility constraints, represented in a topological graph, where loop closure can be performed without the need to re-compute past trajectories or perform bundle adjustment. The software infrastructure as well as the embedded platform is described in detail in a technical report (Jones and Soatto (2009).)},
author = {Jones, Eagle S. and Soatto, Stefano},
doi = {10.1177/0278364910388963},
file = {:home/chris/Documents/Mendeley Desktop/Visual-Inertial Navigation, Mapping and Localization$\backslash$:AScalableReal-TimeCausalApproach.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {Int. J. Rob. Res.},
number = {4},
pages = {407--430},
pmid = {5423178},
title = {{Visual-inertial navigation, mapping and localization: A scalable real-time causal approach}},
url = {http://journals.sagepub.com/doi/10.1177/0278364910388963},
volume = {30},
year = {2011}
}
@article{Carlone2016,
abstract = {Visual attention is the cognitive process that allows humans to parse a large amount of sensory data by selecting relevant information and filtering out irrelevant stimuli. This papers develops a computational framework for visual attention in robots. We consider a Visual Inertial Navigation (VIN) problem in which a robot needs to estimate its state using an on-board camera and an inertial sensor. The robot can allocate limited resources to VIN, due to time and energy constraints. Therefore, we answer the following question: under limited resources, what are the most relevant visual cues to maximize the performance of visual-inertial navigation? Our approach has four key features. First, it is task-driven, in that the selection of the visual cues is guided by a metric quantifying the task performance. Second, it exploits the notion of anticipation, since it uses a simplified model for forward-simulation of robot dynamics, predicting the utility of a set of visual cues over a time horizon. Third, it is efficient and easy to implement, since it leads to a greedy algorithm for the selection of the most relevant visual cues. Fourth, it provides formal performance guarantees: we leverage submodularity to prove that the greedy selection cannot be far from the optimal (combinatorial) selection. Simulations and real experiments on agile drones show that our approach leads to dramatic improvements in the VIN performance. In the easy scenarios, our approach outperforms the state-of-the-art in terms of localization errors. In the most challenging scenarios, it enables accurate visual-inertial navigation while the state of the art fails to track robot's motion during aggressive maneuvers.},
archivePrefix = {arXiv},
arxivId = {1610.03344},
author = {Carlone, Luca and Karaman, Sertac},
eprint = {1610.03344},
file = {:home/chris/Documents/Mendeley Desktop/Attention and Anticipation in Fast Visual-Inertial Navigation.pdf:pdf},
isbn = {9781509046324},
title = {{Attention and Anticipation in Fast Visual-Inertial Navigation}},
url = {http://arxiv.org/abs/1610.03344},
year = {2016}
}
@article{Forster2017,
abstract = {In this paper we propose a novel approach to localization in very large indoor spaces (i.e., 200+ store shopping malls) that takes a single image and a floor plan of the environment as input. We formulate the localization problem as inference in a Markov random field, which jointly reasons about text detection (localizing shop's names in the image with precise bounding boxes), shop facade segmentation, as well as camera's rotation and translation within the entire shopping mall. The power of our approach is that it does not use any prior information about appearance and instead exploits text detections corresponding to the shop names. This makes our method applicable to a variety of domains and robust to store appearance variation across countries, seasons, and illumination conditions. We demonstrate the performance of our approach in a new dataset we collected of two very large shopping malls, and show the power of holistic reasoning.},
archivePrefix = {arXiv},
arxivId = {1204.3968},
author = {Forster, Christian and Zhang, Zichao and Gassner, Michael and Werlberger, Manuel and Scaramuzza, Davide},
doi = {10.1109/TRO.2016.2623335},
eprint = {1204.3968},
file = {:home/chris/Documents/Mendeley Desktop/SVO2.0$\backslash$: Semidirect Visual Odometry for Monocular and Multicamera Systems.pdf:pdf},
isbn = {9783902823625},
issn = {15523098},
journal = {IEEE Trans. Robot.},
keywords = {Robot vision,simultaneous localization and mapping (SLAM)},
number = {2},
pages = {249--265},
title = {{SVO: Semidirect Visual Odometry for Monocular and Multicamera Systems}},
volume = {33},
year = {2017}
}
@article{Bazeille2010,
abstract = {We address the problem of simultaneous localization and mapping (SLAM) by combining visual loop-closure detection with metrical information given by a robot odometry. The proposed algorithm extends a purely appearance-based loop-closure detection method based on bags of visual words 1 which is able to detect when the robot has returned back to a previously visited place. An efficient optimization algorithm is used to integrate odometry information in this method to generate a consistent topo-metrical map. The resulting algorithm which only requires a monocular camera and odometry data and is simple, and robust without requiring any a priori information on the environment.},
author = {Bazeille, S. and Filliat, D.},
doi = {10.1051/ro/2010021},
file = {:home/chris/Documents/Mendeley Desktop/Combining-Odometry-and-Visual-Loop-Closure-Detection-for-Consistent-Topo-Metrical-Mapping.pdf:pdf},
isbn = {0399-0559},
issn = {0399-0559},
journal = {RAIRO - Oper. Res.},
keywords = {mobile robot,monocular vision,odometry,slam},
number = {4},
pages = {365--377},
title = {{Combining Odometry and Visual Loop-Closure Detection for Consistent Topo-Metrical Mapping}},
url = {http://www.rairo-ro.org/10.1051/ro/2010021},
volume = {44},
year = {2010}
}
@article{Huang2014,
abstract = {Visual-inertial navigation systems (VINS) have prevailed in various applications, in part because of the complementary sensing capabilities and decreasing costs as well as sizes. While many of the current VINS algorithms undergo inconsistent estimation, in this paper we introduce a new extended Kalman filter (EKF)-based approach towards consistent estimates. To this end, we impose both state-transition and obervability constraints in computing EKF Jacobians so that the resulting linearized system can best approximate the underlying nonlinear system. Specifically, we enforce the propagation Jacobian to obey the semigroup property, thus being an appropriate state-transition matrix. This is achieved by parametrizing the orientation error state in the global, instead of local, frame of reference, and then evaluating the Jacobian at the propagated, instead of the updated, state estimates. Moreover, the EKF linearized system ensures correct observability by projecting the most-accurate measurement Jacobian onto the observable subspace so that no spurious information is gained. The proposed algorithm is validated by both Monte-Carlo simulation and real-world experimental tests.},
author = {Huang, Guoquan and Kaess, Michael and Leonard, John J.},
doi = {10.1109/ICRA.2014.6907581},
file = {:home/chris/Documents/Mendeley Desktop/Towards Consistent Visual-Inertial Navigation.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {4926--4933},
title = {{Towards consistent visual-inertial navigation}},
year = {2014}
}
@article{Bazeille2010a,
abstract = {We address the problem of simultaneous localization and mapping (SLAM) by combining visual loop-closure detection with metrical information given by a robot odometry. The proposed algorithm extends a purely appearance-based loop-closure detection method based on bags of visual words 1 which is able to detect when the robot has returned back to a previously visited place. An efficient optimization algorithm is used to integrate odometry information in this method to generate a consistent topo-metrical map. The resulting algorithm which only requires a monocular camera and odometry data and is simple, and robust without requiring any a priori information on the environment.},
author = {Bazeille, S. and Filliat, D.},
doi = {10.1051/ro/2010021},
file = {:home/chris/Documents/Mendeley Desktop/Combining{\_}Odometry{\_}and{\_}Visual{\_}Loop-Closure{\_}Detecti.pdf:pdf},
isbn = {0399-0559},
issn = {0399-0559},
journal = {RAIRO - Oper. Res.},
keywords = {mobile robot,monocular vision,odometry,slam},
number = {4},
pages = {365--377},
title = {{Combining Odometry and Visual Loop-Closure Detection for Consistent Topo-Metrical Mapping}},
url = {http://www.rairo-ro.org/10.1051/ro/2010021},
volume = {44},
year = {2010}
}
@article{Thomas2008,
abstract = {Simultaneous localisation and mapping (SLAM) has been the focus of intensive research in the last decade due to the potential benefits it offers to the field of autonomous mobile robotics. SLAM is concerned with the ability of an autonomous vehicle to navigate through an unex- plored environment and incrementally construct a map of the environment and localise itself within this map. This thesis describes an entirely vision-based, large-area, 6DoF SLAM sys- tem that was developed specifically for real-time deployment on an autonomous underwater vehicle (AUV) equipped with a calibrated stereo system. This SLAM system is based on the extended Kalman filter (EKF) and incorporates a novel approach to landmark description and data association in which landmarks are essentially local submaps that consist of a cloud of 3D points and their associated SIFT or SURF descriptors. Furthermore, landmarks are sparsely distributed in the constructed map which greatly simplifies and accelerates data association and map updates. In addition to performing localisation based on landmark observations the system also performs visual odometry and predicts vehicle motion using a constant-velocity model. For a simulated 87m long 3D loop trajectory the mean squared localisation error of the system was 3.16 and the maximum absolute error in roll, pitch and yaw angles was 11.6 o , 24.3 o and 24.4 o respectively when the stereo and landmark correspondences contained Gaussian noise with a standard deviation of 0.1 pixels and 10{\%} of correspondences were outliers. This thesis represents an important contribution to entirely vision-based 6DoF SLAM as very few implementations currently exist, and the approach utilised in this thesis achieves comparable results and has the potential to operate in real-time.},
author = {Thomas, Stephen},
doi = {10.1109/WISP.2007.4447566},
file = {:home/chris/Documents/Mendeley Desktop/Real-time Stereo Visual SLAM.pdf:pdf},
isbn = {9781424408290},
issn = {03029743},
journal = {2007 IEEE Int. Symp. Intell. Signal Process.},
pages = {1--6},
title = {{Real-time Stereo Visual SLAM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4447566},
year = {2008}
}
@article{Ozog2014,
abstract = {This paper reports on a method for an au- tonomous underwater vehicle to perform real-time visual si- multaneous localization and mapping (SLAM) on large ship hulls over multiple sessions. Along with a monocular camera, our method uses a piecewise-planar model to explicitly optimize the ship hull surface in our factor-graph framework, and anchor nodes to co-register multiple surveys. To enable real- time performance for long-term SLAM, we use the recent Generic Linear Constraints (GLC) framework to sparsify our factor-graph. This paper analyzes how our single-session SLAM techniques can be used in the GLC framework, and describes a particle filter reacquisition algorithm so that an underwater session can be automatically re-localized to a previously built SLAM graph. We provide real-world experimental results involving automated ship hull inspection, and show that our lo- calization filter out-performs Fast Appearance-Based Mapping (FAB-MAP), a popular place-recognition system. Using our approach, we can automatically align surveys that were taken days, months, and even years apart.},
author = {Ozog, Paul and Eustice, Ryan M.},
doi = {10.1109/ICRA.2014.6907415},
file = {:home/chris/Documents/Mendeley Desktop/Toward long-term, automated ship hull inspection with visual SLAM, explicit surface optimization, and generic graph-sparsification.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3832--3839},
title = {{Toward long-term, automated ship hull inspection with visual SLAM, explicit surface optimization, and generic graph-sparsification}},
year = {2014}
}
@article{Wang2011,
abstract = {Simultaneous localization and mapping (SLAM) problem is an attractive topic in the AUV research community. This paper presents a new SLAM algorithm based on support vector machines(SVM) adaptive Extended Kalman Filter(EKF) for autonomous underwater vehicle(AUV) to reduce the influence of the change of statistical characteristics of the system noise and the observe noise. First establish a feature based map, then use the EKF to create a map. SVM is employed to generate the adaptive factors according to the ratio of the theoretical covariance to its actual covariance of the innovation sequence. Simulation shows that the improved SLAM algorithm reduces the influence of change of statistical characteristics of noise and enhances the navigation accuracy of SLAM system.},
author = {Wang, Hong-jian and Wang, Jing and Yu, Le and Liu, Zhen-ye},
file = {:home/chris/Documents/Mendeley Desktop/A new SLAM method based on SVM-AEKF for AUV.pdf:pdf},
isbn = {9781457714276},
journal = {Ocean. 2011},
pages = {1--6},
title = {{A new SLAM method based on SVM-AEKF for AUV}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=6107204},
year = {2011}
}
@article{Mallios2011,
abstract = {In spite of the recent advances in unmanned underwater vehicles (UUV) navigation techniques, robustly solving their localization in unstructured and unconstrained areas is still a challenging problem. In this paper, we propose a pose-based algorithm to solve the full Simultaneous Localization And Mapping (SLAM) problem for an Autonomous Underwater Vehicle (AUV), navigating in the unknown and unstructured environment. A probabilistic scan matching technique using range scans gathered from a Mechanical Scanning Imaging Sonar (MSIS) is used together with the robot dead-reckoning displacements. The raw data from the sensors are processed and fused in-line with an augmented state extended Kalman filter (EKF), that estimates and keeps the scans poses. The proposed SLAM method has been tested with a real world dataset acquired from the Sparus AUV, guided in a natural underwater environment.},
author = {Mallios, a. and Ridao, P. and Carreras, M. and Hernandez, E.},
file = {:home/chris/Documents/Mendeley Desktop/Navigating and mapping with the SPARUS AUV in a natural and unstructured underwater environment.pdf:pdf},
isbn = {978-0-933957-39-8 Unique ID: WOS:000299005801011},
issn = {0197-7385},
journal = {Ocean. 2011},
pages = {1--7},
title = {{Navigating and mapping with the SPARUS AUV in a natural and unstructured underwater environment}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=6107105},
year = {2011}
}
@article{Forster2007,
abstract = {We describe a new algorithm for robot localization, efficient both in terms of memory and processing time. It transforms a stream of laser range sensor data into a probabilistic calculation of the robot's position, using a bidirectional Long Short-Term Memory (LSTM) recurrent neural network (RNN) to learn the structure of the environment and to answer queries such as: in which room is the robot? To achieve this, the RNN builds an implicit map of the environment.},
author = {F{\"{o}}rster, A and Graves, A and Schmidhuber, J},
doi = {10.1360/aas-007-0048},
file = {:home/chris/Documents/Mendeley Desktop/RNN-based Learning of Compact Maps forEfficient Robot Localization.pdf:pdf},
isbn = {2-930307-07-2},
issn = {0254-4156},
journal = {Esann},
number = {2000020},
pages = {537--542},
title = {{RNN-based Learning of Compact Maps for Efficient Robot Localization}},
url = {http://dblp.uni-trier.de/db/conf/esann/esann2007.html{\#}ForsterGS07},
year = {2007}
}
@article{Miljkovic2015,
author = {Miljkovi{\'{c}}, Z and Vukovi{\'{c}}, N and Miti{\'{c}}, M},
doi = {10.1177/0954406215586589},
file = {:home/chris/Documents/Mendeley Desktop/Neural extended Kalman filter formonocular SLAM in indoor environment.pdf:pdf},
issn = {0954-4062},
journal = {Proc. Inst. {\ldots}},
keywords = {22 april 2015,9 july 2014,accepted,date received,extended kalman filter,feedforward neural network,inverse depth,mobile robot,monocular camera},
number = {0},
pages = {1--11},
title = {{Neural extended Kalman filter for monocular SLAM in indoor environment}},
url = {http://pic.sagepub.com/content/early/2015/05/12/0954406215586589.abstract},
volume = {0},
year = {2015}
}
@article{Kendall2016,
abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/},
archivePrefix = {arXiv},
arxivId = {1505.07427},
author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
doi = {10.1109/ICCV.2015.336},
eprint = {1505.07427},
file = {:home/chris/Documents/Mendeley Desktop/PoseNet$\backslash$: A Convolutional Network for Real-Time 6-DOF Camera Relocalization.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {2938--2946},
title = {{PoseNet: A convolutional network for real-time 6-dof camera relocalization}},
volume = {11-18-Dece},
year = {2016}
}
@article{Conte2008,
abstract = {The topic of the paper is a technique for underwater Simultaneous Localization and Mapping by sonar sensors. The proposed procedure consists in the application of suitable Neural Network and Iterative Closest Point algorithms for objects detection, agent localization and environment mapping. General Regression Neural Network and improved ICP algorithms have been implemented in order to process sonar sensor data, minimize the computational time, maximize the efficiency of the localization tasks and be free from agent dynamical model. Experimental tests have been performed in a structured static environment and data have been collected by a 675 KHz single beam mechanical scanning sonar.},
author = {Conte, G and Scaradozzi, D and Zanoli, S M and Gambella, L and Marani, G},
file = {:home/chris/Documents/Mendeley Desktop/Underwater SLAM with ICP Localization and Neural Network Objects Classification.pdf:pdf},
isbn = {9781880653708},
issn = {10986189},
journal = {18th 2008 Int. Offshore Polar Eng. Conf. ISOPE 2008},
keywords = {grnn,icp,slam,sonar,underwater},
number = {January},
pages = {351--357},
title = {{Underwater SLAM with ICP Localization and Neural Network Objects Classification}},
url = {http://www.scopus.com/record/display.uri?eid=2-s2.0-58449112697{\&}origin=inward{\&}txGid=0},
year = {2008}
}
@article{Burguera2014,
author = {Burguera, A and Bonin-Font, F and Oliver, G},
doi = {10.5220/0004682005390544},
file = {:home/chris/Documents/Mendeley Desktop/Towards Robust Image Registration for Underwater Visual SLAM.pdf:pdf},
isbn = {9789897580093},
journal = {Proc. Int. Conf. Comput. Vision, Theory Appl. {\{}(VISAPP){\}}.},
pages = {539--544},
title = {{Towards robust Image Registration for Underwater Visual Slam}},
year = {2014}
}
@article{Hildebrandt2013,
abstract = {In this work the development of an algorithm for visual underwater local- ization is described. It spans the complete process from the initial idea, the development of a suitable underwater vehicle for testing to the algorithm's experimental validation in real underwater environments. Besides the devel- opment and validation of the visual SLAM algorithm, the methodology for its evaluation is a key aspect of this work. The resulting SURE-SLAM algo- rithm uses a stereo camera system and basic vehicle sensors (AHRS, DPS) to compute a complete, error-bounded localization solution for underwater vehicles in real-time with similar quality as state-of-the-art acoustically sta- bilized dead-reckoning approaches. The robustness of the algorithm as well as its limitations and failure-cases are established by extensive field testing with the AUV Dagon, which was developed during this thesis as test and evaluation vehicle.},
author = {Hildebrandt, Marc},
file = {:home/chris/Documents/Mendeley Desktop/Development, Evaluation and Validation of a Stereo Camera Underwater SLAM Algorithm.pdf:pdf},
journal = {PhD Thesis, Univ. Bremen},
keywords = {SLAM,auv,underwater,vision},
title = {{Development, evaluation and validation of a stereo camera underwater SLAM Algorithm}},
year = {2013}
}
@article{R.Havangi2011,
author = {R.Havangi and M.A.Nekoui and H.D.Taghirad and M.Teshnehlab},
file = {:home/chris/Documents/Mendeley Desktop/SLAM based on intelligent unscented Kalman filter.pdf:pdf},
isbn = {9781467316903},
journal = {Int. Conf. Control. Instrum. Autom.},
number = {4},
pages = {877--882},
title = {{SLAM Based on Intelligent Unscented Kalman Filter}},
year = {2011}
}
@article{Kurt-Yavuz2012,
abstract = {This study aims to contribute a comparison of various simultaneous localization and mapping (SLAM) algorithms that have been proposed in literature. The performance of Extended Kalman Filter (EKF) SLAM, Unscented Kalman Filter (UKF) SLAM, EKF-based FastSLAM version 2.0, and UKF-based FastSLAM (uFastSLAM) algorithms are compared in terms of accuracy of state estimations for localization of a robot and mapping of its environment. The algorithms were run using the same type of robot on Player/Stage environment. The results show that the UKF-based FastSLAM has the best performance in terms of accuracy of localization and mapping. Unlike most of the previous applications of FastSLAM in literature, no waypoints are used in this study.},
author = {Kurt-Yavuz, Zeyneb and Yavuz, Sirma},
doi = {10.1109/INES.2012.6249866},
file = {:home/chris/Documents/Mendeley Desktop/A comparison of EKF, UKF, FastSLAM2.0, and UKF-based FastSLAM algorithms.pdf:pdf},
isbn = {9781467326957},
journal = {INES 2012 - IEEE 16th Int. Conf. Intell. Eng. Syst. Proc.},
pages = {37--43},
title = {{A comparison of EKF, UKF, FastSLAM2.0, and UKF-based FastSLAM algorithms}},
year = {2012}
}
@article{Chaves2016,
abstract = {This paper reports on an active SLAM framework for performing large-scale inspections with an underwa-ter robot. We propose a path planning algorithm integrated with visual SLAM that plans loop-closure paths in order to decrease navigation uncertainty. While loop-closing revisit actions bound the robot's uncertainty, they also lead to redundant area coverage and increased path length. Our pro-posed opportunistic framework leverages sampling-based techniques and information filtering to plan revisit paths that are coverage efficient. We employ Gaussian process regres-sion for modeling the prediction of camera registrations and use a two-step optimization procedure for selecting revisit actions. We show that the proposed method offers many benefits over existing solutions and good performance for bounding navigation uncertainty in long-term autonomous operations with hybrid simulation experiments and real-world field trials performed by an underwater inspection robot.},
author = {Chaves, Stephen M. and Kim, Ayoung and Galceran, Enric and Eustice, Ryan M.},
doi = {10.1007/s10514-016-9597-6},
file = {:home/chris/Documents/Mendeley Desktop/Opportunistic sampling-based active visual SLAM for underwater inspection.pdf:pdf},
isbn = {9781479969333},
issn = {15737527},
journal = {Auton. Robots},
keywords = {Active SLAM,Gaussian processes,Sampling-based planning,Underwater robotics},
number = {7},
pages = {1245--1265},
publisher = {Springer US},
title = {{Opportunistic sampling-based active visual SLAM for underwater inspection}},
volume = {40},
year = {2016}
}
@article{Huang2009,
abstract = {This paper addresses two key limitations of the unscented Kalman filter (UKF) when applied to the simultaneous localization and mapping (SLAM) problem: the cubic, in the number of states, computational complexity, and the inconsistency of the state estimates. In particular, we introduce a new sampling strategy that minimizes the linearization error and whose computational complexity is constant (i.e., independent of the size of the state vector). As a result, the overall computational complexity of UKF-based SLAM becomes of the same order as that of the extended Kalman filter (EKF) when applied to SLAM. Furthermore, we investigate the observability properties of the linear-regression-based model employed by the UKF, and propose a new algorithm, termed the observability-constrained (OC)-UKF, that improves the consistency of the state estimates. The superior performance of the OC-UKF compared to the standard UKF and its robustness to large linearization errors are validated by extensive simulations.},
author = {Huang, Guoquan P. and Mourikis, Anastasios I. and Roumeliotis, Stergios I.},
doi = {10.1109/ROBOT.2009.5152793},
file = {:home/chris/Documents/Mendeley Desktop/On the Complexity and Consistency of UKF-based SLAM.pdf:pdf},
isbn = {9781424427895},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {4401--4408},
title = {{On the complexity and consistency of UKF-based SLAM}},
year = {2009}
}
@article{Kim2012,
abstract = {One of the major challenges in the field of underwater robotics is the opacity of the water medium to radio frequency transmission modes, which precludes the use of a global positioning system (GPS) and high speed radio communication in underwater navigation and mapping applications. One approach to underwater robotics that overcomes this limitation is vision-based simultaneous localization and mapping (SLAM), a framework that enables a robot to localize itself, while simultaneously building a map of an unknown environment. The SLAM algorithm provides a probabilistic map that contains the estimated state of the system, including a map of the environment and the pose of the robot. Because the quality of vision-based navigation varies spatially within the environment the performance of visual SLAM strongly depends on the path and motion that the robot follows. While traditionally treated as two separate problems, SLAM and path planning are indeed interrelated: the performance of SLAM depends significantly on the environment and motion; however, control of the robot motion fully depends on the information from SLAM. Therefore, an integrated SLAM control scheme is needed?one that can direct motion for better localization and mapping, and thereby provide more accurate state information back to the controller. This thesis develops perception-driven control, an integrated SLAM and path planning framework that improves the performance of visual SLAM in an informative and efficient way by jointly considering the reward predicted by a candidate camera measurement, along with its likelihood of success based upon visual saliency. The proposed control architecture identifies highly informative candidate locations for SLAM loop-closure that are also visually distinctive, such that a camera-derived pose-constraint is probable. Results are shown for autonomous underwater hull inspection experiments using the Bluefin Robotics Hovering Autonomous Underwater Vehicle (HAUV).},
author = {Kim, Ayoung},
file = {:home/chris/Documents/Mendeley Desktop/Active Visual SLAM with Exploration for Autonomous Underwater Navigation.pdf:pdf},
pages = {170},
title = {{Active Visual SLAM with Exploration for Autonomous Underwater Navigation}},
year = {2012}
}
@article{Wang2013,
abstract = {This work proposes an improved unscented Kalman filter (UKF)-based simultaneous localization and mapping (SLAM) algorithm based on an adaptive unscented Kalman filter (AUKF) with a noise statistic estimator. The algorithm solves the issue that conventional UKF-SLAM algorithms have declining accuracy, with divergence occurring when the prior noise statistic is unknown and time-varying. The new SLAM algorithm performs an online estimation of the statistical parameters of unknown system noise by introducing a modified Sage-Husa noise statistic estimator. The algorithm also judges whether the filter is divergent and restrains potential filtering divergence using a covariance matching method. This approach reduces state estimation error, effectively improving navigation accuracy of the SLAM system. A line feature extraction is implemented through a Hough transform based on the ranging sonar model. Test results based on unmanned underwater vehicle (UUV) sea trial data indicate that the proposed AUKF-SLAM algorithm is valid and feasible and provides better accuracy than the standard UKF-SLAM system.},
author = {Wang, Hongjian and Fu, Guixia and Li, Juan and Yan, Zheping and Bian, Xinqian},
doi = {10.1155/2013/605981},
file = {:home/chris/Documents/Mendeley Desktop/An Adaptive UKF Based SLAM Method forUnmanned Underwater Vehicle.pdf:pdf},
issn = {1024123X},
journal = {Math. Probl. Eng.},
title = {{An adaptive UKF based SLAM method for unmanned underwater vehicle}},
volume = {2013},
year = {2013}
}
@article{Kim,
author = {Kim, Ayoung and Eustice, Ryan},
file = {:home/chris/Documents/Mendeley Desktop/Pose-graph visual SLAM with geometric model selection for autonomous underwater ship hull inspection.pdf:pdf},
title = {{Pose-graph Visual SLAM with Geometric Model Selection for AUV.pdf}}
}
@article{Burguera2015,
abstract = {We present a new vision-based localization system applied to an autonomous underwater vehicle (AUV) with limited sensing and computation capabilities. The trad using an iterated Kalman filter (IEKF). Experiments have been conducted using a real underwater vehicle in controlled scenarios and in shallow sea waters, showing an excellent performance with very small errors, both in the vehicle pose and in the overall trajectory estimates.itional EKF-SLAM approaches are usually expensive in terms of execution time; the approach presented in this paper strengthens this method by adopting a trajectory-based schema that reduces the computational requirements. The pose of the vehicle is estimated using an extended Kalman filter (EKF), which predicts the vehicle motion by means of a visual odometer and corrects these predictions using the data associations (loop closures) between the current frame and the previous ones. One of the most important steps in this procedure is the image registration method, as it reinforces the data association and, thus, makes it possible to close loops reliably. Since the use of standard EKFs entail linearization errors that can distort the vehicle pose estimations, the approach has also been tested},
author = {Burguera, Antoni and Bonin-Font, Francisco and Oliver, Gabriel},
doi = {10.3390/s150101708},
file = {:home/chris/Documents/Mendeley Desktop/Trajectory-Based Visual Localization in Underwater Surveying Missions.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Data association,Image registration,Underwater robotics,Visual localization},
number = {1},
pages = {1708--1735},
title = {{Trajectory-based visual localization in underwater surveying missions}},
volume = {15},
year = {2015}
}
@article{Burguera2014a,
author = {Burguera, Antoni and Gonz, Yolanda and Oliver, Gabriel},
doi = {10.1007/978-3-319-03413-3},
file = {:home/chris/Documents/Mendeley Desktop/RANSAC Based Data Association for Underwater Visual SLAM.pdf:pdf},
isbn = {978-3-319-03412-6},
issn = {21945357},
keywords = {underwater robotics,visual slam},
title = {{RANSAC Based Data Association for Underwater Visual SLAM?}},
url = {http://link.springer.com/10.1007/978-3-319-03413-3},
volume = {252},
year = {2014}
}
@article{Bonin-Font2014,
abstract = {Effectiveness in loop closing detection is crucial to increase accuracy in SLAM (Simultaneous Localization and Mapping) for mobile robots. The most representative approaches to visual loop closing detection are based on feature matching or BOW (Bag of Words), being slow and needing a lot of memory resources or a previously defined vocabulary, which complicates and delays the whole process. This paper present a new visual LSH (Locality Sensitive Hashing)-based approach for loop closure detection, where images are hashed to accelerate considerably the whole comparison process. The algorithm is applied in AUV (Autonomous Underwater Vehicles), in several aquatic scenarios, showing promising results and the validity of this proposal to be applied online.},
author = {Bonin-Font, Francisco and Carrasco, Pep Lluis Negre and Burguera, Antoni Burguera and Codina, Gabriel Oliver},
doi = {10.1109/ETFA.2014.7005245},
file = {:home/chris/Documents/Mendeley Desktop/LSH for Loop Closing Detection in Underwater Visual SLAM.pdf:pdf},
isbn = {9781479948468},
issn = {1946-0740},
journal = {19th IEEE Int. Conf. Emerg. Technol. Fact. Autom. ETFA 2014},
title = {{LSH for loop closing detection in underwater visual SLAM}},
year = {2014}
}
@book{Roskilly2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Roskilly, Tony and Mikalsen, Rikard},
doi = {10.1016/B978-0-08-099996-8.00001-X},
eprint = {arXiv:1011.1669v3},
file = {:home/chris/Documents/Mendeley Desktop/Appendix-B-Mathematics-Background{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Appendix-C-Solutions-to-Questions{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Chapter-Five-Closed-Loop-Stability{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Chapter-Four-Feedback-Control{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Chapter-Six-Frequency-Domain-Analysis{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Chapter-Three-System-Transfer-Functions{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Front-Matter{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Model Testing.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Appendix-A-Laplace-Transforms-Table{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Chapter-One-Introduction{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Chapter-Two-System-Representation-in-the-Time-Domain{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Copyright{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Index{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf;:home/chris/Documents/Mendeley Desktop/Preface{\_}2015{\_}Marine-Systems-Identification-Modeling-and-Control.pdf:pdf},
isbn = {9780080999968},
issn = {00115266},
number = {0},
pages = {1--13},
pmid = {23138097},
publisher = {Elsevier Inc.},
title = {{Marine Systems Identification, Modeling and Control}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B978008099996809995X http://linkinghub.elsevier.com/retrieve/pii/B9780080999968099985 http://linkinghub.elsevier.com/retrieve/pii/B9780080999968099924 http://linkinghub.elsevier.com/retrieve/pii/B978008099996809},
year = {2015}
}
@article{Dudek2007,
abstract = {AQUA, an amphibious robot that swims via the motion of its legs rather than using thrusters and control surfaces for propulsion, can walk along the shore, swim along the surface in open water, or walk on the bottom of the ocean. The vehicle uses a variety of sensors to estimate its position with respect to local visual features and provide a global frame of reference},
author = {Dudek, Gregory and Giguere, Philippe and Prahacs, Chris and Saunderson, Shane and Sattar, Junaed and Torres-Mendez, Luz Abril and Jenkin, Michael and German, Andrew and Hogue, Andrew and Ripsman, Arlene and Zacher, James and Milios, Evangelos and Liu, Hui and Zhang, Pifu and Buehler, Martin and Georgiades, Christina},
doi = {10.1109/MC.2007.6},
file = {:home/chris/Documents/Mendeley Desktop/AQUA$\backslash$: An Amphibious Autonomous Robot.pdf:pdf},
isbn = {0018-9162},
issn = {00189162},
journal = {Computer (Long. Beach. Calif).},
keywords = {AQUA,Autonomous robot development,Robotics,SASR tasks},
number = {1},
pages = {46--53},
pmid = {21522869},
title = {{AQUA: An amphibious autonomous robot}},
volume = {40},
year = {2007}
}
@article{Drews2009,
abstract = {The use of autonomous underwater vehicles (AUVs) for visual inspection tasks is a promising robotic field. The images captured by the robots can also aid in their localization/navigation. In this context, this paper proposes an approach to localization and mapping problem of underwater vehicle. Supposing the use of inspection cameras, this proposal is composed of two stages: i) the use of computer vision through the algorithm SIFT to extract the features in underwater image sequences and ii) the development of topological maps to localization and navigation. The integration of such systems will allow simultaneous localization and mapping of the environment. A set of tests with real robots was accomplished, regarding online and performance issues. The results reveals an accuracy and robust approach to several underwater conditions, as illumination and noise, leading to a promissory and original SLAM technique.},
author = {Drews, Paulo and Botelho, Silvia and Gomes, S. Sebasti??o},
doi = {10.1109/LARS.2008.32},
file = {:home/chris/Documents/Mendeley Desktop/SLAM in Underwater Environment Using SIFT and Topologic Maps.pdf:pdf},
isbn = {9780769535364},
journal = {5th Lat. Am. Robot. Symp. LARS 2008},
pages = {91--96},
title = {{SLAM in underwater environment using SIFT and topologic maps}},
year = {2009}
}
@article{Elibol2014,
abstract = {Robotic platforms have advanced greatly in terms of their remote sensing capabilities, including obtaining optical information using cameras. Alongside these advances, visual mapping has become a very active research area, which facilitates the mapping of areas inaccessible to humans. This requires the efficient processing of data to increase the final mosaic quality and computational efficiency. In this paper, we propose an efficient image mosaicing algorithm for large area visual mapping in underwater environments using multiple underwater robots. Our method identifies overlapping image pairs in the trajectories carried out by the different robots during the topology estimation process, being this a cornerstone for efficiently mapping large areas of the seafloor. We present comparative results based on challenging real underwater datasets, which simulated multi-robot mapping. ?? 2014 Elsevier B.V. All rights reserved.},
author = {Elibol, Armagan and Kim, Jinwhan and Gracias, Nuno and Garcia, Rafael},
doi = {10.1016/j.patrec.2014.04.020},
file = {:home/chris/Documents/Mendeley Desktop/Efficient image mosaicing for multi-robot visual underwater mapping.pdf:pdf},
isbn = {978-1-4673-0056-8},
issn = {01678655},
journal = {Pattern Recognit. Lett.},
keywords = {Image mosaicing,Unmanned underwater vehicles,Visual mapping},
pages = {20--26},
publisher = {Elsevier B.V.},
title = {{Efficient image mosaicing for multi-robot visual underwater mapping}},
url = {http://dx.doi.org/10.1016/j.patrec.2014.04.020},
volume = {46},
year = {2014}
}
@article{Ferreira2012,
abstract = {This article discusses the possibility of building in real-time a mosaic of the seafloor relying on a simultaneous localization and mapping (SLAM) framework. The goal is to provide an unmanned underwater vehicle with a relatively rough visual map of the seafloor to support basic navigation and context awareness. To achieve that goal, an accurate estimation of the location of the visual landmarks and, in particular, the correct data association when a visual landmark is re-visited by the vehicle are the crucial points. Instead of using a global mosaic, thiswork uses the combination of a set of local mosaics constructed in the vicinity of the SLAM visual landmarks. The contributions of this article are mainly the use of SURF features, the local mosaics approach and the real-time capability. The use of SURF features allows eliminating false positives in the data association of SLAM visual landmarks. The local mosaics approach is an effective way of correcting the effects of the drift on the mosaic in real time. The main contribution is the real-time capability as it will be seen. The algorithm was tested using a batch of experimental data in typical operating conditions and the results prove the effectiveness of the approach.},
author = {Ferreira, Fausto and Veruggio, Gianmarco and Caccia, Massimo and Bruzzone, Gabriele},
doi = {10.1007/s11370-011-0103-x},
file = {:home/chris/Documents/Mendeley Desktop/Real-time optical SLAM-based mosaicking for unmanned underwater vehicles.pdf:pdf},
issn = {18612776},
journal = {Intell. Serv. Robot.},
keywords = {Local mosaics,Mosaicking,Online SLAM},
number = {1},
pages = {55--71},
title = {{Real-time optical SLAM-based mosaicking for unmanned underwater vehicles}},
volume = {5},
year = {2012}
}
@article{Mahon2004,
abstract = {This paper presents techniques developed to apply the simultaneous localisation and mapping (SLAM) algorithm to an unmanned underwater vehicle operating in an unstructured, natural environment. It is shown that information from on-board sonar and vision sensors can be fused to select and track regions of the environment that may be used as features to estimate the vehicle's motion. Results including vehicle pose estimates and resulting environment models are shown for data acquired at the Great Barrier Reef in Australia.},
author = {Mahon, I. and Williams, S.},
doi = {10.1109/ICARCV.2004.1469484},
file = {:home/chris/Documents/Mendeley Desktop/SLAM using natural features in an underwater environment.pdf:pdf},
isbn = {0-7803-8653-1},
journal = {ICARCV 2004 8th Control. Autom. Robot. Vis. Conf. 2004.},
number = {December},
pages = {2076--2081},
title = {{SLAM using natural features in an underwater environment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1469484},
volume = {3},
year = {2004}
}
@article{Guth2014,
abstract = {The unstructured scenario, the extraction of significant features, the imprecision of sensors along with the impossibility of using GPS signals are some of the challenges encountered in underwater environments. Given this adverse context, the Simultaneous Localization and Mapping techniques (SLAM) attempt to localize the robot in an efficient way in an unknown underwater environment while, at the same time, generate a representative model of the environment. In this paper, we focus on key topics related to SLAM applications in underwater environments. Moreover, a review of major studies in the literature and proposed solutions for addressing the problem are presented. Given the limitations of probabilistic approaches, a new alternative based on a bio-inspired model is highlighted.},
author = {Guth, Felipe and Silveira, Luan and Botelho, Silvia and Drews, Paulo and Ballester, Pedro},
doi = {10.1109/BIOROB.2014.6913908},
file = {:home/chris/Documents/Mendeley Desktop/Underwater SLAM$\backslash$: Challenges, state of the art, algorithms and a new biologically-inspired approach.pdf:pdf},
isbn = {978-1-4799-3128-6},
issn = {2155-1774},
journal = {5th IEEE RAS/EMBS Int. Conf. Biomed. Robot. Biomechatronics},
keywords = {Feature extraction,Sensor phenomena and characterization,Simultaneous localization and mapping,Sonar},
pages = {981--986},
title = {{Underwater SLAM: Challenges, state of the art, algorithms and a new biologically-inspired approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6913908},
year = {2014}
}
@article{Ribas2007,
abstract = {This paper describes a navigation system for autonomous underwater vehicles (AUVs) in partially structured environments, such as dams, harbors, marinas or marine platforms. A mechanical scanning imaging sonar is used to obtain information about the location of planar structures present in such environments. A modified version of the Hough transform has been developed to extract line features, together with their uncertainty, from the continuous sonar dataflow. The information obtained is incorporated into a feature-based SLAM algorithm running an Extended Kalman Filter (EKF). Simultaneously, the AUV's position estimate is provided to the feature extraction algorithm to correct the distortions that the vehicle motion produces in the acoustic images. Experiments carried out in a marina located in the Costa Brava (Spain) with the Ictineu AUV show the viability of the proposed approach.},
author = {Ribas, David and Ridao, Pere and Tard{\'{o}}s, Juan Domingo and Neira, Jos{\'{e}}},
doi = {10.1109/IROS.2007.4399222},
file = {:home/chris/Documents/Mendeley Desktop/Underwater SLAM in a marina environment.pdf:pdf},
isbn = {1424409128},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {1455--1460},
title = {{Underwater SLAM in a marina environment}},
year = {2007}
}
@article{Botelho2009,
abstract = {The use of Autonomous Underwater Vehicles (AUVs) for underwater tasks is a promising robotic field. These robots can carry visual inspection cameras. Besides serving the activities of inspection and mapping, the captured images can also be used to aid navigation and localization of the robots. Visual odometry is the process of determining the position and orientation of a robot by analyzing the associated camera images. It has been used in a wide variety of non-standard locomotion robotic methods. In this context, this paper proposes an approach to visual odometry and mapping of underwater vehicles. Supposing the use of inspection cameras, this proposal is composed of two stages: i) the use of computer vision for visual odometry, extracting landmarks in underwater image sequences and ii) the development of topological maps for localization and navigation. The integration of such systems will allow visual odometry, localization and mapping of the environment. A set of tests with real robots was accomplished, regarding online and performance issues. The results reveals an accuracy and robust approach to several underwater conditions, as illumination and noise, leading to a promissory and original visual odometry and mapping technique.},
author = {Botelho, Silvia Silva Da Costa and Drews, Paulo and Oliveira, Gabriel Leivas and Figueiredo, M??nica Da Silva},
doi = {10.1109/LARS.2009.5418320},
file = {:home/chris/Documents/Mendeley Desktop/Visual odometry and mapping for Underwater Autonomous Vehicles.pdf:pdf},
isbn = {9781424462568},
journal = {2009 6th Lat. Am. Robot. Symp. LARS 2009},
keywords = {Artificial intelligence,Computer vision,Robotics,Self-localization and mapping,Underwater vehicle},
title = {{Visual odometry and mapping for underwater autonomous vehicles}},
year = {2009}
}
@article{Mahon2008,
abstract = {This paper presents a simultaneous localization and mapping algorithm suitable for large-scale visual navigation. The estimation process is based on the viewpoint augmented navigation (VAN) framework using an extended information filter. Cholesky factorization modifications are used to maintain a factor of the VAN information matrix, enabling efficient recovery of state estimates and covariances. The algorithm is demonstrated using data acquired by an autonomous underwater vehicle performing a visual survey of sponge beds. Loop-closure observations produced by a stereo vision system are used to correct the estimated vehicle trajectory produced by dead reckoning sensors.},
author = {Mahon, Ian and Williams, Stefan B. and Pizarro, Oscar and Johnson-Roberson, Matthew},
doi = {10.1109/TRO.2008.2004888},
file = {:home/chris/Documents/Mendeley Desktop/Efficient View-Based SLAM Using VisualLoop Closures.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Trans. Robot.},
keywords = {Autonomous underwater vehicle (AUV) navigation,Cholesky factorization,Extended information filter (EIF),Simultaneous localization and mapping (SLAM)},
number = {5},
pages = {1002--1014},
title = {{Efficient view-based SLAM using visual loop closures}},
volume = {24},
year = {2008}
}
@article{S??ez2006,
abstract = {The aquatic realm is ideal for testing autonomous robotic technology. The challenges presented in this environment are numerous due to the highly dynamic nature of the medium. Applications for underwater robotics include the autonomous inspection of coral reef, ships, pipelines, and other environmental assessment programs. In this paper we present current results in using 6DOF entropy minimization SLAM (simultaneous localization and mapping) for creating dense 3D visual maps of underwater environments that are suitable for such applications. The proposed SLAM algorithm exploits dense information coming from a stereo system, and performs robust egomotion estimation and global-rectification following an optimization approach},
author = {S??ez, Juan Manuel and Hogue, Andrew and Escolano, Francisco and Jenkin, Michael},
doi = {10.1109/ROBOT.2006.1642246},
file = {:home/chris/Documents/Mendeley Desktop/Underwater 3D SLAM through Entropy Minimization.pdf:pdf},
isbn = {0780395069},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
number = {May},
pages = {3562--3567},
title = {{Underwater 3D SLAM through entropy minimization}},
volume = {2006},
year = {2006}
}
@article{Dubbelman2015,
author = {Dubbelman, Gijs and Browning, Brett},
doi = {10.1109/TRO.2015.2473455},
file = {:home/chris/Documents/Mendeley Desktop/COP-SLAM-Closed-Form-Online-Pose-Chain-Optimization-for-Visual-SLAM.pdf:pdf},
issn = {15523098},
journal = {IEEE Trans. Robot.},
keywords = {Computer vision,Simultaneous localization and mapping (SLAM),pose-graph optimization},
number = {5},
pages = {1194--1213},
title = {{COP-SLAM: Closed-Form Online Pose-Chain Optimization for Visual SLAM}},
volume = {31},
year = {2015}
}
@article{Agarwal2016,
abstract = {The SLAM problem is known to have a special property that when robot orientation is known, estimating the history of robot poses and feature locations can be posed as a standard linear least squares problem. In this work, we develop a SLAM framework that uses relative feature-to-feature measurements to exploit this structural property of SLAM. Relative feature measurements are used to pose a linear estimation problem for pose-to-pose orientation constraints. This is followed by solving an iterative non-linear on-manifold optimization problem to compute the maximum likelihood estimate for robot orientation given relative rotation constraints. Once the robot orientation is computed, we solve a linear problem for robot position and map estimation. Our approach reduces the computational burden of non-linear optimization by posing a smaller optimization problem as compared to standard graph-based methods for feature-based SLAM. Further, empirical results show our method avoids catastrophic failures that arise in existing methods due to using odometery as an initial guess for non-linear optimization, while its accuracy degrades gracefully as sensor noise is increased. We demonstrate our method through extensive simulations and comparisons with an existing state-of-the-art solver.},
archivePrefix = {arXiv},
arxivId = {1609.05235},
author = {Agarwal, Saurav and Shree, Vikram and Chakravorty, Suman},
eprint = {1609.05235},
file = {:home/chris/Documents/Mendeley Desktop/RFM-SLAM$\backslash$: Exploiting Relative Feature Measurements to Separate Orientation and Position Estimation in SLAM.pdf:pdf},
isbn = {9781509046324},
keywords = {graph-based slam,mapping,measurements-based simultaneous localization and,non-linear optimiza-,relative measurements,slam,tion},
title = {{RFM-SLAM: Exploiting Relative Feature Measurements to Separate Orientation and Position Estimation in SLAM}},
url = {http://arxiv.org/abs/1609.05235},
year = {2016}
}
@article{Corke2007,
abstract = {— This paper describes a novel experiment in which two very different methods of underwater robot localization are compared. The first method is based on a geometric approach in which a mobile node moves within a field of static nodes, and all nodes are capable of estimating the range to their neighbours acoustically. The second method uses visual odometry, from stereo cameras, by integrating scaled optical flow. The fun-damental algorithmic principles of each localization technique is described. We also present experimental results comparing acoustic localization with GPS for surface operation, and a comparison of acoustic and visual methods for underwater operation.},
author = {Corke, Peter and Detweiler, Carrick and Dunbabin, Matthew and Hamilton, Michael and Rus, Daniela and Vasilescu, Iuliu},
doi = {10.1109/ROBOT.2007.364181},
file = {:home/chris/Documents/Mendeley Desktop/Experiments with Underwater Robot Localization and Tracking.pdf:pdf},
isbn = {1424406013},
issn = {1050-4729},
journal = {Robot. Autom. 2007 IEEE Int. Conf.},
pages = {4556--4561},
title = {{Experiments with underwater robot localization and tracking}},
year = {2007}
}
@article{Thrun2004,
abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on a factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and real-world data.},
author = {Thrun, Sebastian and Montemerlo, Michael and Koller, Daphne and Wegbreit, Ben and Nieto, Juan and Nebot, Eduardo},
doi = {10.1.1.16.2153},
file = {:home/chris/Documents/Mendeley Desktop/FastSLAM$\backslash$: An Efficient Solution to the Simultaneous Localization And Mapping Problem with Unknown Data Association.pdf:pdf},
isbn = {0262511290},
journal = {J. Mach. Learn. Res.},
number = {3},
pages = {380--407},
title = {{Fastslam: An efficient solution to the simultaneous localization and mapping problem with unknown data association}},
volume = {4},
year = {2004}
}
@article{Eustice2005,
author = {Eustice, R.M. and Pizarro, O. and Singh, H.},
doi = {10.1109/JOE.2008.923547},
file = {:home/chris/Documents/Mendeley Desktop/Large-Area Visually Augmented Navigation for Autonomous Underwater Vehicles.pdf:pdf},
issn = {0364-9059},
journal = {IEEE J. Ocean. Eng.},
number = {2},
pages = {103--122},
title = {{Visually Augmented Navigation for Autonomous Underwater Vehicles}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4625213},
volume = {33},
year = {2005}
}
@article{Singh2007,
abstract = {Large area mapping at high resolution underwater continues to be constrained by the mismatch between available navigation as compared to sensor accuracy. In this paper we present advances that exploit consistency and redundancy within local sensor measurements to build high resolution optical and acoustic maps that are a consistent representation of the environment. We present our work in the context of real world data acquired using Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs) working in diverse applications including shallow water coral reef surveys with the Seabed AUV, a forensic survey of the RMS Titanic in the North Atlantic at a depth of 4100 meters using the Hercules ROV and a survey of the TAG hydrothermal vent area in the mid-Atlantic at a depth of 2600m using the Jason II ROV. Specifically we focus on the related problems of Structure from Motion and Visually Based Navigation from underwater optical imagery assuming pose instrumented calibrated cameras. We present general wide baseline solutions for these problems based on the extension of techniques from the SLAM, photogrammetric and the computer vision communities. We also examine how such techniques can be extended for the very different sensing modality and scale associated with multi-beam bathymetric mapping. For both the optical and acoustic mapping cases we also show how the consistency in mapping can be used not only for better mapping but also to refine navigation estimates.},
author = {Singh, H and Roman, C and Pizarro, O and Eustice, R},
file = {:home/chris/Documents/Mendeley Desktop/Advances in High Resolution Imaging from Underwater Vehicles.pdf:pdf},
isbn = {978-3-540-48110-2},
journal = {Robot. Res.},
pages = {430--448},
title = {{Advances in high resolution imaging from underwater vehicles}},
volume = {28},
year = {2007}
}
@article{Dosovitskiy2014,
abstract = {Deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning paradigm, where sufficiently many input-output pairs are required for training. Acquisition of large training sets is one of the key challenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. In contrast to supervised network training, the resulting feature representation is not class specific. It rather provides robustness to the transformations that have been applied during training. This generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256). While such generic features cannot compete with class specific features from supervised training on a classification task, we show that they are advantageous on geometric matching problems, where they also outperform the SIFT descriptor.},
archivePrefix = {arXiv},
arxivId = {1406.6909},
author = {Dosovitskiy, Alexey and Fischer, Philipp and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
doi = {10.1109/TPAMI.2015.2496141},
eprint = {1406.6909},
file = {:home/chris/Documents/Mendeley Desktop/Discriminative Unsupervised Feature Learningwith Exemplar Convolutional Neural Networks.pdf:pdf},
issn = {1939-3539},
pages = {1--14},
pmid = {26540673},
title = {{Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1406.6909},
year = {2014}
}
@article{Li2015,
abstract = {Map learning and self-localization based on perception of the environment's structure are fundamental capacities required for intelligent robots to realize true autonomy. Simultaneous Localization and Mapping (SLAM) is an effective technique for such robots, as it addresses the problem of incrementally building an environment map from noisy sensory data and tracking the robot's path with the built map. As a popular SLAM solution, FastSLAM suffers from limitation on error accumulation introduced by incorrect odometry model and inaccurate linearization of the SLAM nonlinear functions. To overcome the problem, a new Jacobian free neural network (NN) based FastSLAM algorithm is derived and discussed in this paper. The main contribution of the algorithm is twofold: on the one hand, the odometry error is online compensated by using a multilayer NN, and the NN is online trained during the SLAM process; on the other hand, the third-degree Cubature rule for Gaussian weighted integral, which calculates nonlinear transition density of Gaussian prior up to the 3rd order nonlinearity, is utilized to estimate the SLAM state (i.e., the robot path and environment map) and to online train the NN compensator. The performance of proposed SLAM is investigated and compared with that of popular FastSLAM2.0 in simulations and experiments. Results show that the proposed method improves the SLAM performance.},
author = {Li, Qing-Ling and Song, Yu and Hou, Zeng-Guang},
doi = {10.1016/j.neucom.2014.06.095},
file = {:home/chris/Documents/Mendeley Desktop/Neural network based FastSLAM for autonomous robots in unknown environments.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Autonomous robot,Cubature rule,Gaussian Weighted Integral (GWI),Neural network,Particle filter,Simultaneous Localization and Mapping (SLAM)},
pages = {99--110},
publisher = {Elsevier},
title = {{Neural network based FastSLAM for autonomous robots in unknown environments}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215004312},
volume = {165},
year = {2015}
}
@article{Choi2007,
abstract = {This paper addresses the problem of simultaneous localization and map building (SLAM) using a neural network aided extended Kalman filter (NNEKF) algorithm. Since the EKF is based on the white noise assumption, if there are colored noise or systematic bias error in the system, EKF inevitably diverges. The neural network in this algorithm is used to approximate the uncertainty of the system model due to mismodeling and extreme nonlinearities. Simulation results are presented to illustrate the proposed algorithm NNEKF is very effective compared with the standard EKF algorithm under the practical condition where the mobile robot has bias error in its modeling and environment has strong uncertainties. In this paper, we propose an algorithm which enables a biased control input in vehicle model using neural network},
author = {Choi, Minyong and Sakthivel, R. and Chung, Wan Kyun},
doi = {10.1109/ROBOT.2007.363565},
file = {:home/chris/Documents/Mendeley Desktop/Neural Network-Aided Extended Kalman Filter for SLAM Problem.pdf:pdf},
isbn = {1424406021},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
number = {April},
pages = {1686--1690},
title = {{Neural network-aided extended Kalman filter for SLAM problem}},
year = {2007}
}
@article{Rublee2011,
abstract = {Feature matching is at the base of many computer vi-sion problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for de-tection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magni-tude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world ap-plications, including object detection and patch-tracking on a smart phone.},
author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
doi = {10.1109/ICCV.2011.6126544},
file = {:home/chris/Documents/Mendeley Desktop/ORB$\backslash$: an efficient alternative to SIFT or SURF.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {2564--2571},
pmid = {20033598},
title = {{ORB: An efficient alternative to SIFT or SURF}},
year = {2011}
}
@article{Paull2014,
abstract = {Autonomous underwater vehicle (AUV) navigation and localization in underwater environments is particularly challenging due to the rapid attenuation of Global Positioning System (GPS) and radio-frequency signals. Underwater communications are low bandwidth and unreliable, and there is no access to a global positioning system. Past approaches to solve the AUV localization problem have employed expensive inertial sensors, used installed beacons in the region of interest, or required periodic surfacing of the AUV. While these methods are useful, their performance is fundamentally limited. Advances in underwater communications and the application of simultaneous localization and mapping (SLAM) technology to the underwater realm have yielded new possibilities in the field. This paper presents a review of the state of the art of AUV navigation and localization, as well as a description of some of the more commonly used methods. In addition, we highlight areas of future research potential.},
author = {Paull, Liam and Saeedi, Sajad and Seto, Mae and Li, Howard},
doi = {10.1109/JOE.2013.2278891},
file = {:home/chris/Documents/Mendeley Desktop/AUV Navigation and Localization$\backslash$: A Review.pdf:pdf},
isbn = {0364-9059},
issn = {03649059},
journal = {IEEE J. Ocean. Eng.},
keywords = {Autonomous underwater vehicles (AUVs),marine navigation,simultaneous localization and mapping},
number = {1},
pages = {131--149},
title = {{AUV navigation and localization: A review}},
volume = {39},
year = {2014}
}
@article{Agrawal2016,
abstract = {The dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it possible to learn useful features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigate if the awareness of egomotion can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We show that given the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on visual tasks of scene recognition, object recognition, visual odometry and keypoint matching.},
archivePrefix = {arXiv},
arxivId = {1505.01596},
author = {Agrawal, Pulkit and Carreira, Joao and Malik, Jitendra},
doi = {10.1109/ICCV.2015.13},
eprint = {1505.01596},
file = {:home/chris/Documents/Mendeley Desktop/Learning to See by Moving.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {37--45},
title = {{Learning to see by moving}},
volume = {11-18-Dece},
year = {2016}
}
@article{Hildebrandt2010,
abstract = {This paper addresses the problem of AUV navigation by showing the feasibility of a stereo visual-inertial approach to odometry retrieval for an AUV. This information is intended as input for a complete SLAM system. After its classification among many other similar approaches in recent work is shown, the algorithm is described in detail. A number of experiments conducted on synthetic data show the performance in respect to precision and computational cost. As a conclusion, future extensions and applications are briefly discussed.},
author = {Hildebrandt, Marc and Kirchner, Frank},
doi = {10.1109/OCEANSSYD.2010.5603681},
file = {:home/chris/Documents/Mendeley Desktop/IMU-aided stereo visual odometry for ground-tracking AUV applications.pdf:pdf},
isbn = {9781424452217},
journal = {Ocean. IEEE Sydney, Ocean. 2010},
keywords = {AUV IMU Stereo Visual Odometry},
title = {{IMU-aided stereo visual odometry for ground-tracking AUV applications}},
year = {2010}
}
@article{He2009,
abstract = {The paper describes a localization system for autonomous underwater vehicles (AUV). It uses a DVL (Doppler velocity log) sensor and AHRS (attitude and heading reference system) sensor to measure AUV's depth, attitude and velocities relative to the bottom. A mechanically scanning imaging sonar (MSIS) is employed to obtain acoustic images of objects in underwater environment. In order to estimate optimally AUV pose without a priori map of the environment, simultaneous localization and map building (SLAM), a prevailing method in the past decade, is presented based on point features extraction and EKF-based estimator. Use Fluvia Nautic marina data set we compare the proposed method with traditional dead-reckoning, results show that our solution can reduce estimation error significantly.},
author = {He, Bo and Yang, Ke and Zhao, Shuai and Wang, Yitong},
doi = {10.1109/ICMA.2009.5246398},
file = {:home/chris/Documents/Mendeley Desktop/Underwater simultaneous localization and mapping based on EKF and point features.pdf:pdf},
isbn = {9781424426935},
journal = {2009 IEEE Int. Conf. Mechatronics Autom. ICMA 2009},
keywords = {Kalman filter,Point feature,SLAM,Sonar},
pages = {4845--4850},
title = {{Underwater simultaneous localization and mapping based on EKF and point features}},
year = {2009}
}
@article{West2006,
abstract = {This paper will present a robust extended Kalman filter (REKF) applied to the navigation of an autonomous underwater vehicle (AUV) using robust Simultaneous Localization and Mapping (SLAM) techniques. Conventional Kalman Filter methods suffer from the assumption of Gaussian noise statistics, which often lead to failures when these assumptions do not hold. Additionally, the linearization errors associated with the implementation of the standard EKF can also severely degrade the performance of the localization estimate. Currently, Stochastic Mapping provides a framework for the concurrent mapping of landmarks and localization of the vehicle with respect to the landmarks. However, the Stochastic Map is essentially an augmented EKF with the limitations thereof. This research addresses the linearization and Guassian assumption errors as they relate to the SLAM problem by proposing a new method, Robust Stochastic Mapping. The Robust Stochastic Map uses a Robust EKF (REKF) in order to address these limitations through the implementation of the bounded H∞ norm. Simulated data are presented to illustrate the advantage of the localization using the proposed estimation procedure.},
author = {West, ME Michael E. and Syrmos, VL Vassilis L.},
doi = {10.1109/CACSD-CCA-ISIC.2006.4776914},
file = {:home/chris/Documents/Mendeley Desktop/Navigation of an autonomous underwater vehicle (AUV) using robust SLAM.pdf:pdf},
isbn = {0780397967},
journal = {Comput. Aided Control Syst. Des. {\ldots}},
keywords = {(Under)water vehicles,Autonomous systems,Dead reckoning,Degradation,Filtering,Gaussian noise,Navigation,Noise robustness,Robust control,Simultaneous localization and mapping,Statistics,Stochastic processes,Underwater vehicles},
pages = {1801--1806},
title = {{Navigation of an autonomous underwater vehicle (AUV) using robust SLAM}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4776914{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4776914},
year = {2006}
}
@article{Botelho2009a,
abstract = {The use of Autonomous Underwater Vehicles (AUVs) for underwater tasks is a promising robotic field. These robots can carry visual inspection cameras. Besides serving the activities of inspection and mapping, the captured images can also be used to aid navigation and localization of the robots. Visual odometry is the process of determining the position and orientation of a robot by analyzing the associated camera images. It has been used in a wide variety of non-standard locomotion robotic methods. In this context, this paper proposes an approach to visual odometry and mapping of underwater vehicles. Supposing the use of inspection cameras, this proposal is composed of two stages: i) the use of computer vision for visual odometry, extracting landmarks in underwater image sequences and ii) the development of topological maps for localization and navigation. The integration of such systems will allow visual odometry, localization and mapping of the environment. A set of tests with real robots was accomplished, regarding online and performance issues. The results reveals an accuracy and robust approach to several underwater conditions, as illumination and noise, leading to a promissory and original visual odometry and mapping technique.},
author = {Botelho, Silvia Silva da Costa and {Drews Junior}, Paulo Lilles Jorge and Figueiredo, M{\^{o}}nica da Silva and Rocha, Celina Haffele Da and Oliveira, Gabriel Leivas},
doi = {10.1007/BF03194505},
file = {:home/chris/Documents/Mendeley Desktop/Appearance-based odometry and mapping with feature descriptors for underwater robots.pdf:pdf},
issn = {0104-6500},
journal = {J. Brazilian Comput. Soc.},
keywords = {computer vision,robotics,self-localization and mapping,topological maps,underwater vehicles},
number = {3},
pages = {47--54},
title = {{Appearance-based odometry and mapping with feature descriptors for underwater robots}},
url = {http://link.springer.com/10.1007/BF03194505},
volume = {15},
year = {2009}
}
@article{Lee2013,
abstract = {This paper presents experiments on vision-based localization of an autonomous underwater vehicle (AUV) using graph-based simultaneous localization and mapping (SLAM). Relative range and bearing values of each landmark are obtained from image processing results. And a graph structure is built using the landmark detection results and dead-reckoning data of the AUV. The structured graph is optimized by a graph-based SLAM algorithm. Finally, the performance of the graph-based SLAM is compared to an EKF-based SLAM result.},
author = {Lee, Donghwa and Kim, Donghoon and Lee, Sangwon and Myung, Hyun and Choi, Hyun Taek},
doi = {10.1109/URAI.2013.6677329},
file = {:home/chris/Documents/Mendeley Desktop/Experiments on localization of an AUV using graph-based SLAM.pdf:pdf},
isbn = {978-1-4799-1197-4},
journal = {2013 10th Int. Conf. Ubiquitous Robot. Ambient Intell. URAI 2013},
keywords = {AUV,EKF-based SLAM,SLAM,graph-based SLAM,underwater localization},
pages = {526--527},
title = {{Experiments on localization of an AUV using graph-based SLAM}},
year = {2013}
}
@article{Zhou2008,
abstract = {This paper presents a novel vision-based sensory package and an information-efficient simultaneous localization and mapping (SLAM) algorithm. Together, we offer a solution for building 3-D dense map in an unknown and unstructured environment with minimal computational costs. The sensory package we adopt consists of a conventional camera and a range imager, which provide range and bearing and elevation inputs as commonly used by 3-D feature-based SLAM. In addition, we propose an algorithm to give the robots the `intelligencerdquo to select, out of the steadily collected data, the maximally informative observations to be used in the estimation process. We show that, although the actual evaluation of information gain for each frame introduces an additional computational cost, the overall efficiency is significantly increased by keeping the matrix compact. The noticeable advantage of this strategy is that the continuously gathered data are not heuristically segmented prior to being input to the filter. Quite the opposite, the scheme lends itself to be statistically optimal and is capable of handling large datasets collected at realistic sampling rates.},
author = {Zhou, Weizhen and {Valls Miro}, Jaime and Dissanayake, Gamini},
doi = {10.1109/TRO.2008.2004834},
file = {:home/chris/Documents/Mendeley Desktop/Information-Efficient 3-D Visual SLAM for Unstructured Domains.pdf:pdf},
isbn = {978-1-4244-1501-4},
issn = {15523098},
journal = {IEEE Trans. Robot.},
keywords = {Delayed state formulation,Information form,Simultaneous localization and mapping (SLAM),Urban search and rescue (USAR),Vision},
number = {5},
pages = {1078--1087},
title = {{Information-efficient 3-D visual SLAM for unstructured domains}},
volume = {24},
year = {2008}
}
@article{Bonin-Font2014a,
abstract = {This paper proposes a straightforward but effective approach to per-form visual SLAM, especially suitable for underwater vehicles. One of the most important steps in this procedure is the image registration method, since it reinforces the data association and thus makes it possible to close loops reliably. Since the traditional EKF-SLAM approaches are usually costly in terms of running time, the approach presented in this paper strengthens this method by adopting a trajectory-based schema that re-duces the computational requirements. The pose of the vehicle is esti-mated using an Extended Kalman Filter (EKF), which predicts the vehi-cle motion by means of a visual odometer and corrects these predictions using the data associations (loop closures) between the current frame and the previous ones. Since the use of standard EKFs entail linearization er-rors that can distort the vehicle pose estimations, the approach was also tested using an Iterated Kalman Filter (IEKF) instead. The approach has been tested on real underwater vehicles, in controlled scenarios and in shallow sea waters. The approach has shown an excellent performance in diverse experiments, with very limited errors of the estimated trajectory.},
author = {Bonin-Font, Francisco and Burguera, Antoni and Oliver, Gabriel},
file = {:home/chris/Documents/Mendeley Desktop/Underwater Visual SLAM using a Bottom Looking Camera.pdf:pdf},
keywords = {SLAM,underwater,vision},
number = {1361},
pages = {1--28},
title = {{Underwater Visual SLAM using a Bottom Looking Camera}},
url = {http://srv.uib.es/ref/1361.pdf},
year = {2014}
}
@article{Kim2011,
abstract = {This paper reports on a method to combine expected information gain with visual saliency scores in order to choose geometrically and visually informative loop-closure candidates for pose-graph visual simultaneous localization and mapping (SLAM). Two different bag-of-words saliency metrics are introduced{\&}{\#}x2014;global saliency and local saliency. Global saliency measures the rarity of an image throughout the entire data set, while local saliency describes the amount of texture richness in an image. The former is important in measuring an overall global saliency map for a given area, and is motivated from inverse document frequency (a measure of rarity) in information retrieval. Local saliency is defined by computing the entropy of the bag-of-words histogram, and is useful to avoid adding visually benign key frames to the map. The two different metrics are presented and experimentally evaluated with indoor and underwater imagery to verify their utility.},
author = {Kim, Ayoung and Eustice, Ryan M.},
doi = {10.1109/IROS.2011.6048439},
file = {:home/chris/Documents/Mendeley Desktop/Combined visually and geometrically informative link hypothesis for pose-graph visual SLAM using bag-of-words.pdf:pdf},
isbn = {9781612844541},
issn = {2153-0858},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {1647--1654},
title = {{Combined visually and geometrically informative link hypothesis for pose-graph visual SLAM using bag-of-words}},
year = {2011}
}
@article{Petrich2011,
abstract = {Attitude control systems for autonomous underwater vehicles are often implemented with separate controllers for pitch motion in the vertical plane and yaw motion in the horizontal plane. We propose a novel time-varying model for a streamlined autonomous underwater vehicle that explicitly displays the coupling between yaw and pitch motion due to nonzero roll angle and/or roll rate. The model facilitates the use of a multi-input multi-output H??? control design that is robust to yawpitch coupling. The efficacy of our approach is demonstrated with field trials. ?? 2010 Elsevier Ltd. All rights reserved.},
author = {Petrich, Jan and Stilwell, Daniel J.},
doi = {10.1016/j.oceaneng.2010.10.007},
file = {:home/chris/Documents/Mendeley Desktop/Robust-control-for-an-autonomous-underwater-vehicle-that-suppresses-pitch-and-yaw-coupling{\_}2011{\_}Ocean-Engineering.pdf:pdf},
isbn = {0029-8018},
issn = {00298018},
journal = {Ocean Eng.},
keywords = {Actuator and sensor allocation,Autonomous underwater vehicles,Fin-mixing,H??? synthesis,Loop shaping,Robust attitude control},
number = {1},
pages = {197--204},
publisher = {Elsevier},
title = {{Robust control for an autonomous underwater vehicle that suppresses pitch and yaw coupling}},
url = {http://dx.doi.org/10.1016/j.oceaneng.2010.10.007},
volume = {38},
year = {2011}
}
@article{Yuh1994,
abstract = {Remotely operated, underwater robotic vehicles have become the important tool to explore the secrete life undersea. They are used for various purposes: inspection, recovery, construction, etc. With the increased utilization of remotely operated vehicles in subsea applica- tions, the development of autonomous vehicles becomes highly desirable to enhance operator efficiency. However, engineering problems associ- ated with the high density, nonuniform and unstructured seawater environment, and the nonlinear response of the vehicle make a high degree of autonomy difficult to achieve. The dynamic model of the untethered vehicle is presented, and an adaptive control strategy for such vehicles is described. The robustness of the control system with respect to the nonlinear dynamic behavior and parameter uncertainties is investigated by computer simulation. The results show that the use of the adaptive control system can provide the high performance of the vehicle in the presence of unpredictable changes in the dynamics of the vehicle and its environment.},
author = {Yuh, J.},
doi = {10.1109/37.272779},
file = {:home/chris/Documents/Mendeley Desktop/Modeling and Control of Underwater Robotic Vehicles.pdf:pdf},
isbn = {0-7803-0823-9},
issn = {02721708},
journal = {IEEE Control Syst. Mag.},
number = {2},
pages = {39--46},
title = {{Modeling and Control of Underwater Robotic Vehicles}},
volume = {14},
year = {1994}
}
@article{Xu2012,
abstract = {The inherent strongly nonlinear and coupling performance of the Autonomous Underwater Vehicles (AUV), maneuvering motion in the diving plane determines its difficulty in parametric identification. The motion parameters in diving plane are obtained by executing the Zigzag-like motion based on a mathematical model of maneuvering motion. A separate identification method is put forward for parametric identification by investigating the motion equations. Support vector machine is proposed to estimate the hydrodynamic derivatives by analyzing the data of surge, heave and pitch motions. Compared with the standard coefficients, the identified parameters show the validation of the proposed identification method. Sensitivity analysis based on numerical simulation demonstrates that poor sensitive derivative gives bad estimation results. Finally the motion simulation is implemented based on the dominant sensitive derivatives to verify the reconstructed model. {\textcopyright} 2012 Publishing House for Journal of Hydrodynamics.},
author = {Xu, Feng and Zou, Zao Jian and Yin, Jian Chuan and Cao, Jian},
doi = {10.1016/S1001-6058(11)60299-0},
file = {:home/chris/Documents/Mendeley Desktop/Parametric-identification-and-sensitivity-analysis-for-Autonomous-Underwater-Vehicles-in-diving-plane{\_}2012{\_}Journal-of-Hydrodynamics-Ser-.pdf:pdf},
issn = {10016058},
journal = {J. Hydrodyn.},
keywords = {Autonomous Underwater Vehicles (AUVs),parametric identification,sensitivity analysis,support vector machine},
number = {5},
pages = {744--751},
publisher = {Publishing House for Journal of Hydrodynamics},
title = {{Parametric identification and sensitivity analysis for Autonomous Underwater Vehicles in diving plane}},
url = {http://dx.doi.org/10.1016/S1001-6058(11)60299-0},
volume = {24},
year = {2012}
}
@article{Skoglund2012,
abstract = {We compare dead-reckoning of underwater vehicles based on inertial sensors and kinematic models on one hand, and control inputs and hydrodynamic model on the other hand. Both can be used in an inertial navigation system to provide relative motion and absolute orientation of the vehicle. The combination of them is particularly useful for robust navigation in the case of missing data from the crucial doppler log speedometer. As a concrete result, we demonstrate that the performance critical doppler log can be replaced with longitudinal dynamics in the case of missing data, based on field test data of a remotely operated vehicle.},
author = {Skoglund, Martin and Gustafsson, Fredrik},
file = {:home/chris/Documents/Mendeley Desktop/Modeling and Sensor Fusion of a Remotely Operated Underwater Vehicle.pdf:pdf},
isbn = {9780982443842},
journal = {{\ldots} Fusion (FUSION), 2012 {\ldots}},
keywords = {autonomous underwater vehicles,hydrodynamics,ine},
number = {July 2012},
pages = {947--954},
title = {{Modeling and sensor fusion of a remotely operated underwater vehicle}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6289904},
volume = {2},
year = {2012}
}
@phdthesis{Barenthin2006,
abstract = {can be formulated as convex optimization programs. An Achilles' heel in input design is that the solution depends on the system itself, and this problem can be handled by iterative procedures where the input design is based on a model of the system. Benefits of optimal input design are quantified for typical industrial appli- cations. The result shows that the experiment time can be substantially shortened and that the input power can be reduced. Another contribution of the thesis is a procedure where input design is connected to robust control. For a certain system structure with uncertain parameters, it is shown that the existence of a feedback controller that guarantees a given performance specification can be formulated as a convex optimization program. Furthermore, a method for input design for mul- tivariable systems is proposed. The constraint on the model quality is transformed to a linear matrix inequality using a separation of graphs theorem. The result in- dicates that in order to obtain a model suitable for control design, it is important to increase the power of the input in the low-gain direction of the system relative to the power in the high-gain direction. A critical issue when validating closed-loop stability is to obtain an accurate estimate of the maximum gain of the system. This problem boils down to finding the input signal that maximizes the gain. Procedures for gain estimation of nonlinear systems are proposed and compared. One approach uses a model of the system to design the optimal input. In other approaches, no model is required, and the system itself determines the optimal input sequence in repeated experiments.},
author = {Barenthin, M{\"{a}}rta},
booktitle = {KTH Sch. Electr. Eng.},
file = {:home/chris/Documents/Mendeley Desktop/On Input Design in System Identification for Control.pdf:pdf},
isbn = {91-7178-400-4},
issn = {1653-5146},
title = {{On Input Design in System Identification for Control}},
year = {2006}
}
@article{Goodwin1971,
author = {Goodwin, G.C.},
doi = {10.1049/piee.1971.0183},
file = {:home/chris/Documents/Mendeley Desktop/Optimal Inpput Signals for Nonlinear System Identification.pdf:pdf},
issn = {00203270},
journal = {Proc. IEE},
number = {7},
pages = {922},
title = {{Optimal input signals for nonlinear system identification}},
url = {http://digital-library.theiet.org/content/journals/10.1049/piee.1971.0183{\%}0Ahttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5251508},
volume = {118},
year = {1971}
}
@article{Fernandez2015,
abstract = {—This paper proposes the use of UWSim (an under-water simulator) in combination with Simurv (a kinematic and dynamic library for Underwater Vehicle-Manipulator Systems control algorithms) and ROS (a well-known robotics framework) in order to simulate the dynamics of an Intervention Autonomous Underwater Vehicle and its application to the benchmarking of autonomous control algorithms in the field of archaeology dredging.},
author = {Fernandez, J. J. and Perez, J. and Penalver, A. and Sales, J. and Fornas, D. and Sanz, P. J.},
doi = {10.1109/OCEANS-Genova.2015.7271514},
file = {:home/chris/Documents/Mendeley Desktop/Benchmarking using UWSim, Simurv and ROS.pdf:pdf},
isbn = {9781479987368},
journal = {MTS/IEEE Ocean. 2015 - Genova Discov. Sustain. Ocean Energy a New World},
keywords = {I-AUV,UWSim,benchmarking,free floating control,simulation,underwater autonomous intervention},
title = {{Benchmarking using UWSim, Simurv and ROS: An autonomous free floating dredging intervention case study}},
year = {2015}
}
@article{Andersson1998,
author = {Andersson, Lennart},
file = {:home/chris/Documents/Mendeley Desktop/A Manual for System Identification.pdf:pdf},
journal = {{\ldots} Identification. KF {\ldots}},
pages = {1--8},
title = {{A manual for system identification}},
url = {http://automatica.dei.unipd.it/public/Schenato/PSC/2010{\_}2011/gruppo4-Building{\_}termo{\_}identification/IdentificazioneTermodinamica20072008/Biblio/Articoli/a manual for system identification.pdf},
year = {1998}
}
@article{Silveira2015,
abstract = {We present a bio-inspired approach to deal with the localization and spatial mapping problem, extending the successful previous RatSLAM approach from 2D ground vehicles to the 3D underwater environments. Our approach, called DolphinSLAM, is a SLAM system based on mammals navigation. Experiments in simulation and real environments were conducted involving long-term navigation tasks with different robots and sensors. Our proposal is opensource, being integrated with the Robot Operating System (ROS).},
author = {Silveira, Luan and Guth, Felipe and Drews, Paulo and Ballester, Pedro and Machado, Matheus and Codevilla, Felipe and Duarte-Filho, Nelson and Botelho, Silvia},
doi = {10.1016/j.ifacol.2015.06.035},
file = {:home/chris/Documents/Mendeley Desktop/An Open-source Bio-inspired Solution to Underwater SLAM.pdf:pdf},
isbn = {14746670 (ISSN)},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Slam and bio-inspired approaches,Underwater robotics},
number = {2},
pages = {212--217},
title = {{An open-source bio-inspired solution to underwater SLAM}},
volume = {28},
year = {2015}
}
@article{Tateno2017,
abstract = {Given the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for accurate and dense monocular reconstruction. We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM. Our fusion scheme privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction for estimating the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM. Finally, we propose a framework to efficiently fuse semantic labels, obtained from a single frame, with dense SLAM, yielding semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.},
archivePrefix = {arXiv},
arxivId = {1704.03489},
author = {Tateno, Keisuke and Tombari, Federico and Laina, Iro and Navab, Nassir},
eprint = {1704.03489},
file = {:home/chris/Documents/Mendeley Desktop/CNN-SLAM$\backslash$: Real-time dense monocular SLAM with learned depth prediction.pdf:pdf},
title = {{CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction}},
url = {http://arxiv.org/abs/1704.03489},
year = {2017}
}
@article{Hammond2015,
abstract = {This paper presents a SLAM-based approach for creating maps of underwater terrain using AUVs with poor inertial information. The initial motivating application for this work was mapping in the non-inertial frame of a free-drifting Antarctic iceberg, but poor inertial information can also occur if low-cost, high drift inertial instrumentation is used in standard mapping tasks, or if DVL bottom lock is lost during the mission. This paper presents a SLAM-based approach in which features are extracted from concatenated multibeam data and descriptors are created, allowing these features to be compared against past terrain as the vehicle traverses the area. There have been a number of previous research efforts that used feature-based SLAM techniques for underwater mapping, but they have generally made assumptions or relied on sensors that are inconsistent with this paper's motivating application, such as a flat bottom, the availability of visual imagery, or manmade fiducial markers. The method presented here uses natural terrain, is robust to water turbidity, and can be used in areas with vertical terrain like the walls of canyons and icebergs. Results are presented on data collected from Monterey Canyon using a vehicle with a high-grade IMU but that lost DVL bottom lock during the mapping run. {\textcopyright} 2014 IEEE.},
author = {Hammond, Marcus and Rock, Stephen M.},
doi = {10.1109/AUV.2014.7054419},
file = {:home/chris/Documents/Mendeley Desktop/A SLAM-based Approach to Underwater Mapping using AUVs with Poor Intertial Information.pdf:pdf},
isbn = {9781479943449},
journal = {2014 IEEE/OES Auton. Underw. Veh. AUV 2014},
title = {{A SLAM-based approach for underwater mapping using AUVs with poor inertial information}},
year = {2015}
}
@article{Grisetti2010,
author = {Grisetti, Giorgio and K{\"{u}}mmerle, Rainer and Stachniss, Cyrill and Burgard, Worlfram},
doi = {10.1109/MITS.2010.939925},
file = {:home/chris/Documents/Mendeley Desktop/A Tutorial On Graph-based SLAM.pdf:pdf},
isbn = {1939-1390 VO - 2},
issn = {1939-1390},
journal = {IEEE Intell. Transp. Syst. Mag.},
pages = {31--43},
title = {{A tutorial on graph-based SLAM}},
year = {2010}
}
@book{Thrun1999,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Thrun, Sebastian},
booktitle = {Commun. ACM},
doi = {10.1145/504729.504754},
eprint = {arXiv:1011.1669v3},
file = {:home/chris/Documents/Mendeley Desktop/ProbabilisticRobotics.pdf:pdf},
isbn = {9788578110796},
issn = {00010782},
number = {3},
pmid = {25246403},
title = {{Probabilistic robotics}},
url = {http://portal.acm.org/citation.cfm?doid=504729.504754},
volume = {45},
year = {2002}
}
@article{Hassan2013,
abstract = {Modelling and control of Autonomous Underwater Vehicles (AUVs) pose serious challenges due to their complex, inherently nonlinear and time-varying dynamics. Thus there is a necessity for designing a robust and stable control system that would selftune the system when the performance degrades during the operation. Due to the complexities associated with the AUV dynamics, a black box identification technique is used for modelling the behaviour of the AUV. To improve online identification in the presence of noisy data, this thesis introduces a novel identifier scheme for identification of non-linear systems with disturbances, that we call semi serial-parallel model (SSPM). The model based controller and the identification model of the UNSW@ADFA AUV are developed using the offline and online techniques and are based upon Fuzzy system and Hybrid Neuro-Fuzzy Network techniques. A novel auto-generating mechanism with entropy based Differential Evolution (DE) system modelling is proposed to generate the AUV model without any prior knowledge of the physical relationship inside the system or its behaviour. It is carried out in two steps; an off-line procedure and an on-line procedure. The method comprises of an automatic structure generating phase using entropy based technique and a parameter-learning phase. The accuracy of the model is suitably controlled using the entropy measure. The parameter learning phase uses the back-propagation technique. To improve the accuracy and also for generalization of the model to handle different data sets, DE technique is employed whereby the parameters of the model are suitably tuned using evolutionary technique. A number of benchmark problems are considered to validate the methodology and the accuracy of the results from the proposed method are found to be superior compared with the existing techniques. The proposed mechanism is used to design the control system. These techniques need to be real time implementable. Real time validations using Hardware In Loop (HIL) simulations show that both identification methods are feasible for control. The accuracy of identification using online auto-generating mechanism is better than the offline technique in terms of accuracy and computational time. HIL simulations applied to simulate the performance of the developed control system of surge, pitch and yaw movements. Finally, real time experiments of the proposed algorithm for identifying the AUV dynamics are implemented. The experimental results show that the proposed identification mechanism and the model adaptive controller are capable of controlling the AUV suitably in a real environment, demonstrating its robustness characteristics.},
author = {Hassan, Osama Ibrahim Hassanein},
file = {:home/chris/Documents/Mendeley Desktop/Black-Box Identification and Control for Autonomous Underwater Vehicles.pdf:pdf},
number = {March},
title = {{Black-Box Identification and Control for Autonomous Underwater Vehicles}},
year = {2013}
}
@book{Antonelli2014,
abstract = {papers3://publication/doi/10.1007/978-3-662-14387-2},
author = {Antonelli, Gianluca},
booktitle = {Springer Tracts Adv. Robot.},
doi = {10.1007/978-3-319-02877-4},
file = {:home/chris/Documents/Mendeley Desktop/Underwater Robots.pdf:pdf},
isbn = {9783540317524},
issn = {1610742X},
title = {{Underwater Robots}},
volume = {96},
year = {2014}
}
@article{Naeem2003,
author = {Naeem, W. and Sutton, R. and Chudley, J.},
doi = {10.1016/S1474-6670(17)37777-7},
file = {:home/chris/Documents/Mendeley Desktop/System Identification, Modelling and Control of an AUV.pdf:pdf},
issn = {14746670},
journal = {Proc. MCMC2003 Conf.},
keywords = {modelling and lqg control,system identification,underwater vehicles},
pages = {37--42},
title = {{System identification, modelling and control of an autonomous underwater vehicle}},
volume = {296},
year = {2003}
}
@article{Tan2012,
author = {Tan, K. M. and Anvar, A. and Lu, T. F.},
file = {:home/chris/Documents/Mendeley Desktop/Autonomous-Underwater-Vehicle-AUV-Dynamics-Modeling-and-Performance-Evaluation.pdf:pdf},
journal = {Int. J. Mech. Aerospace, Ind. Mechatron. Manuf. Eng.},
title = {{Autonomous Underwater Vehicle (AUV) Dynamics Modeling and Performance Evaluation}},
volume = {6},
year = {2012}
}
@article{Xu2010,
abstract = {This paper investigates the application of neural networks-based generalised predictive motion control for an autonomous underwater vehicle (AUV). The modified Elman neural networks (MENNs) are used as the multi-step predictive model; and the fused identification model is proposed to improve the predictive and control precision. The MENNs online learning improves the control system adaptability to the unpredictable operating environment for AUV. Simulations on AUV yaw velocity control are concluded to illustrate the effectiveness of the proposed control scheme.},
author = {Xu, Jianan and Zhang, Mingjun and Wang, Yujia},
file = {:home/chris/Documents/Mendeley Desktop/Neural networks modelling and generalised predictive control for an AUV.pdf:pdf},
title = {{Neural networks modelling and generalised predictive control for an autonomous underwater vehicle}},
volume = {11},
year = {2010}
}
@article{Mills1995,
abstract = {Underwater vehicles are employed to perform many tasks: structure inspection, recovery, geographic surveys, e.t.c. but most in current operation require constant human supervision. In an attempt to make these vehicles more autonomous, this research project aims to develop a multivariable, neurofuzzy network controller for application to an Autonomous Underwater Vehicle (AUV). The vehicle being studied is Ocean Voyager, an AUV based at Florida Atlantic University. Ocean Voyager is a torpedo shaped, 22 feet long and 13 inches in diameter with a displacement of 2700 lb. The design speed is 10.1 ft/s, with an overall cruise time of 2 hours. A thruster provides the forward propulsion and the vehicle motion is controlled by two surfaces, the stern plane and rudder. A simulation of the vehicle dynamics has been made available to the ISIS research group but for the purposes of this project it has been treated as a black box. Simulation of the vehicle dynamics is achieved by using the standard submarine equations of motion defined by the David Taylor Naval Ship Research and Development Centre (DTNSRDC). Before attempting any control, the problem of modelling the AUV dynamics was studied. The AUV is a nonlinear, multivariable, dynamic process which means that to produce an accurate approximation the selected model must not only be nonlinear but also capable of dealing with high dimensional inputs. The required nonlinearities can be provided by a neurofuzzy system. Neurofuzzy systems combine the positive attributes of a neural network and a fuzzy system. Neural networks have become popular largely due to their ability to universally approximate any continuous nonlinear function using only the information contained in a set of input/output training pairs. The properties of a neural network are determined by its structure and the learning rule used to adapt the weights. Common examples include the multilayer perceptron and the radial basis function network but more recently B-spline networks have been suggested due to their superior numerical properties. The B-spline network belongs to the class of Associative Memory Networks (AMNs) as it generalises locally, i.e. similar inputs map to similar outputs whereas dissimilar inputs map to independent outputs. They also have the advantage that only the output layer weights are adapted which means that well established linear training algorithms can be employed with provable behavioural characteristics. However, a major criticism of most neural networks is their opaque structure where the information stored can not be easily interpreted by the designer. A fuzzy system generally consists of a rule base composed of vague production rules such as IF (error is small) THEN (output is small). The rules are generally linguistic representations and because the information can be easily interpreted by the designer, it is said to be transparent. The power of a fuzzy system lies in the way these production rules are given a precise mathematical meaning so that the resulting system can generalise to produce an appropriate output for previously unseen inputs. However, fuzzy systems have a serious drawback when applied to many applications, their rules are often very difficult or even impossible to determine. This has motivated the development of adaptive fuzzy systems which adjust their rule base parameters via heuristic training rules about which little can be proved. Recently the similarities between neural networks and fuzzy systems have been noted, allowing the positive attributes of both approaches to be combined. The result is termed a neurofuzzy system since it embodies the well established modelling and learning capabilities of a neural networks with the transparent knowledge representation of fuzzy systems. In particular, it has been shown that B-spline networks and certain forms of fuzzy system are equivalent. Unfortunately, these conventional lattice based neurofuzzy systems are limited to problems involving only a small number of inputs. Modelling in high dimensions is difficult due to a phenomenum called the curse of dimensionality, a term first introduced by Bellman in 1961, and which has plagued researchers from many disciplines ever since. The curse can be explained by considering the number of data points need to maintain the same sampling density as the input dimension rises. For example, if the domain of interest is the unit hypercube D = 0,1 n and the required resolution is 0.1 then a minimum of 10 n observations must be stored. Obtaining and storing such a large training set quickly becomes infeasible for n{\textgreater}4. There have many attempts to alleviate the curse of dimensionality and in particular, much progress has been made in the field of statistics. A straightforward and popular approach is to try to exploit structural information which is known (either a priori or is discovered during the training process) about the form of the desired function. For example, an ANalysis Of VAriance (ANOVA) decomposition for an n dimensional function f(x), is expressed as: f(x) = f0 + ... where f0 is the bias and the remaining terms represent the combinations of univariate, bivariate, e.t.c. subfunctions that additively decompose the function f. An ANOVA decomposition describes the relationship between the different input variables but it is only useful if the interactions involving more than say four inputs are identically zero. This constraint could limit the potential for applying an ANOVA representation but often an adequate approximation is still obtained. The question must be asked: are all the nonlinear features important or will a model composed of simple subfunctions suffice? An immediate advantage of the ANOVA representation is that each subfunction can be a lattice based neurofuzzy system and so network transparency is retained. Also, the output is a linear function of the concatenated weight vectors for each subfunction which means the training algorithms derived for conventional neurofuzzy systems still apply. The potential reduction in the number of fuzzy rules can be illustrated by considering the approximation of Powell's function, a system with four inputs and one output. This function is an ideal candidate for an ANOVA decomposition since it contains four, two-dimensional subfunctions. If seven basis functions are used on each axis, the ANOVA system will use approximately 200 rules whereas a conventional neurofuzzy system uses over 2000, the majority of which are redundant.},
author = {Mills, D J and Harris, C J},
file = {:home/chris/Documents/Mendeley Desktop/Neurofuzzy modelling and control of a six degree of freedom AUV.pdf:pdf},
journal = {Control},
keywords = {AUV,neuro-fuzzy control},
pages = {1--3},
title = {{Neurofuzzy modelling and control of a six degree of freedom AUV}},
url = {http://eprints.soton.ac.uk/250103/},
year = {1995}
}
@article{Hidalgo,
abstract = {Localization and mapping are key elements in autonomous vehicles hence robots need to keep track of their position and the environment to trace a path, navigate and avoid obstacles. In the last years, different developments in underwater SLAM (Simultaneous Localization and Mapping) have been achieved from three main approaches: Extended Kalman Filter SLAM (EKF-SLAM), FastSLAM, GraphSLAM. The foundations of these algorithms and their application to underwater scenarios are discussed in this paper. Furthermore, typical instrumentation that makes SLAM possible in these situations are also described. Simulation results show how each approach improves localization and mapping for a robot compared to a straightforward estimation.},
author = {Hidalgo, Franco and Br{\"{a}}unl, Thomas},
file = {:home/chris/Documents/Mendeley Desktop/Underwater SLAM.pdf:pdf},
title = {{Underwater Robot SLAM : Instrumentation and Frameworks}},
year = {2016}
}
@article{Fauske2007,
abstract = {This paper presents a method for identifying dynamic models of autonomous underwater vehicles (AUV) from logged data and a physically motivated model structure. Such models are instrumental for model-based control system design, but also for integrated navigation systems. We motive our work from the perspective of developing second generation integrated navigation systems, which use a sensor fusion approach to merge external information with a dynamic model for purposes of redundancy, integrity, and for fault detection and isolation.},
author = {Fauske, Kjell Magne and Gustafsson, Fredrik and Hegren??s, ??yvind},
doi = {10.1109/ICIF.2007.4408044},
file = {:home/chris/Documents/Mendeley Desktop/Estimation{\_}of{\_}AUV{\_}dynamics{\_}for{\_}sensor{\_}fusion.pdf:pdf},
isbn = {0662478304},
journal = {FUSION 2007 - 2007 10th Int. Conf. Inf. Fusion},
keywords = {AUV,Estimation,Kalman filtering},
title = {{Estimation of AUV dynamics for sensor fusion}},
year = {2007}
}
@article{Valeriano-Medina2012,
author = {Valeriano-Medina, Y. and Mart{\'{i}}nez, A. and Hern{\'{a}}ndez, L. and Sahli, H. and Rodr{\'{i}}guez, Y. and Ca{\~{n}}izares, J.R.},
doi = {10.1080/13873954.2012.717226},
file = {:home/chris/Documents/Mendeley Desktop/Dynamic model for an autonomous underwater vehicle based on experimental data{\_}very{\_}good.pdf:pdf},
issn = {1387-3954},
journal = {Math. Comput. Model. Dyn. Syst.},
number = {July},
pages = {1--26},
title = {{Dynamic model for an autonomous underwater vehicle based on experimental data}},
volume = {3954},
year = {2012}
}
@article{CutipaLuque2011,
abstract = {The present research paper deals with identification of an Autonomous Underwater Vehicle (AUV) system using Kalman filter and maximum likelihood estimation approaches in order to get an accuracy and more representative nominal model. Despite all efforts in modelling of the vehicle, uncertainties in the parameters can appear because some dynamics are difficult to be modelled and, eventually, are neglected. Complementary, the paper aims the problem of robust control in the sway and yaw dynamics assuming uncertainties, perturbations and noises. The results are shown by simulated data.},
author = {{Cutipa Luque}, Juan C. and Donha, D??cio Crisol},
doi = {10.3182/20110828-6-IT-1002.03554},
file = {:home/chris/Documents/Mendeley Desktop/AUV Identification and Robust Control.pdf:pdf},
isbn = {9783902661937},
issn = {14746670},
journal = {IFAC Proc. Vol.},
keywords = {AUV,Kalman filter,Maximum likelihood,Robust control,System identification},
number = {PART 1},
pages = {14735--14741},
pmid = {12071255},
title = {{AUV identification and robust control}},
volume = {18},
year = {2011}
}
@article{Ribas2010,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ribas, David and Ridao, Pere and Neira, Jos{\'{e}}},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/chris/Documents/Mendeley Desktop/Underwater SLAM for Structured Environments Using an Imaging Sonar.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Springer Tracts Adv. Robot.},
keywords = {Bott},
number = {1},
pages = {1--5},
pmid = {15003161},
title = {{Underwater SLAM for Structured Environments Using an Imaging Sonar}},
year = {2010}
}
@article{Kennedy2007,
abstract = {Researchers at the University of Victoria have developed a hybrid autonomous underwater vehicle (AUV) named MACO capable of 3-D station keeping and manoeuvring without forward velocity. This makes it suitable to perform many of the tasks traditionally accomplished by remotely-piloted underwater vehicles. Once operational, MACO was used in a Defence Research and Development Canada (DRDC) feasibility study for using AUVs to support rapid deployment of acoustic element arrays. The AUV was required to stop and hover, while triggering a low frequency sound source. The performance of MACO during these sea trials is presented as the conclusion to this paper.},
author = {Kennedy, Jeff and Gamroth, Emmett and Bradley, Colin and Proctor, Alison Aa and Heard, Garry J Gj},
doi = {10.3723/175605407782724979},
file = {:home/chris/Documents/Mendeley Desktop/Decoupled modelling and controller design for the hybrid AUV$\backslash$: MACO.pdf:pdf},
isbn = {0141-0814},
issn = {17560543},
journal = {Underw. Technol.},
keywords = {Acoustic arrays,Autonomous underwater vehicles,Control system synthesis,Engineering research,Maneuverability,Mathematical models,Three dimensional},
number = {1},
pages = {11--21},
title = {{Decoupled modelling and controller design for the hybrid autonomous underwater vehicle: MACO}},
volume = {27},
year = {2007}
}
@article{Gonzalez2004,
abstract = {Autonomous underwater vehicles are currently being utilised for scientific, commercial and military underwater applications. These vehicles require autonomous guidance and control systems in order to perform underwater tasks. Modelling, system identification and control of these vehicles are still major active areas of research and development. This thesis is concerned with the design and development of an AUV specifically in- tended for entry into international underwater vehicle competitions. The thesis consists of two phases; the first involves the design and construction of the vehicle while the second phase is concerned with the modelling and system identification of the vehicle, as well as the simulation of a control system. The design and development of the vehicle consisted of implementing a mechanical and electrical system, as well as the integration of subsystems. The development of these systems has resulted in a low-speed, bottom-heavy, open-frame underwater vehicle named the Mako that exhibits high symmetry, modularity and stability. The modelling of the Mako was then performed which involved the application of the dynamic model of an underwater vehicle and the consequent identification of the relevant parameters. The systemidentification of the vehicle parameters consisted of using onboard sensors to perform static and dynamic experiments. Least squares estimation was used to estimate the parameters from the experimental data obtained. For the control system of the Mako, a PID tracking controller based on computed torque control was adopted. The controller was applied to the vehicle's dynamics and simulated using the parameters found in the system identification process. The results of the simulations demonstrate that this type of controller could indeed be successfully implemented on the vehicle. The undertakings in this thesis have resulted in a functioning autonomous underwater vehicle that has undergone modelling, system identification and preliminary control analy- sis. The groundwork has indeed been laid for the Mako's entry into future underwater competitions.},
author = {Gonzalez, Louis Adrew LA},
file = {:home/chris/Documents/Mendeley Desktop/2004-AUV-Gonzalez.pdf:pdf},
journal = {BE Thesis, Univ. West. Aust.},
pages = {156},
title = {{Design , Modelling and Control of an Autonomous Underwater Vehicle}},
url = {http://www.mendeley.com/research/design-modelling-control-autonomous-underwater-vehicle-associate-professor-thomas-br-aunl/{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.6699{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
@book{Do2009,
author = {Do, KD and Pan, J},
booktitle = {Aust. Springer},
doi = {10.1007/978-1-84882-730-1},
file = {:home/chris/Documents/Mendeley Desktop/Control of Ships and Underwater Vehicles - Design for Underactuated and Nonlinear Marine Systems.pdf:pdf},
isbn = {9781848827295},
title = {{Control of ships and underwater vehicles}},
url = {http://link.springer.com/10.1007/978-1-84882-730-1},
year = {2009}
}
@article{Hong2016,
abstract = {Visual inspection of underwater structures including ship-hull inspection has been performed by human divers. It is a highly dangerous task and thus can be a potential application for unmanned underwater vehicles. This paper introduces an efficient visual simultaneous localization and mapping (SLAM) algorithm that can be applied to the autonomous inspection of underwater structures. Considering that visual features are sparsely located on the surface of typical underwater structures, the proposed visual SLAM algorithm employs a selective image registration scheme consisting of key-frame selection and keypair selection. By using only potentially effective images and image pairs for feature-based image registration, the computational burden of the visual SLAM can be substantially reduced, compared with the conventional method. Experimental results using a hover-capable unmanned underwater vehicle verify the practical feasibility and performance of the proposed methodology.},
author = {Hong, Seonghun and Kim, Jinwhan},
doi = {10.1109/AUV.2016.7778670},
file = {:home/chris/Documents/Mendeley Desktop/Efficient Visual SLAM using Selective Image Registration for Autonomous Inspection of Underwater Structures.pdf:pdf},
isbn = {9781509024421},
journal = {Auton. Underw. Veh. 2016, AUV 2016},
keywords = {Autonomous underwater vehicle,Selective image registration,Visual SLAM,Visual inspection},
pages = {189--194},
title = {{Efficient visual SLAM using selective image registration for autonomous inspection of underwater structures}},
year = {2016}
}
@article{Aulinas2011,
author = {Aulinas, Josep and Carreras, Marc and Llado, Xavier and Salvi, Joaquim and Garcia, Rafael and Prados, Ricard},
file = {:home/chris/Documents/Mendeley Desktop/Feature extraction for underwater visual SLAM.pdf:pdf},
title = {{Feature extraction for underwater visual SLAM.pdf}},
year = {2011}
}
@article{Bailey2006,
abstract = {This paper presents an analysis of the extended Kalman filter formulation of simultaneous localisation and mapping (EKF-SLAM). We show that the algorithm produces very optimistic estimates once the "true" uncertainty in vehicle heading exceeds a limit. This failure is subtle and cannot, in general, be detected without ground-truth, although a very inconsistent filter may exhibit observable symptoms, such as disproportionately large jumps in the vehicle pose update. Conventional solutions - adding stabilising noise, using an iterated EKF or unscented filter, etc., - do not improve the situation. However, if "small" heading uncertainty is maintained, EKF-SLAM exhibits consistent behaviour over an extended time-period. Although the uncertainty estimate slowly becomes optimistic, inconsistency can be mitigated indefinitely by applying tactics such as batch updates or stabilising noise. The manageable degradation of small heading variance SLAM indicates the efficacy of submap methods for large-scale maps},
author = {Bailey, Tim and Nieto, Juan and Guivant, Jose and Stevens, Michael and Nebot, Eduardo},
doi = {10.1109/IROS.2006.281644},
file = {:home/chris/Documents/Mendeley Desktop/Consistency of the EKF-SLAM Algorithm.pdf:pdf},
isbn = {142440259X},
issn = {10504729},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
number = {1},
pages = {3562--3568},
title = {{Consistency of the EKF-SLAM algorithm}},
year = {2006}
}
@article{Chen2013,
abstract = {This paper proposes a pose-based GraphSLAM algorithm for robotic fish equipped with a Mechanical Scanning Sonar (MSS) that has a low frequency of range readings. The main contribution of this paper is the construction of a pose graph as the front-end part of the normal GraphSLAM algorithm. The proposed algorithm has three stages as follows: 1) scan generation which incorporates a novel Extended Kalman Filter (EKF) based algorithm that takes the fish motion into account; 2) data association which is based on Mahanalobis distance and shape matching for determining loop closures; 3) scan matching which is for constraints calculation and pose graph construction. The constructed pose graph is then fed into a back-end optimizer - g2o for finding the optimal position of robotic fish. The viability and the accuracy of the proposed algorithm are verified by extensive simulations, compared with the dead reckoning and scan matching approaches.},
author = {Chen, Ling and Wang, Sen and Hu, Huosheng},
doi = {10.1109/ROBIO.2013.6739432},
file = {:home/chris/Documents/Mendeley Desktop/Pose-based GraphSLAM Algorithm for Robotic Fish with a Mechanical Scanning Sonar.pdf:pdf},
isbn = {978-1-4799-2744-9},
journal = {2013 IEEE Int. Conf. Robot. Biomimetics, ROBIO 2013},
number = {December},
pages = {38--43},
title = {{Pose-based GraphSLAM algorithm for robotic fish with a mechanical scanning sonar}},
year = {2013}
}
@article{Newcombe2011,
abstract = {DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense, every pixel methods. As a single hand-held RGB camera flies over a static scene, we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices. We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term, and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework. Interleaved, we track the camera's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model. Our algorithms are highly parallelisable throughout and DTAM achieves real-time performance using current commodity GPU hardware. We demonstrate that a dense model permits superior tracking performance under rapid motion compared to a state of the art method using features; and also show the additional usefulness of the dense model for real-time scene interaction in a physics-enhanced augmented reality application.},
author = {Newcombe, Richard A. and Lovegrove, Steven J. and Davison, Andrew J.},
doi = {10.1109/ICCV.2011.6126513},
file = {:home/chris/Documents/Mendeley Desktop/DTAM$\backslash$: Dense Tracking and Mapping in Real-Time.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {2320--2327},
pmid = {6126513},
title = {{DTAM: Dense tracking and mapping in real-time}},
year = {2011}
}
@article{Cadena2016,
abstract = {Simultaneous Localization and Mapping (SLAM)consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.05830v2},
author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
doi = {10.1109/TRO.2016.2624754},
eprint = {arXiv:1606.05830v2},
file = {:home/chris/Documents/Mendeley Desktop/Past, Present and Future of Simultaneous Localization and Mapping$\backslash$: Towards the Robust-Perception Age.pdf:pdf},
isbn = {9781479936847},
issn = {15523098},
journal = {IEEE Trans. Robot.},
keywords = {Factor graphs,localization,mapping,maximum a posteriori estimation,perception,robots,sensing,simultaneous localization and mapping (SLAM)},
number = {6},
pages = {1309--1332},
pmid = {6576973927449638915},
title = {{Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age}},
volume = {32},
year = {2016}
}
@article{Ribas2008,
abstract = {This paper describes a navigation system for autonomous underwater vehicles (AUVs) in partially structured environments, such as dams, harbors, marinas, and marine plat- forms. A mechanically scanned imaging sonar is used to obtain information about the location of vertical planar structures present in such environments. A robust voting algo- rithm has been developed to extract line features, together with their uncertainty, fromthe continuous sonar data flow. The obtained information is incorporated into a feature-based simultaneous localization and mapping (SLAM) algorithm running an extended Kalman filter. Simultaneously, the AUV's position estimate is provided to the feature extraction al- gorithm to correct the distortions that the vehiclemotion produces in the acoustic images. Moreover, a procedure to build andmaintain a sequence of local maps and to posteriorly recover the full global map has been adapted for the application presented. Experiments carried out in a marina located in the Costa Brava (Spain) with the Ictineu AUV show the viability of the proposed approach.},
author = {Ribas, David and Ridao, Pere and {Domingo Tard{\'{o}}s}, Juan and Neira, Jos{\'{e}}},
doi = {10.1002/rob.20249},
file = {:home/chris/Documents/Mendeley Desktop/Underwater SLAM in Man-Made Structured Environments.pdf:pdf},
journal = {J. F. Robot.},
pages = {1--24},
title = {{Underwater SLAM in Man-Made Structured Environments}},
year = {2008}
}
@article{Bailey2006a,
abstract = {This paper discusses the recursive Bayesian formulation of the simultaneous localization and mapping (SLAM) problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. The paper focuses on three key areas: computational complexity; data association; and environment representation},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Bailey, Tim and Durrant-Whyte, Hugh},
doi = {10.1109/MRA.2006.1678144},
eprint = {there is not},
file = {:home/chris/Documents/Mendeley Desktop/SLAM Part 1.pdf:pdf},
isbn = {1070-9932 VO - 13},
issn = {10709932},
journal = {IEEE Robot. Autom. Mag.},
number = {3},
pages = {108--117},
pmid = {1638022},
title = {{Simultaneous localization and mapping (SLAM): Part I}},
volume = {13},
year = {2006}
}
@article{Kim2013,
abstract = {This paper reports a real-time monocular visual simultaneous localization and mapping (SLAM) algorithm and results for its application in the area of autonomous underwater ship hull inspection. The proposed algorithm overcomes some of the specific challenges associated with underwater visual SLAM, namely, limited field of view imagery and feature-poor regions. It does so by exploiting our SLAM navigation prior within the image registration pipeline and by being selective about which imagery is considered informative in terms of our visual SLAM map. A novel online bag-of-words measure for intra and interimage saliency are introduced and are shown to be useful for image key-frame selection, information-gain-based link hypothesis, and novelty detection. Results from three real-world hull inspection experiments evaluate the overall approach, including one survey comprising a 3.4-h/2.7-km-long trajectory.},
author = {Kim, Ayoung and Eustice, Ryan M.},
doi = {10.1109/TRO.2012.2235699},
file = {:home/chris/Documents/Mendeley Desktop/Real-time visual SLAM for Autonomous Underwater Hull Inspection using Visual Saliency.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Trans. Robot.},
keywords = {Computer vision,information gain,marine robotics,simultaneous localization and mapping (SLAM),visual saliency},
number = {3},
pages = {719--733},
title = {{Real-time visual SLAM for autonomous underwater hull inspection using visual saliency}},
volume = {29},
year = {2013}
}
@article{Bailey2006b,
abstract = {This paper discusses the recursive Bayesian formulation of the simultaneous localization and mapping (SLAM) problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. The paper focuses on three key areas: computational complexity; data association; and environment representation},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Bailey, B Y T I M and Durrant-whyte, Hugh},
doi = {10.1109/MRA.2006.1678144},
eprint = {there is not},
file = {:home/chris/Documents/Mendeley Desktop/SLAM Part 2.pdf:pdf},
isbn = {1610-7438},
issn = {10709932},
journal = {Update},
number = {September},
pages = {108--117},
pmid = {1638022},
title = {{Simultaneous Localization and Mapping ( SLAM ): Part II}},
url = {http://ieeexplore.ieee.org/ielx5/100/35300/01678144.pdf?tp={\&}arnumber=1678144{\&}isnumber=35300},
volume = {13},
year = {2006}
}
@unpublished{Salas-moreno2013,
author = {Salas-moreno, Renato F and Newcombe, Richard A and Davison, Andrew J and Kelly, Paul H J},
doi = {10.1109/CVPR.2013.178},
file = {:home/chris/Documents/Mendeley Desktop/SLAM++$\backslash$: Simultaneous Localisation and Mapping at the Level of Objects.pdf:pdf},
title = {{SLAM ++ : Simultaneous Localisation and Mapping at the Level of Objects}},
url = {https://www.doc.ic.ac.uk/{~}ajd/Publications/salas-moreno{\_}etal{\_}cvpr2013.pdf},
year = {2013}
}
@phdthesis{Konig2015,
author = {K{\"{o}}nig, Malte},
file = {:home/chris/Documents/Mendeley Desktop/Malte TUHH BA.pdf:pdf},
school = {University of Technology Hamburg},
title = {{Modellbildung und Entwurf eines Reglers zur Stabilisierung von Unterwasserfahrzeugen}},
year = {2015}
}
@article{Vervoort2009,
abstract = {The Unmanned Underwater Vehicle (UUV) designed at the Mechanical Engineering Department of the University of Canterbury is in an early stage of development. With the design of the AUV completed, the primary focus now is to design control software. The control software has to be able to stabilize the vehicle at a desired position and let the vehicle follow a desired trajectory within a reasonable error. The AUV is able to operate in six degrees of freedom and the dynamics of an AUV are nonlinear and subjected to a variety of disturbances. A kinematic and dynamic model of the AUV is derived for the six degrees of freedom operating range. The degrees of freedom are decoupled, where only the surge, heave and yaw degrees of freedom will be controlled. A system identification approach is proposed to estimate unknown mass/inertia and damping parameters, treating the surge, heave and yaw degrees of freedom separately. Unfortunately due to sensor malfunctioning the unknown mass/inertia and damping terms could not be estimated, a parameter selection method is used instead. The parameter selection is based on parameter estimation results of software programs and a parameter comparison with other similar shaped AUVs. A feedback linearizing control technique is chosen to design control laws for control in the surge, heave and yaw degrees of freedom. The controller performed well under parameter perturbation and noise contamination on the feedback position and velocity signals. An under-actuated control problem arises in x-y plane, since one is only able to control two degrees of freedom while the AUV has three degrees of freedom. To steer the AUV smoothly, a path planning model is derived, which describes a path from A to B with the use of polar coordinates. It works as expected, except for planning a straight line trajectory due to singularity reasons. iv The under-actuated control problem is again investigated, assuming sway motion of the AUV is not negligible, so Coriolis and centripetal forces need to be considered. A state feedback control method is explained, which is able to follow a reference trajectory in the x-y plane. The only disadvantage is that the yaw degree of freedom of the reference trajectory needs to be persistently exciting, which makes following a straight line impossible.},
author = {Vervoort, J.H.A.M.},
file = {:home/chris/Documents/Mendeley Desktop/Modeling and Control of an Unmanned Underwater Vehicle.pdf:pdf},
journal = {Mate.Tue.Nl},
title = {{Modeling and Control of an Unmanned Underwater Vehicle}},
url = {http://www.mate.tue.nl/mate/pdfs/10894.pdf},
year = {2009}
}
@article{AulinasMaso2011,
abstract = {An overview of underwater SLAM implementations as well as submapping SLAM approaches is given in this paper. Besides, the implementation of the so called selective submap joining SLAM on the SPARUS AUV is presented. SPARUS carries a down-looking optical camera. The information gather by this camera is run through SLAM, together with on-board navigation sensors, producing a precise localization of the vehicle and a consistent final map. Experimental validation on a real dataset is described, showing a promising performance of our implementation.},
author = {{Aulinas Mas{\'{o}}}, J.M. and Petillot, Yvan R. and Lland{\'{o}}, Xavier and Salvi, Joaquim and Garcia, Rafael},
file = {:home/chris/Documents/Mendeley Desktop/Vision-Based Underwater SLAM for the SPARUS AUV.pdf:pdf},
journal = {2011 Int. Conf. Comput. IT Appl. Marit. Ind.},
keywords = {AUV,SLAM,underwater,vision},
pages = {171--180},
title = {{Vision-based underwater SLAM for the SPARUS AUV}},
year = {2011}
}
@phdthesis{Busch2009,
abstract = {Thesis (MScEng (Electrical and Electronic Engineering))--University of Stellenbosch, 2009.},
author = {Busch, Regardt},
file = {:home/chris/Documents/Mendeley Desktop/busch-r-2009.pdf:pdf},
keywords = {Autonomous underwater vehicle5,Dissertations -- Electrical and electronic enginee,Remote submersibles,Theses -- Electrical and electronic engineering},
number = {April},
title = {{Modelling and simulation of an autonomous underwater vehicle}},
url = {http://scholar.sun.ac.za/handle/10019.1/2429},
year = {2009}
}
@article{Ernani2014,
author = {Ernani, M Zare and Bozorg, M and Ebrahimi, S},
file = {:home/chris/Documents/Mendeley Desktop/Identification of an Autonomous Underwater Vehicle Dynamic.pdf:pdf},
keywords = {autonomous underwater vehicle,extended kalman filter,hydrodynamic coefficients},
title = {{Identification of an Autonomous Underwater Vehicle Dynamic Using Extended Kalman Filter with ARMA Noise Model}},
year = {2014}
}
@article{Xu2013,
abstract = {An identification method based on support vector machines (SVM) is proposed for modeling nonlinear dynamics of underwater vehicles (UVs), and a typical torpedo-shaped autonomous underwater vehicle (AUV) is employed for the purpose of validation. To obtain the hydrodynamic derivatives of the vehicle and the dynamical models of the thruster and fins, a series of hydrodynamic experiments are conducted by using vertical planar motion mechanism (PMM) and circulating water channel (CWC). Maneuvering simulation is carried out by using the hydrodynamic model obtained from experiments, and SVM is applied to identify the damping terms together with Coriolis and centripetal terms by analyzing the simulation data. By using the identified nonlinear model and experiment-based hydrodynamic model respectively, maneuvering simulations and control applications are implemented. The results are compared to verify the proposed method, and the effectiveness and good generalization performance of SVM in modeling the nonlinear dynamics of UVs are demonstrated.},
author = {Xu, Feng and Zou, Zao-Jian and Yin, Jian-Chuan and Cao, Jian},
doi = {10.1016/j.oceaneng.2013.02.006},
file = {:home/chris/Documents/Mendeley Desktop/Identification modeling of underwater vehicles nonlinear dynamics based on SVMs.pdf:pdf},
issn = {00298018},
journal = {Ocean Eng.},
keywords = {Maneuvering simulation,Nonlinear dynamics,Support vector machines,Underwater vehicle},
pages = {68--76},
publisher = {Elsevier},
title = {{Identification modeling of underwater vehicles' nonlinear dynamics based on support vector machines}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0029801813000863},
volume = {67},
year = {2013}
}
@article{Ferreira2009,
abstract = {This papers addresses the dynamic characterization of the autonomous underwater vehicle MARES. The paper presents the main dynamic properties of this underwater robotic platform as well as the procedures employed to obtain the parameters that define the vehicle model. Furthermore, the paper also presents a detailed characterization of the elementary motions that this vehicle is able to perform.},
author = {Ferreira, Bruno and Pinto, Miguel and Matos, An??bal and Cruz, Nuno},
doi = {10.1109/IECON.2009.5415198},
file = {:home/chris/Documents/Mendeley Desktop/Hydrodynamic modeling and motion limits of AUV MARES.pdf:pdf},
isbn = {9781424446490},
issn = {1553-572X},
journal = {IECON Proc. (Industrial Electron. Conf.},
pages = {2241--2246},
title = {{Hydrodynamic modeling and motion limits of AUV MARES}},
year = {2009}
}
@article{Avila2013,
abstract = {Most of the works published on hydrodynamic parameter identification of open-frame underwater vehicles focus their attention almost exclusively on good coherence between simulated and measured responses, giving less importance to the determination of “actual values” for hydrodynamic parameters. To gain insight into hydrodynamic parameter experimental identification of open-frame underwater vehicles, an experimental identification procedure is proposed here to determine parameters of uncoupled and coupled models. The identification procedure includes: (i) a prior estimation of actual values of the forces/torques applied to the vehicle, (ii) identification of drag parameters from constant velocity tests and (iii) identification of inertia and coupling parameters from oscillatory tests; at this stage, the estimated values of drag parameter obtained in item (ii) are used. The procedure proposed here was used to identify the hydrodynamic parameters of LAURS—an unmanned underwater vehicle developed at the University of S{\~{a}}o Paulo. The thruster–thruster and thruster–hull interactions and the advance velocity of the vehicle are shown to have a strong impact on the efficiency of thrusters appended to open-frame underwater vehicles, especially for high advance velocities. Results of tests with excitation in 1-DOF and 3-DOF are reported and discussed, showing the feasibility of the developed procedure.},
author = {Avila, Juan P.J. and Donha, D{\'{e}}cio C. and Adamowski, Julio C.},
doi = {10.1016/j.oceaneng.2012.10.007},
file = {:home/chris/Documents/Mendeley Desktop/Experimental-model-identification-of-open-frame-underwater-vehicles{\_}2013{\_}Ocean-Engineering.pdf:pdf},
issn = {00298018},
journal = {Ocean Eng.},
pages = {81--94},
title = {{Experimental model identification of open-frame underwater vehicles}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0029801812003836},
volume = {60},
year = {2013}
}
